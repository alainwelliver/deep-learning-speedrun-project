{
    "experiment_name": "nanogpt_depthwarm",
    "description": "Stage C: DepthWarm progressive layer skipping - full run (5100 iters, 8 H100s)",
    "script": "experiments/train_gpt_depth_warn.py",
    "stage": "C_full",
    "n_gpus": 8,
    "base_seed": 42,
    "modification_type": "architecture",
    "modification_details": "Dynamic depth warmup: early in training only shallow layers are always active, deeper layers are stochastically skipped; later in training all layers are active.",
    "target_val_loss": 3.28,
    "baseline_val_loss": 3.27,
    "baseline_std": 0.02,
    "stage_config": {
      "recommended_runs": 5,
      "notes": "Full training (5100 iters) - uses default hyperparameters from training script"
    },
    "hyperparameters": {
      "batch_size": 512,
      "device_batch_size": 64,
      "sequence_length": 1024,
      "num_iterations": 5100,
      "learning_rate": 0.0036,
      "warmup_iters": 0,
      "warmdown_iters": 1450,
      "weight_decay": 0,
      "val_loss_every": 125,
      "val_tokens": 10485760,
      "notes": "Same hyperparameters as baseline train_gpt.py; only DepthWarm scheduling is new."
    }
}