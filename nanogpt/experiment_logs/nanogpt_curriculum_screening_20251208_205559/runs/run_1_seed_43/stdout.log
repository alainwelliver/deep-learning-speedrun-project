/deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:31: SyntaxWarning: invalid escape sequence '\s'
  """
using device: cuda:0
Training DataLoader: total number of tokens: 900000000 across 9 files
Validation DataLoader: total number of tokens: 100000000 across 1 files
step:0/1000 val_loss:15.9977 train_time:350ms step_avg:nanms T_curr:256
[rank0]: Traceback (most recent call last):
[rank0]:   File "/deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py", line 557, in <module>
[rank0]:     _, loss = model(x_crop, y_crop, return_logits=False)
[rank0]:               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1784, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/nn/parallel/distributed.py", line 1648, in forward
[rank0]:     else self._run_ddp_forward(*inputs, **kwargs)
[rank0]:          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/nn/parallel/distributed.py", line 1474, in _run_ddp_forward
[rank0]:     return self.module(*inputs, **kwargs)  # type: ignore[index]
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 375, in __call__
[rank0]:     return super().__call__(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1784, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 749, in compile_wrapper
[rank0]:     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
[rank0]:     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/output_graph.py", line 1871, in _call_user_compiler
[rank0]:     raise BackendCompilerFailed(
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/output_graph.py", line 1846, in _call_user_compiler
[rank0]:     compiled_fn = compiler_fn(gm, example_inputs)
[rank0]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/backends/distributed.py", line 572, in compile_fn
[rank0]:     submod_compiler.run(*example_inputs)
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/fx/interpreter.py", line 173, in run
[rank0]:     self.env[node] = self.run_node(node)
[rank0]:                      ^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/backends/distributed.py", line 303, in run_node
[rank0]:     compiled_submod_real = self.compile_submod(real_mod, new_args, kwargs)
[rank0]:                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/backends/distributed.py", line 218, in compile_submod
[rank0]:     self.compiler(input_mod, args),
[rank0]:     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/repro/after_dynamo.py", line 150, in __call__
[rank0]:     compiled_gm = compiler_fn(gm, example_inputs)
[rank0]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/__init__.py", line 2380, in __call__
[rank0]:     return compile_fx(model_, inputs_, config_patches=self.config)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py", line 2418, in compile_fx
[rank0]:     return aot_autograd(
[rank0]:            ^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/backends/common.py", line 109, in __call__
[rank0]:     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)
[rank0]:          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1199, in aot_module_simplified
[rank0]:     compiled_fn = AOTAutogradCache.load(
[rank0]:                   ^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/autograd_cache.py", line 1140, in load
[rank0]:     compiled_fn = dispatch_and_compile()
[rank0]:                   ^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1184, in dispatch_and_compile
[rank0]:     compiled_fn, _ = create_aot_dispatcher_function(
[rank0]:                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 576, in create_aot_dispatcher_function
[rank0]:     return _create_aot_dispatcher_function(
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 836, in _create_aot_dispatcher_function
[rank0]:     compiled_fn, fw_metadata = compiler_fn(
[rank0]:                                ^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py", line 1576, in aot_dispatch_autograd
[rank0]:     ) = fakified_out_wrapper.pre_compile(
[rank0]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 601, in pre_compile
[rank0]:     n.meta["val"] for n in (list(fw_module.graph.nodes)[-1].args[0])
[rank0]:     ^^^^^^
[rank0]: torch._dynamo.exc.BackendCompilerFailed: backend='compile_fn' raised:
[rank0]: AttributeError: 'int' object has no attribute 'meta'

[rank0]: While executing %submod_0 : [num_users=3] = call_module[target=submod_0](args = (%l_idx_, %s20, %l_self_modules_transformer_modules_wte_parameters_weight_), kwargs = {})
[rank0]: GraphModule: class GraphModule(torch.nn.Module):
[rank0]:     def forward(self, L_self_modules_transformer_modules_wte_parameters_weight_: "f32[50304, 768][768, 1]", s20: "Sym(s20)", L_idx_: "i64[16, 256][s20, 1]", L_self_modules_transformer_modules_h_modules_0_modules_attn_modules_c_q_parameters_weight_: "f32[768, 768][768, 1]", L_self_modules_transformer_modules_h_modules_0_modules_attn_modules_c_k_parameters_weight_: "f32[768, 768][768, 1]", L_self_modules_transformer_modules_h_modules_0_modules_attn_modules_c_v_parameters_weight_: "f32[768, 768][768, 1]", L_self_modules_transformer_modules_h_modules_0_modules_attn_modules_rotary_cos_cached: "bf16[256, 64][64, 1]", L_self_modules_transformer_modules_h_modules_0_modules_attn_modules_rotary_sin_cached: "bf16[256, 64][64, 1]", L_self_modules_transformer_modules_h_modules_0_modules_attn_modules_c_proj_parameters_weight_: "f32[768, 768][768, 1]", L_self_modules_transformer_modules_h_modules_0_modules_mlp_modules_c_fc_parameters_weight_: "f32[3072, 768][768, 1]", L_self_modules_transformer_modules_h_modules_0_modules_mlp_modules_c_proj_parameters_weight_: "f32[768, 3072][3072, 1]", L_self_modules_transformer_modules_h_modules_1_modules_attn_modules_c_q_parameters_weight_: "f32[768, 768][768, 1]", L_self_modules_transformer_modules_h_modules_1_modules_attn_modules_c_k_parameters_weight_: "f32[768, 768][768, 1]", L_self_modules_transformer_modules_h_modules_1_modules_attn_modules_c_v_parameters_weight_: "f32[768, 768][768, 1]", L_self_modules_transformer_modules_h_modules_1_modules_attn_modules_rotary_cos_cached: "bf16[256, 64][64, 1]", L_self_modules_transformer_modules_h_modules_1_modules_attn_modules_rotary_sin_cached: "bf16[256, 64][64, 1]", L_self_modules_transformer_modules_h_modules_1_modules_attn_modules_c_proj_parameters_weight_: "f32[768, 768][768, 1]", L_self_modules_transformer_modules_h_modules_1_modules_mlp_modules_c_fc_parameters_weight_: "f32[3072, 768][768, 1]", L_self_modules_transformer_modules_h_modules_1_modules_mlp_modules_c_proj_parameters_weight_: "f32[768, 3072][3072, 1]", L_self_modules_transformer_modules_h_modules_2_modules_attn_modules_c_q_parameters_weight_: "f32[768, 768][768, 1]", L_self_modules_transformer_modules_h_modules_2_modules_attn_modules_c_k_parameters_weight_: "f32[768, 768][768, 1]", L_self_modules_transformer_modules_h_modules_2_modules_attn_modules_c_v_parameters_weight_: "f32[768, 768][768, 1]", L_self_modules_transformer_modules_h_modules_2_modules_attn_modules_rotary_cos_cached: "bf16[256, 64][64, 1]", L_self_modules_transformer_modules_h_modules_2_modules_attn_modules_rotary_sin_cached: "bf16[256, 64][64, 1]", L_self_modules_transformer_modules_h_modules_2_modules_attn_modules_c_proj_parameters_weight_: "f32[768, 768][768, 1]", L_self_modules_transformer_modules_h_modules_2_modules_mlp_modules_c_fc_parameters_weight_: "f32[3072, 768][768, 1]", L_self_modules_transformer_modules_h_modules_2_modules_mlp_modules_c_proj_parameters_weight_: "f32[768, 3072][3072, 1]", L_self_modules_transformer_modules_h_modules_3_modules_attn_modules_c_q_parameters_weight_: "f32[768, 768][768, 1]", L_self_modules_transformer_modules_h_modules_3_modules_attn_modules_c_k_parameters_weight_: "f32[768, 768][768, 1]", L_self_modules_transformer_modules_h_modules_3_modules_attn_modules_c_v_parameters_weight_: "f32[768, 768][768, 1]", L_self_modules_transformer_modules_h_modules_3_modules_attn_modules_rotary_cos_cached: "bf16[256, 64][64, 1]", L_self_modules_transformer_modules_h_modules_3_modules_attn_modules_rotary_sin_cached: "bf16[256, 64][64, 1]", L_self_modules_transformer_modules_h_modules_3_modules_attn_modules_c_proj_parameters_weight_: "f32[768, 768][768, 1]", L_self_modules_transformer_modules_h_modules_3_modules_mlp_modules_c_fc_parameters_weight_: "f32[3072, 768][768, 1]", L_self_modules_transformer_modules_h_modules_3_modules_mlp_modules_c_proj_parameters_weight_: "f32[768, 3072][3072, 1]", L_self_modules_transformer_modules_h_modules_4_modules_attn_modules_c_q_parameters_weight_: "f32[768, 768][768, 1]", L_self_modules_transformer_modules_h_modules_4_modules_attn_modules_c_k_parameters_weight_: "f32[768, 768][768, 1]", L_self_modules_transformer_modules_h_modules_4_modules_attn_modules_c_v_parameters_weight_: "f32[768, 768][768, 1]", L_self_modules_transformer_modules_h_modules_4_modules_attn_modules_rotary_cos_cached: "bf16[256, 64][64, 1]", L_self_modules_transformer_modules_h_modules_4_modules_attn_modules_rotary_sin_cached: "bf16[256, 64][64, 1]", L_self_modules_transformer_modules_h_modules_4_modules_attn_modules_c_proj_parameters_weight_: "f32[768, 768][768, 1]", L_self_modules_transformer_modules_h_modules_4_modules_mlp_modules_c_fc_parameters_weight_: "f32[3072, 768][768, 1]", L_self_modules_transformer_modules_h_modules_4_modules_mlp_modules_c_proj_parameters_weight_: "f32[768, 3072][3072, 1]", L_self_modules_transformer_modules_h_modules_5_modules_attn_modules_c_q_parameters_weight_: "f32[768, 768][768, 1]", L_self_modules_transformer_modules_h_modules_5_modules_attn_modules_c_k_parameters_weight_: "f32[768, 768][768, 1]", L_self_modules_transformer_modules_h_modules_5_modules_attn_modules_c_v_parameters_weight_: "f32[768, 768][768, 1]", L_self_modules_transformer_modules_h_modules_5_modules_attn_modules_rotary_cos_cached: "bf16[256, 64][64, 1]", L_self_modules_transformer_modules_h_modules_5_modules_attn_modules_rotary_sin_cached: "bf16[256, 64][64, 1]", L_self_modules_transformer_modules_h_modules_5_modules_attn_modules_c_proj_parameters_weight_: "f32[768, 768][768, 1]", L_self_modules_transformer_modules_h_modules_5_modules_mlp_modules_c_fc_parameters_weight_: "f32[3072, 768][768, 1]", L_self_modules_transformer_modules_h_modules_5_modules_mlp_modules_c_proj_parameters_weight_: "f32[768, 3072][3072, 1]", L_self_modules_transformer_modules_h_modules_6_modules_attn_modules_c_q_parameters_weight_: "f32[768, 768][768, 1]", L_self_modules_transformer_modules_h_modules_6_modules_attn_modules_c_k_parameters_weight_: "f32[768, 768][768, 1]", L_self_modules_transformer_modules_h_modules_6_modules_attn_modules_c_v_parameters_weight_: "f32[768, 768][768, 1]", L_self_modules_transformer_modules_h_modules_6_modules_attn_modules_rotary_cos_cached: "bf16[256, 64][64, 1]", L_self_modules_transformer_modules_h_modules_6_modules_attn_modules_rotary_sin_cached: "bf16[256, 64][64, 1]", L_self_modules_transformer_modules_h_modules_6_modules_attn_modules_c_proj_parameters_weight_: "f32[768, 768][768, 1]", L_self_modules_transformer_modules_h_modules_6_modules_mlp_modules_c_fc_parameters_weight_: "f32[3072, 768][768, 1]", L_self_modules_transformer_modules_h_modules_6_modules_mlp_modules_c_proj_parameters_weight_: "f32[768, 3072][3072, 1]", L_self_modules_transformer_modules_h_modules_7_modules_attn_modules_c_q_parameters_weight_: "f32[768, 768][768, 1]", L_self_modules_transformer_modules_h_modules_7_modules_attn_modules_c_k_parameters_weight_: "f32[768, 768][768, 1]", L_self_modules_transformer_modules_h_modules_7_modules_attn_modules_c_v_parameters_weight_: "f32[768, 768][768, 1]", L_self_modules_transformer_modules_h_modules_7_modules_attn_modules_rotary_cos_cached: "bf16[256, 64][64, 1]", L_self_modules_transformer_modules_h_modules_7_modules_attn_modules_rotary_sin_cached: "bf16[256, 64][64, 1]", L_self_modules_transformer_modules_h_modules_7_modules_attn_modules_c_proj_parameters_weight_: "f32[768, 768][768, 1]", L_self_modules_transformer_modules_h_modules_7_modules_mlp_modules_c_fc_parameters_weight_: "f32[3072, 768][768, 1]", L_self_modules_transformer_modules_h_modules_7_modules_mlp_modules_c_proj_parameters_weight_: "f32[768, 3072][3072, 1]", L_self_modules_transformer_modules_h_modules_8_modules_attn_modules_c_q_parameters_weight_: "f32[768, 768][768, 1]", L_self_modules_transformer_modules_h_modules_8_modules_attn_modules_c_k_parameters_weight_: "f32[768, 768][768, 1]", L_self_modules_transformer_modules_h_modules_8_modules_attn_modules_c_v_parameters_weight_: "f32[768, 768][768, 1]", L_self_modules_transformer_modules_h_modules_8_modules_attn_modules_rotary_cos_cached: "bf16[256, 64][64, 1]", L_self_modules_transformer_modules_h_modules_8_modules_attn_modules_rotary_sin_cached: "bf16[256, 64][64, 1]", L_self_modules_transformer_modules_h_modules_8_modules_attn_modules_c_proj_parameters_weight_: "f32[768, 768][768, 1]", L_self_modules_transformer_modules_h_modules_8_modules_mlp_modules_c_fc_parameters_weight_: "f32[3072, 768][768, 1]", L_self_modules_transformer_modules_h_modules_8_modules_mlp_modules_c_proj_parameters_weight_: "f32[768, 3072][3072, 1]", L_self_modules_transformer_modules_h_modules_9_modules_attn_modules_c_q_parameters_weight_: "f32[768, 768][768, 1]", L_self_modules_transformer_modules_h_modules_9_modules_attn_modules_c_k_parameters_weight_: "f32[768, 768][768, 1]", L_self_modules_transformer_modules_h_modules_9_modules_attn_modules_c_v_parameters_weight_: "f32[768, 768][768, 1]", L_self_modules_transformer_modules_h_modules_9_modules_attn_modules_rotary_cos_cached: "bf16[256, 64][64, 1]", L_self_modules_transformer_modules_h_modules_9_modules_attn_modules_rotary_sin_cached: "bf16[256, 64][64, 1]", L_self_modules_transformer_modules_h_modules_9_modules_attn_modules_c_proj_parameters_weight_: "f32[768, 768][768, 1]", L_self_modules_transformer_modules_h_modules_9_modules_mlp_modules_c_fc_parameters_weight_: "f32[3072, 768][768, 1]", L_self_modules_transformer_modules_h_modules_9_modules_mlp_modules_c_proj_parameters_weight_: "f32[768, 3072][3072, 1]", L_self_modules_transformer_modules_h_modules_10_modules_attn_modules_c_q_parameters_weight_: "f32[768, 768][768, 1]", L_self_modules_transformer_modules_h_modules_10_modules_attn_modules_c_k_parameters_weight_: "f32[768, 768][768, 1]", L_self_modules_transformer_modules_h_modules_10_modules_attn_modules_c_v_parameters_weight_: "f32[768, 768][768, 1]", L_self_modules_transformer_modules_h_modules_10_modules_attn_modules_rotary_cos_cached: "bf16[256, 64][64, 1]", L_self_modules_transformer_modules_h_modules_10_modules_attn_modules_rotary_sin_cached: "bf16[256, 64][64, 1]", L_self_modules_transformer_modules_h_modules_10_modules_attn_modules_c_proj_parameters_weight_: "f32[768, 768][768, 1]", L_self_modules_transformer_modules_h_modules_10_modules_mlp_modules_c_fc_parameters_weight_: "f32[3072, 768][768, 1]", L_self_modules_transformer_modules_h_modules_10_modules_mlp_modules_c_proj_parameters_weight_: "f32[768, 3072][3072, 1]", L_self_modules_transformer_modules_h_modules_11_modules_attn_modules_c_q_parameters_weight_: "f32[768, 768][768, 1]", L_self_modules_transformer_modules_h_modules_11_modules_attn_modules_c_k_parameters_weight_: "f32[768, 768][768, 1]", L_self_modules_transformer_modules_h_modules_11_modules_attn_modules_c_v_parameters_weight_: "f32[768, 768][768, 1]", L_self_modules_transformer_modules_h_modules_11_modules_attn_modules_rotary_cos_cached: "bf16[256, 64][64, 1]", L_self_modules_transformer_modules_h_modules_11_modules_attn_modules_rotary_sin_cached: "bf16[256, 64][64, 1]", L_self_modules_transformer_modules_h_modules_11_modules_attn_modules_c_proj_parameters_weight_: "f32[768, 768][768, 1]", L_self_modules_transformer_modules_h_modules_11_modules_mlp_modules_c_fc_parameters_weight_: "f32[3072, 768][768, 1]", L_self_modules_transformer_modules_h_modules_11_modules_mlp_modules_c_proj_parameters_weight_: "f32[768, 3072][3072, 1]", L_targets_: "i64[16, 256][s20, 1]"):
[rank0]:         l_self_modules_transformer_modules_wte_parameters_weight_ = L_self_modules_transformer_modules_wte_parameters_weight_
[rank0]:         l_idx_ = L_idx_
[rank0]:         l_self_modules_transformer_modules_h_modules_0_modules_attn_modules_c_q_parameters_weight_ = L_self_modules_transformer_modules_h_modules_0_modules_attn_modules_c_q_parameters_weight_
[rank0]:         l_self_modules_transformer_modules_h_modules_0_modules_attn_modules_c_k_parameters_weight_ = L_self_modules_transformer_modules_h_modules_0_modules_attn_modules_c_k_parameters_weight_
[rank0]:         l_self_modules_transformer_modules_h_modules_0_modules_attn_modules_c_v_parameters_weight_ = L_self_modules_transformer_modules_h_modules_0_modules_attn_modules_c_v_parameters_weight_
[rank0]:         l_self_modules_transformer_modules_h_modules_0_modules_attn_modules_rotary_cos_cached = L_self_modules_transformer_modules_h_modules_0_modules_attn_modules_rotary_cos_cached
[rank0]:         l_self_modules_transformer_modules_h_modules_0_modules_attn_modules_rotary_sin_cached = L_self_modules_transformer_modules_h_modules_0_modules_attn_modules_rotary_sin_cached
[rank0]:         l_self_modules_transformer_modules_h_modules_0_modules_attn_modules_c_proj_parameters_weight_ = L_self_modules_transformer_modules_h_modules_0_modules_attn_modules_c_proj_parameters_weight_
[rank0]:         l_self_modules_transformer_modules_h_modules_0_modules_mlp_modules_c_fc_parameters_weight_ = L_self_modules_transformer_modules_h_modules_0_modules_mlp_modules_c_fc_parameters_weight_
[rank0]:         l_self_modules_transformer_modules_h_modules_0_modules_mlp_modules_c_proj_parameters_weight_ = L_self_modules_transformer_modules_h_modules_0_modules_mlp_modules_c_proj_parameters_weight_
[rank0]:         l_self_modules_transformer_modules_h_modules_1_modules_attn_modules_c_q_parameters_weight_ = L_self_modules_transformer_modules_h_modules_1_modules_attn_modules_c_q_parameters_weight_
[rank0]:         l_self_modules_transformer_modules_h_modules_1_modules_attn_modules_c_k_parameters_weight_ = L_self_modules_transformer_modules_h_modules_1_modules_attn_modules_c_k_parameters_weight_
[rank0]:         l_self_modules_transformer_modules_h_modules_1_modules_attn_modules_c_v_parameters_weight_ = L_self_modules_transformer_modules_h_modules_1_modules_attn_modules_c_v_parameters_weight_
[rank0]:         l_self_modules_transformer_modules_h_modules_1_modules_attn_modules_rotary_cos_cached = L_self_modules_transformer_modules_h_modules_1_modules_attn_modules_rotary_cos_cached
[rank0]:         l_self_modules_transformer_modules_h_modules_1_modules_attn_modules_rotary_sin_cached = L_self_modules_transformer_modules_h_modules_1_modules_attn_modules_rotary_sin_cached
[rank0]:         l_self_modules_transformer_modules_h_modules_1_modules_attn_modules_c_proj_parameters_weight_ = L_self_modules_transformer_modules_h_modules_1_modules_attn_modules_c_proj_parameters_weight_
[rank0]:         l_self_modules_transformer_modules_h_modules_1_modules_mlp_modules_c_fc_parameters_weight_ = L_self_modules_transformer_modules_h_modules_1_modules_mlp_modules_c_fc_parameters_weight_
[rank0]:         l_self_modules_transformer_modules_h_modules_1_modules_mlp_modules_c_proj_parameters_weight_ = L_self_modules_transformer_modules_h_modules_1_modules_mlp_modules_c_proj_parameters_weight_
[rank0]:         l_self_modules_transformer_modules_h_modules_2_modules_attn_modules_c_q_parameters_weight_ = L_self_modules_transformer_modules_h_modules_2_modules_attn_modules_c_q_parameters_weight_
[rank0]:         l_self_modules_transformer_modules_h_modules_2_modules_attn_modules_c_k_parameters_weight_ = L_self_modules_transformer_modules_h_modules_2_modules_attn_modules_c_k_parameters_weight_
[rank0]:         l_self_modules_transformer_modules_h_modules_2_modules_attn_modules_c_v_parameters_weight_ = L_self_modules_transformer_modules_h_modules_2_modules_attn_modules_c_v_parameters_weight_
[rank0]:         l_self_modules_transformer_modules_h_modules_2_modules_attn_modules_rotary_cos_cached = L_self_modules_transformer_modules_h_modules_2_modules_attn_modules_rotary_cos_cached
[rank0]:         l_self_modules_transformer_modules_h_modules_2_modules_attn_modules_rotary_sin_cached = L_self_modules_transformer_modules_h_modules_2_modules_attn_modules_rotary_sin_cached
[rank0]:         l_self_modules_transformer_modules_h_modules_2_modules_attn_modules_c_proj_parameters_weight_ = L_self_modules_transformer_modules_h_modules_2_modules_attn_modules_c_proj_parameters_weight_
[rank0]:         l_self_modules_transformer_modules_h_modules_2_modules_mlp_modules_c_fc_parameters_weight_ = L_self_modules_transformer_modules_h_modules_2_modules_mlp_modules_c_fc_parameters_weight_
[rank0]:         l_self_modules_transformer_modules_h_modules_2_modules_mlp_modules_c_proj_parameters_weight_ = L_self_modules_transformer_modules_h_modules_2_modules_mlp_modules_c_proj_parameters_weight_
[rank0]:         l_self_modules_transformer_modules_h_modules_3_modules_attn_modules_c_q_parameters_weight_ = L_self_modules_transformer_modules_h_modules_3_modules_attn_modules_c_q_parameters_weight_
[rank0]:         l_self_modules_transformer_modules_h_modules_3_modules_attn_modules_c_k_parameters_weight_ = L_self_modules_transformer_modules_h_modules_3_modules_attn_modules_c_k_parameters_weight_
[rank0]:         l_self_modules_transformer_modules_h_modules_3_modules_attn_modules_c_v_parameters_weight_ = L_self_modules_transformer_modules_h_modules_3_modules_attn_modules_c_v_parameters_weight_
[rank0]:         l_self_modules_transformer_modules_h_modules_3_modules_attn_modules_rotary_cos_cached = L_self_modules_transformer_modules_h_modules_3_modules_attn_modules_rotary_cos_cached
[rank0]:         l_self_modules_transformer_modules_h_modules_3_modules_attn_modules_rotary_sin_cached = L_self_modules_transformer_modules_h_modules_3_modules_attn_modules_rotary_sin_cached
[rank0]:         l_self_modules_transformer_modules_h_modules_3_modules_attn_modules_c_proj_parameters_weight_ = L_self_modules_transformer_modules_h_modules_3_modules_attn_modules_c_proj_parameters_weight_
[rank0]:         l_self_modules_transformer_modules_h_modules_3_modules_mlp_modules_c_fc_parameters_weight_ = L_self_modules_transformer_modules_h_modules_3_modules_mlp_modules_c_fc_parameters_weight_
[rank0]:         l_self_modules_transformer_modules_h_modules_3_modules_mlp_modules_c_proj_parameters_weight_ = L_self_modules_transformer_modules_h_modules_3_modules_mlp_modules_c_proj_parameters_weight_
[rank0]:         l_self_modules_transformer_modules_h_modules_4_modules_attn_modules_c_q_parameters_weight_ = L_self_modules_transformer_modules_h_modules_4_modules_attn_modules_c_q_parameters_weight_
[rank0]:         l_self_modules_transformer_modules_h_modules_4_modules_attn_modules_c_k_parameters_weight_ = L_self_modules_transformer_modules_h_modules_4_modules_attn_modules_c_k_parameters_weight_
[rank0]:         l_self_modules_transformer_modules_h_modules_4_modules_attn_modules_c_v_parameters_weight_ = L_self_modules_transformer_modules_h_modules_4_modules_attn_modules_c_v_parameters_weight_
[rank0]:         l_self_modules_transformer_modules_h_modules_4_modules_attn_modules_rotary_cos_cached = L_self_modules_transformer_modules_h_modules_4_modules_attn_modules_rotary_cos_cached
[rank0]:         l_self_modules_transformer_modules_h_modules_4_modules_attn_modules_rotary_sin_cached = L_self_modules_transformer_modules_h_modules_4_modules_attn_modules_rotary_sin_cached
[rank0]:         l_self_modules_transformer_modules_h_modules_4_modules_attn_modules_c_proj_parameters_weight_ = L_self_modules_transformer_modules_h_modules_4_modules_attn_modules_c_proj_parameters_weight_
[rank0]:         l_self_modules_transformer_modules_h_modules_4_modules_mlp_modules_c_fc_parameters_weight_ = L_self_modules_transformer_modules_h_modules_4_modules_mlp_modules_c_fc_parameters_weight_
[rank0]:         l_self_modules_transformer_modules_h_modules_4_modules_mlp_modules_c_proj_parameters_weight_ = L_self_modules_transformer_modules_h_modules_4_modules_mlp_modules_c_proj_parameters_weight_
[rank0]:         l_self_modules_transformer_modules_h_modules_5_modules_attn_modules_c_q_parameters_weight_ = L_self_modules_transformer_modules_h_modules_5_modules_attn_modules_c_q_parameters_weight_
[rank0]:         l_self_modules_transformer_modules_h_modules_5_modules_attn_modules_c_k_parameters_weight_ = L_self_modules_transformer_modules_h_modules_5_modules_attn_modules_c_k_parameters_weight_
[rank0]:         l_self_modules_transformer_modules_h_modules_5_modules_attn_modules_c_v_parameters_weight_ = L_self_modules_transformer_modules_h_modules_5_modules_attn_modules_c_v_parameters_weight_
[rank0]:         l_self_modules_transformer_modules_h_modules_5_modules_attn_modules_rotary_cos_cached = L_self_modules_transformer_modules_h_modules_5_modules_attn_modules_rotary_cos_cached
[rank0]:         l_self_modules_transformer_modules_h_modules_5_modules_attn_modules_rotary_sin_cached = L_self_modules_transformer_modules_h_modules_5_modules_attn_modules_rotary_sin_cached
[rank0]:         l_self_modules_transformer_modules_h_modules_5_modules_attn_modules_c_proj_parameters_weight_ = L_self_modules_transformer_modules_h_modules_5_modules_attn_modules_c_proj_parameters_weight_
[rank0]:         l_self_modules_transformer_modules_h_modules_5_modules_mlp_modules_c_fc_parameters_weight_ = L_self_modules_transformer_modules_h_modules_5_modules_mlp_modules_c_fc_parameters_weight_
[rank0]:         l_self_modules_transformer_modules_h_modules_5_modules_mlp_modules_c_proj_parameters_weight_ = L_self_modules_transformer_modules_h_modules_5_modules_mlp_modules_c_proj_parameters_weight_
[rank0]:         l_self_modules_transformer_modules_h_modules_6_modules_attn_modules_c_q_parameters_weight_ = L_self_modules_transformer_modules_h_modules_6_modules_attn_modules_c_q_parameters_weight_
[rank0]:         l_self_modules_transformer_modules_h_modules_6_modules_attn_modules_c_k_parameters_weight_ = L_self_modules_transformer_modules_h_modules_6_modules_attn_modules_c_k_parameters_weight_
[rank0]:         l_self_modules_transformer_modules_h_modules_6_modules_attn_modules_c_v_parameters_weight_ = L_self_modules_transformer_modules_h_modules_6_modules_attn_modules_c_v_parameters_weight_
[rank0]:         l_self_modules_transformer_modules_h_modules_6_modules_attn_modules_rotary_cos_cached = L_self_modules_transformer_modules_h_modules_6_modules_attn_modules_rotary_cos_cached
[rank0]:         l_self_modules_transformer_modules_h_modules_6_modules_attn_modules_rotary_sin_cached = L_self_modules_transformer_modules_h_modules_6_modules_attn_modules_rotary_sin_cached
[rank0]:         l_self_modules_transformer_modules_h_modules_6_modules_attn_modules_c_proj_parameters_weight_ = L_self_modules_transformer_modules_h_modules_6_modules_attn_modules_c_proj_parameters_weight_
[rank0]:         l_self_modules_transformer_modules_h_modules_6_modules_mlp_modules_c_fc_parameters_weight_ = L_self_modules_transformer_modules_h_modules_6_modules_mlp_modules_c_fc_parameters_weight_
[rank0]:         l_self_modules_transformer_modules_h_modules_6_modules_mlp_modules_c_proj_parameters_weight_ = L_self_modules_transformer_modules_h_modules_6_modules_mlp_modules_c_proj_parameters_weight_
[rank0]:         l_self_modules_transformer_modules_h_modules_7_modules_attn_modules_c_q_parameters_weight_ = L_self_modules_transformer_modules_h_modules_7_modules_attn_modules_c_q_parameters_weight_
[rank0]:         l_self_modules_transformer_modules_h_modules_7_modules_attn_modules_c_k_parameters_weight_ = L_self_modules_transformer_modules_h_modules_7_modules_attn_modules_c_k_parameters_weight_
[rank0]:         l_self_modules_transformer_modules_h_modules_7_modules_attn_modules_c_v_parameters_weight_ = L_self_modules_transformer_modules_h_modules_7_modules_attn_modules_c_v_parameters_weight_
[rank0]:         l_self_modules_transformer_modules_h_modules_7_modules_attn_modules_rotary_cos_cached = L_self_modules_transformer_modules_h_modules_7_modules_attn_modules_rotary_cos_cached
[rank0]:         l_self_modules_transformer_modules_h_modules_7_modules_attn_modules_rotary_sin_cached = L_self_modules_transformer_modules_h_modules_7_modules_attn_modules_rotary_sin_cached
[rank0]:         l_self_modules_transformer_modules_h_modules_7_modules_attn_modules_c_proj_parameters_weight_ = L_self_modules_transformer_modules_h_modules_7_modules_attn_modules_c_proj_parameters_weight_
[rank0]:         l_self_modules_transformer_modules_h_modules_7_modules_mlp_modules_c_fc_parameters_weight_ = L_self_modules_transformer_modules_h_modules_7_modules_mlp_modules_c_fc_parameters_weight_
[rank0]:         l_self_modules_transformer_modules_h_modules_7_modules_mlp_modules_c_proj_parameters_weight_ = L_self_modules_transformer_modules_h_modules_7_modules_mlp_modules_c_proj_parameters_weight_
[rank0]:         l_self_modules_transformer_modules_h_modules_8_modules_attn_modules_c_q_parameters_weight_ = L_self_modules_transformer_modules_h_modules_8_modules_attn_modules_c_q_parameters_weight_
[rank0]:         l_self_modules_transformer_modules_h_modules_8_modules_attn_modules_c_k_parameters_weight_ = L_self_modules_transformer_modules_h_modules_8_modules_attn_modules_c_k_parameters_weight_
[rank0]:         l_self_modules_transformer_modules_h_modules_8_modules_attn_modules_c_v_parameters_weight_ = L_self_modules_transformer_modules_h_modules_8_modules_attn_modules_c_v_parameters_weight_
[rank0]:         l_self_modules_transformer_modules_h_modules_8_modules_attn_modules_rotary_cos_cached = L_self_modules_transformer_modules_h_modules_8_modules_attn_modules_rotary_cos_cached
[rank0]:         l_self_modules_transformer_modules_h_modules_8_modules_attn_modules_rotary_sin_cached = L_self_modules_transformer_modules_h_modules_8_modules_attn_modules_rotary_sin_cached
[rank0]:         l_self_modules_transformer_modules_h_modules_8_modules_attn_modules_c_proj_parameters_weight_ = L_self_modules_transformer_modules_h_modules_8_modules_attn_modules_c_proj_parameters_weight_
[rank0]:         l_self_modules_transformer_modules_h_modules_8_modules_mlp_modules_c_fc_parameters_weight_ = L_self_modules_transformer_modules_h_modules_8_modules_mlp_modules_c_fc_parameters_weight_
[rank0]:         l_self_modules_transformer_modules_h_modules_8_modules_mlp_modules_c_proj_parameters_weight_ = L_self_modules_transformer_modules_h_modules_8_modules_mlp_modules_c_proj_parameters_weight_
[rank0]:         l_self_modules_transformer_modules_h_modules_9_modules_attn_modules_c_q_parameters_weight_ = L_self_modules_transformer_modules_h_modules_9_modules_attn_modules_c_q_parameters_weight_
[rank0]:         l_self_modules_transformer_modules_h_modules_9_modules_attn_modules_c_k_parameters_weight_ = L_self_modules_transformer_modules_h_modules_9_modules_attn_modules_c_k_parameters_weight_
[rank0]:         l_self_modules_transformer_modules_h_modules_9_modules_attn_modules_c_v_parameters_weight_ = L_self_modules_transformer_modules_h_modules_9_modules_attn_modules_c_v_parameters_weight_
[rank0]:         l_self_modules_transformer_modules_h_modules_9_modules_attn_modules_rotary_cos_cached = L_self_modules_transformer_modules_h_modules_9_modules_attn_modules_rotary_cos_cached
[rank0]:         l_self_modules_transformer_modules_h_modules_9_modules_attn_modules_rotary_sin_cached = L_self_modules_transformer_modules_h_modules_9_modules_attn_modules_rotary_sin_cached
[rank0]:         l_self_modules_transformer_modules_h_modules_9_modules_attn_modules_c_proj_parameters_weight_ = L_self_modules_transformer_modules_h_modules_9_modules_attn_modules_c_proj_parameters_weight_
[rank0]:         l_self_modules_transformer_modules_h_modules_9_modules_mlp_modules_c_fc_parameters_weight_ = L_self_modules_transformer_modules_h_modules_9_modules_mlp_modules_c_fc_parameters_weight_
[rank0]:         l_self_modules_transformer_modules_h_modules_9_modules_mlp_modules_c_proj_parameters_weight_ = L_self_modules_transformer_modules_h_modules_9_modules_mlp_modules_c_proj_parameters_weight_
[rank0]:         l_self_modules_transformer_modules_h_modules_10_modules_attn_modules_c_q_parameters_weight_ = L_self_modules_transformer_modules_h_modules_10_modules_attn_modules_c_q_parameters_weight_
[rank0]:         l_self_modules_transformer_modules_h_modules_10_modules_attn_modules_c_k_parameters_weight_ = L_self_modules_transformer_modules_h_modules_10_modules_attn_modules_c_k_parameters_weight_
[rank0]:         l_self_modules_transformer_modules_h_modules_10_modules_attn_modules_c_v_parameters_weight_ = L_self_modules_transformer_modules_h_modules_10_modules_attn_modules_c_v_parameters_weight_
[rank0]:         l_self_modules_transformer_modules_h_modules_10_modules_attn_modules_rotary_cos_cached = L_self_modules_transformer_modules_h_modules_10_modules_attn_modules_rotary_cos_cached
[rank0]:         l_self_modules_transformer_modules_h_modules_10_modules_attn_modules_rotary_sin_cached = L_self_modules_transformer_modules_h_modules_10_modules_attn_modules_rotary_sin_cached
[rank0]:         l_self_modules_transformer_modules_h_modules_10_modules_attn_modules_c_proj_parameters_weight_ = L_self_modules_transformer_modules_h_modules_10_modules_attn_modules_c_proj_parameters_weight_
[rank0]:         l_self_modules_transformer_modules_h_modules_10_modules_mlp_modules_c_fc_parameters_weight_ = L_self_modules_transformer_modules_h_modules_10_modules_mlp_modules_c_fc_parameters_weight_
[rank0]:         l_self_modules_transformer_modules_h_modules_10_modules_mlp_modules_c_proj_parameters_weight_ = L_self_modules_transformer_modules_h_modules_10_modules_mlp_modules_c_proj_parameters_weight_
[rank0]:         l_self_modules_transformer_modules_h_modules_11_modules_attn_modules_c_q_parameters_weight_ = L_self_modules_transformer_modules_h_modules_11_modules_attn_modules_c_q_parameters_weight_
[rank0]:         l_self_modules_transformer_modules_h_modules_11_modules_attn_modules_c_k_parameters_weight_ = L_self_modules_transformer_modules_h_modules_11_modules_attn_modules_c_k_parameters_weight_
[rank0]:         l_self_modules_transformer_modules_h_modules_11_modules_attn_modules_c_v_parameters_weight_ = L_self_modules_transformer_modules_h_modules_11_modules_attn_modules_c_v_parameters_weight_
[rank0]:         l_self_modules_transformer_modules_h_modules_11_modules_attn_modules_rotary_cos_cached = L_self_modules_transformer_modules_h_modules_11_modules_attn_modules_rotary_cos_cached
[rank0]:         l_self_modules_transformer_modules_h_modules_11_modules_attn_modules_rotary_sin_cached = L_self_modules_transformer_modules_h_modules_11_modules_attn_modules_rotary_sin_cached
[rank0]:         l_self_modules_transformer_modules_h_modules_11_modules_attn_modules_c_proj_parameters_weight_ = L_self_modules_transformer_modules_h_modules_11_modules_attn_modules_c_proj_parameters_weight_
[rank0]:         l_self_modules_transformer_modules_h_modules_11_modules_mlp_modules_c_fc_parameters_weight_ = L_self_modules_transformer_modules_h_modules_11_modules_mlp_modules_c_fc_parameters_weight_
[rank0]:         l_self_modules_transformer_modules_h_modules_11_modules_mlp_modules_c_proj_parameters_weight_ = L_self_modules_transformer_modules_h_modules_11_modules_mlp_modules_c_proj_parameters_weight_
[rank0]:         l_targets_ = L_targets_
[rank0]:         
[rank0]:         # No stacktrace found for following nodes
[rank0]:         submod_0 = self.submod_0(l_idx_, s20, l_self_modules_transformer_modules_wte_parameters_weight_);  l_idx_ = None
[rank0]:         getitem = submod_0[0]
[rank0]:         getitem_1 = submod_0[1]
[rank0]:         getitem_2 = submod_0[2];  submod_0 = None
[rank0]:         submod_1 = self.submod_1(getitem, l_self_modules_transformer_modules_h_modules_0_modules_attn_modules_c_q_parameters_weight_, getitem_1, l_self_modules_transformer_modules_h_modules_0_modules_attn_modules_c_k_parameters_weight_, l_self_modules_transformer_modules_h_modules_0_modules_attn_modules_c_v_parameters_weight_, l_self_modules_transformer_modules_h_modules_0_modules_attn_modules_rotary_cos_cached, l_self_modules_transformer_modules_h_modules_0_modules_attn_modules_rotary_sin_cached, l_self_modules_transformer_modules_h_modules_0_modules_attn_modules_c_proj_parameters_weight_, getitem_2, l_self_modules_transformer_modules_h_modules_0_modules_mlp_modules_c_fc_parameters_weight_, l_self_modules_transformer_modules_h_modules_0_modules_mlp_modules_c_proj_parameters_weight_);  getitem = l_self_modules_transformer_modules_h_modules_0_modules_attn_modules_c_q_parameters_weight_ = getitem_1 = l_self_modules_transformer_modules_h_modules_0_modules_attn_modules_c_k_parameters_weight_ = l_self_modules_transformer_modules_h_modules_0_modules_attn_modules_c_v_parameters_weight_ = l_self_modules_transformer_modules_h_modules_0_modules_attn_modules_rotary_cos_cached = l_self_modules_transformer_modules_h_modules_0_modules_attn_modules_rotary_sin_cached = l_self_modules_transformer_modules_h_modules_0_modules_attn_modules_c_proj_parameters_weight_ = getitem_2 = l_self_modules_transformer_modules_h_modules_0_modules_mlp_modules_c_fc_parameters_weight_ = l_self_modules_transformer_modules_h_modules_0_modules_mlp_modules_c_proj_parameters_weight_ = None
[rank0]:         getitem_3 = submod_1[0]
[rank0]:         getitem_4 = submod_1[1];  submod_1 = None
[rank0]:         submod_2 = self.submod_2(getitem_3, l_self_modules_transformer_modules_h_modules_1_modules_attn_modules_c_q_parameters_weight_, l_self_modules_transformer_modules_h_modules_1_modules_attn_modules_c_k_parameters_weight_, l_self_modules_transformer_modules_h_modules_1_modules_attn_modules_c_v_parameters_weight_, l_self_modules_transformer_modules_h_modules_1_modules_attn_modules_rotary_cos_cached, l_self_modules_transformer_modules_h_modules_1_modules_attn_modules_rotary_sin_cached, l_self_modules_transformer_modules_h_modules_1_modules_attn_modules_c_proj_parameters_weight_, getitem_4, l_self_modules_transformer_modules_h_modules_1_modules_mlp_modules_c_fc_parameters_weight_, l_self_modules_transformer_modules_h_modules_1_modules_mlp_modules_c_proj_parameters_weight_);  getitem_3 = l_self_modules_transformer_modules_h_modules_1_modules_attn_modules_c_q_parameters_weight_ = l_self_modules_transformer_modules_h_modules_1_modules_attn_modules_c_k_parameters_weight_ = l_self_modules_transformer_modules_h_modules_1_modules_attn_modules_c_v_parameters_weight_ = l_self_modules_transformer_modules_h_modules_1_modules_attn_modules_rotary_cos_cached = l_self_modules_transformer_modules_h_modules_1_modules_attn_modules_rotary_sin_cached = l_self_modules_transformer_modules_h_modules_1_modules_attn_modules_c_proj_parameters_weight_ = getitem_4 = l_self_modules_transformer_modules_h_modules_1_modules_mlp_modules_c_fc_parameters_weight_ = l_self_modules_transformer_modules_h_modules_1_modules_mlp_modules_c_proj_parameters_weight_ = None
[rank0]:         getitem_5 = submod_2[0]
[rank0]:         getitem_6 = submod_2[1];  submod_2 = None
[rank0]:         submod_3 = self.submod_3(getitem_5, l_self_modules_transformer_modules_h_modules_2_modules_attn_modules_c_q_parameters_weight_, l_self_modules_transformer_modules_h_modules_2_modules_attn_modules_c_k_parameters_weight_, l_self_modules_transformer_modules_h_modules_2_modules_attn_modules_c_v_parameters_weight_, l_self_modules_transformer_modules_h_modules_2_modules_attn_modules_rotary_cos_cached, l_self_modules_transformer_modules_h_modules_2_modules_attn_modules_rotary_sin_cached, l_self_modules_transformer_modules_h_modules_2_modules_attn_modules_c_proj_parameters_weight_, getitem_6, l_self_modules_transformer_modules_h_modules_2_modules_mlp_modules_c_fc_parameters_weight_, l_self_modules_transformer_modules_h_modules_2_modules_mlp_modules_c_proj_parameters_weight_);  getitem_5 = l_self_modules_transformer_modules_h_modules_2_modules_attn_modules_c_q_parameters_weight_ = l_self_modules_transformer_modules_h_modules_2_modules_attn_modules_c_k_parameters_weight_ = l_self_modules_transformer_modules_h_modules_2_modules_attn_modules_c_v_parameters_weight_ = l_self_modules_transformer_modules_h_modules_2_modules_attn_modules_rotary_cos_cached = l_self_modules_transformer_modules_h_modules_2_modules_attn_modules_rotary_sin_cached = l_self_modules_transformer_modules_h_modules_2_modules_attn_modules_c_proj_parameters_weight_ = getitem_6 = l_self_modules_transformer_modules_h_modules_2_modules_mlp_modules_c_fc_parameters_weight_ = l_self_modules_transformer_modules_h_modules_2_modules_mlp_modules_c_proj_parameters_weight_ = None
[rank0]:         getitem_7 = submod_3[0]
[rank0]:         getitem_8 = submod_3[1];  submod_3 = None
[rank0]:         submod_4 = self.submod_4(getitem_7, l_self_modules_transformer_modules_h_modules_3_modules_attn_modules_c_q_parameters_weight_, l_self_modules_transformer_modules_h_modules_3_modules_attn_modules_c_k_parameters_weight_, l_self_modules_transformer_modules_h_modules_3_modules_attn_modules_c_v_parameters_weight_, l_self_modules_transformer_modules_h_modules_3_modules_attn_modules_rotary_cos_cached, l_self_modules_transformer_modules_h_modules_3_modules_attn_modules_rotary_sin_cached, l_self_modules_transformer_modules_h_modules_3_modules_attn_modules_c_proj_parameters_weight_, getitem_8, l_self_modules_transformer_modules_h_modules_3_modules_mlp_modules_c_fc_parameters_weight_, l_self_modules_transformer_modules_h_modules_3_modules_mlp_modules_c_proj_parameters_weight_);  getitem_7 = l_self_modules_transformer_modules_h_modules_3_modules_attn_modules_c_q_parameters_weight_ = l_self_modules_transformer_modules_h_modules_3_modules_attn_modules_c_k_parameters_weight_ = l_self_modules_transformer_modules_h_modules_3_modules_attn_modules_c_v_parameters_weight_ = l_self_modules_transformer_modules_h_modules_3_modules_attn_modules_rotary_cos_cached = l_self_modules_transformer_modules_h_modules_3_modules_attn_modules_rotary_sin_cached = l_self_modules_transformer_modules_h_modules_3_modules_attn_modules_c_proj_parameters_weight_ = getitem_8 = l_self_modules_transformer_modules_h_modules_3_modules_mlp_modules_c_fc_parameters_weight_ = l_self_modules_transformer_modules_h_modules_3_modules_mlp_modules_c_proj_parameters_weight_ = None
[rank0]:         getitem_9 = submod_4[0]
[rank0]:         getitem_10 = submod_4[1];  submod_4 = None
[rank0]:         submod_5 = self.submod_5(getitem_9, l_self_modules_transformer_modules_h_modules_4_modules_attn_modules_c_q_parameters_weight_, l_self_modules_transformer_modules_h_modules_4_modules_attn_modules_c_k_parameters_weight_, l_self_modules_transformer_modules_h_modules_4_modules_attn_modules_c_v_parameters_weight_, l_self_modules_transformer_modules_h_modules_4_modules_attn_modules_rotary_cos_cached, l_self_modules_transformer_modules_h_modules_4_modules_attn_modules_rotary_sin_cached, l_self_modules_transformer_modules_h_modules_4_modules_attn_modules_c_proj_parameters_weight_, getitem_10, l_self_modules_transformer_modules_h_modules_4_modules_mlp_modules_c_fc_parameters_weight_, l_self_modules_transformer_modules_h_modules_4_modules_mlp_modules_c_proj_parameters_weight_);  getitem_9 = l_self_modules_transformer_modules_h_modules_4_modules_attn_modules_c_q_parameters_weight_ = l_self_modules_transformer_modules_h_modules_4_modules_attn_modules_c_k_parameters_weight_ = l_self_modules_transformer_modules_h_modules_4_modules_attn_modules_c_v_parameters_weight_ = l_self_modules_transformer_modules_h_modules_4_modules_attn_modules_rotary_cos_cached = l_self_modules_transformer_modules_h_modules_4_modules_attn_modules_rotary_sin_cached = l_self_modules_transformer_modules_h_modules_4_modules_attn_modules_c_proj_parameters_weight_ = getitem_10 = l_self_modules_transformer_modules_h_modules_4_modules_mlp_modules_c_fc_parameters_weight_ = l_self_modules_transformer_modules_h_modules_4_modules_mlp_modules_c_proj_parameters_weight_ = None
[rank0]:         getitem_11 = submod_5[0]
[rank0]:         getitem_12 = submod_5[1];  submod_5 = None
[rank0]:         submod_6 = self.submod_6(getitem_11, l_self_modules_transformer_modules_h_modules_5_modules_attn_modules_c_q_parameters_weight_, l_self_modules_transformer_modules_h_modules_5_modules_attn_modules_c_k_parameters_weight_, l_self_modules_transformer_modules_h_modules_5_modules_attn_modules_c_v_parameters_weight_, l_self_modules_transformer_modules_h_modules_5_modules_attn_modules_rotary_cos_cached, l_self_modules_transformer_modules_h_modules_5_modules_attn_modules_rotary_sin_cached, l_self_modules_transformer_modules_h_modules_5_modules_attn_modules_c_proj_parameters_weight_, getitem_12, l_self_modules_transformer_modules_h_modules_5_modules_mlp_modules_c_fc_parameters_weight_, l_self_modules_transformer_modules_h_modules_5_modules_mlp_modules_c_proj_parameters_weight_);  getitem_11 = l_self_modules_transformer_modules_h_modules_5_modules_attn_modules_c_q_parameters_weight_ = l_self_modules_transformer_modules_h_modules_5_modules_attn_modules_c_k_parameters_weight_ = l_self_modules_transformer_modules_h_modules_5_modules_attn_modules_c_v_parameters_weight_ = l_self_modules_transformer_modules_h_modules_5_modules_attn_modules_rotary_cos_cached = l_self_modules_transformer_modules_h_modules_5_modules_attn_modules_rotary_sin_cached = l_self_modules_transformer_modules_h_modules_5_modules_attn_modules_c_proj_parameters_weight_ = getitem_12 = l_self_modules_transformer_modules_h_modules_5_modules_mlp_modules_c_fc_parameters_weight_ = l_self_modules_transformer_modules_h_modules_5_modules_mlp_modules_c_proj_parameters_weight_ = None
[rank0]:         getitem_13 = submod_6[0]
[rank0]:         getitem_14 = submod_6[1];  submod_6 = None
[rank0]:         submod_7 = self.submod_7(getitem_13, l_self_modules_transformer_modules_h_modules_6_modules_attn_modules_c_q_parameters_weight_, l_self_modules_transformer_modules_h_modules_6_modules_attn_modules_c_k_parameters_weight_, l_self_modules_transformer_modules_h_modules_6_modules_attn_modules_c_v_parameters_weight_, l_self_modules_transformer_modules_h_modules_6_modules_attn_modules_rotary_cos_cached, l_self_modules_transformer_modules_h_modules_6_modules_attn_modules_rotary_sin_cached, l_self_modules_transformer_modules_h_modules_6_modules_attn_modules_c_proj_parameters_weight_, getitem_14, l_self_modules_transformer_modules_h_modules_6_modules_mlp_modules_c_fc_parameters_weight_, l_self_modules_transformer_modules_h_modules_6_modules_mlp_modules_c_proj_parameters_weight_);  getitem_13 = l_self_modules_transformer_modules_h_modules_6_modules_attn_modules_c_q_parameters_weight_ = l_self_modules_transformer_modules_h_modules_6_modules_attn_modules_c_k_parameters_weight_ = l_self_modules_transformer_modules_h_modules_6_modules_attn_modules_c_v_parameters_weight_ = l_self_modules_transformer_modules_h_modules_6_modules_attn_modules_rotary_cos_cached = l_self_modules_transformer_modules_h_modules_6_modules_attn_modules_rotary_sin_cached = l_self_modules_transformer_modules_h_modules_6_modules_attn_modules_c_proj_parameters_weight_ = getitem_14 = l_self_modules_transformer_modules_h_modules_6_modules_mlp_modules_c_fc_parameters_weight_ = l_self_modules_transformer_modules_h_modules_6_modules_mlp_modules_c_proj_parameters_weight_ = None
[rank0]:         getitem_15 = submod_7[0]
[rank0]:         getitem_16 = submod_7[1];  submod_7 = None
[rank0]:         submod_8 = self.submod_8(getitem_15, l_self_modules_transformer_modules_h_modules_7_modules_attn_modules_c_q_parameters_weight_, l_self_modules_transformer_modules_h_modules_7_modules_attn_modules_c_k_parameters_weight_, l_self_modules_transformer_modules_h_modules_7_modules_attn_modules_c_v_parameters_weight_, l_self_modules_transformer_modules_h_modules_7_modules_attn_modules_rotary_cos_cached, l_self_modules_transformer_modules_h_modules_7_modules_attn_modules_rotary_sin_cached, l_self_modules_transformer_modules_h_modules_7_modules_attn_modules_c_proj_parameters_weight_, getitem_16, l_self_modules_transformer_modules_h_modules_7_modules_mlp_modules_c_fc_parameters_weight_, l_self_modules_transformer_modules_h_modules_7_modules_mlp_modules_c_proj_parameters_weight_);  getitem_15 = l_self_modules_transformer_modules_h_modules_7_modules_attn_modules_c_q_parameters_weight_ = l_self_modules_transformer_modules_h_modules_7_modules_attn_modules_c_k_parameters_weight_ = l_self_modules_transformer_modules_h_modules_7_modules_attn_modules_c_v_parameters_weight_ = l_self_modules_transformer_modules_h_modules_7_modules_attn_modules_rotary_cos_cached = l_self_modules_transformer_modules_h_modules_7_modules_attn_modules_rotary_sin_cached = l_self_modules_transformer_modules_h_modules_7_modules_attn_modules_c_proj_parameters_weight_ = getitem_16 = l_self_modules_transformer_modules_h_modules_7_modules_mlp_modules_c_fc_parameters_weight_ = l_self_modules_transformer_modules_h_modules_7_modules_mlp_modules_c_proj_parameters_weight_ = None
[rank0]:         getitem_17 = submod_8[0]
[rank0]:         getitem_18 = submod_8[1];  submod_8 = None
[rank0]:         submod_9 = self.submod_9(getitem_17, l_self_modules_transformer_modules_h_modules_8_modules_attn_modules_c_q_parameters_weight_, l_self_modules_transformer_modules_h_modules_8_modules_attn_modules_c_k_parameters_weight_, l_self_modules_transformer_modules_h_modules_8_modules_attn_modules_c_v_parameters_weight_, l_self_modules_transformer_modules_h_modules_8_modules_attn_modules_rotary_cos_cached, l_self_modules_transformer_modules_h_modules_8_modules_attn_modules_rotary_sin_cached, l_self_modules_transformer_modules_h_modules_8_modules_attn_modules_c_proj_parameters_weight_, getitem_18, l_self_modules_transformer_modules_h_modules_8_modules_mlp_modules_c_fc_parameters_weight_, l_self_modules_transformer_modules_h_modules_8_modules_mlp_modules_c_proj_parameters_weight_);  getitem_17 = l_self_modules_transformer_modules_h_modules_8_modules_attn_modules_c_q_parameters_weight_ = l_self_modules_transformer_modules_h_modules_8_modules_attn_modules_c_k_parameters_weight_ = l_self_modules_transformer_modules_h_modules_8_modules_attn_modules_c_v_parameters_weight_ = l_self_modules_transformer_modules_h_modules_8_modules_attn_modules_rotary_cos_cached = l_self_modules_transformer_modules_h_modules_8_modules_attn_modules_rotary_sin_cached = l_self_modules_transformer_modules_h_modules_8_modules_attn_modules_c_proj_parameters_weight_ = getitem_18 = l_self_modules_transformer_modules_h_modules_8_modules_mlp_modules_c_fc_parameters_weight_ = l_self_modules_transformer_modules_h_modules_8_modules_mlp_modules_c_proj_parameters_weight_ = None
[rank0]:         getitem_19 = submod_9[0]
[rank0]:         getitem_20 = submod_9[1];  submod_9 = None
[rank0]:         submod_10 = self.submod_10(getitem_19, l_self_modules_transformer_modules_h_modules_9_modules_attn_modules_c_q_parameters_weight_, l_self_modules_transformer_modules_h_modules_9_modules_attn_modules_c_k_parameters_weight_, l_self_modules_transformer_modules_h_modules_9_modules_attn_modules_c_v_parameters_weight_, l_self_modules_transformer_modules_h_modules_9_modules_attn_modules_rotary_cos_cached, l_self_modules_transformer_modules_h_modules_9_modules_attn_modules_rotary_sin_cached, l_self_modules_transformer_modules_h_modules_9_modules_attn_modules_c_proj_parameters_weight_, getitem_20, l_self_modules_transformer_modules_h_modules_9_modules_mlp_modules_c_fc_parameters_weight_, l_self_modules_transformer_modules_h_modules_9_modules_mlp_modules_c_proj_parameters_weight_);  getitem_19 = l_self_modules_transformer_modules_h_modules_9_modules_attn_modules_c_q_parameters_weight_ = l_self_modules_transformer_modules_h_modules_9_modules_attn_modules_c_k_parameters_weight_ = l_self_modules_transformer_modules_h_modules_9_modules_attn_modules_c_v_parameters_weight_ = l_self_modules_transformer_modules_h_modules_9_modules_attn_modules_rotary_cos_cached = l_self_modules_transformer_modules_h_modules_9_modules_attn_modules_rotary_sin_cached = l_self_modules_transformer_modules_h_modules_9_modules_attn_modules_c_proj_parameters_weight_ = getitem_20 = l_self_modules_transformer_modules_h_modules_9_modules_mlp_modules_c_fc_parameters_weight_ = l_self_modules_transformer_modules_h_modules_9_modules_mlp_modules_c_proj_parameters_weight_ = None
[rank0]:         getitem_21 = submod_10[0]
[rank0]:         getitem_22 = submod_10[1];  submod_10 = None
[rank0]:         submod_11 = self.submod_11(getitem_21, l_self_modules_transformer_modules_h_modules_10_modules_attn_modules_c_q_parameters_weight_, l_self_modules_transformer_modules_h_modules_10_modules_attn_modules_c_k_parameters_weight_, l_self_modules_transformer_modules_h_modules_10_modules_attn_modules_c_v_parameters_weight_, l_self_modules_transformer_modules_h_modules_10_modules_attn_modules_rotary_cos_cached, l_self_modules_transformer_modules_h_modules_10_modules_attn_modules_rotary_sin_cached, l_self_modules_transformer_modules_h_modules_10_modules_attn_modules_c_proj_parameters_weight_, getitem_22, l_self_modules_transformer_modules_h_modules_10_modules_mlp_modules_c_fc_parameters_weight_, l_self_modules_transformer_modules_h_modules_10_modules_mlp_modules_c_proj_parameters_weight_);  getitem_21 = l_self_modules_transformer_modules_h_modules_10_modules_attn_modules_c_q_parameters_weight_ = l_self_modules_transformer_modules_h_modules_10_modules_attn_modules_c_k_parameters_weight_ = l_self_modules_transformer_modules_h_modules_10_modules_attn_modules_c_v_parameters_weight_ = l_self_modules_transformer_modules_h_modules_10_modules_attn_modules_rotary_cos_cached = l_self_modules_transformer_modules_h_modules_10_modules_attn_modules_rotary_sin_cached = l_self_modules_transformer_modules_h_modules_10_modules_attn_modules_c_proj_parameters_weight_ = getitem_22 = l_self_modules_transformer_modules_h_modules_10_modules_mlp_modules_c_fc_parameters_weight_ = l_self_modules_transformer_modules_h_modules_10_modules_mlp_modules_c_proj_parameters_weight_ = None
[rank0]:         getitem_23 = submod_11[0]
[rank0]:         getitem_24 = submod_11[1];  submod_11 = None
[rank0]:         submod_12 = self.submod_12(getitem_23, l_self_modules_transformer_modules_h_modules_11_modules_attn_modules_c_q_parameters_weight_, l_self_modules_transformer_modules_h_modules_11_modules_attn_modules_c_k_parameters_weight_, l_self_modules_transformer_modules_h_modules_11_modules_attn_modules_c_v_parameters_weight_, l_self_modules_transformer_modules_h_modules_11_modules_attn_modules_rotary_cos_cached, l_self_modules_transformer_modules_h_modules_11_modules_attn_modules_rotary_sin_cached, l_self_modules_transformer_modules_h_modules_11_modules_attn_modules_c_proj_parameters_weight_, getitem_24, l_self_modules_transformer_modules_h_modules_11_modules_mlp_modules_c_fc_parameters_weight_, l_self_modules_transformer_modules_h_modules_11_modules_mlp_modules_c_proj_parameters_weight_);  getitem_23 = l_self_modules_transformer_modules_h_modules_11_modules_attn_modules_c_q_parameters_weight_ = l_self_modules_transformer_modules_h_modules_11_modules_attn_modules_c_k_parameters_weight_ = l_self_modules_transformer_modules_h_modules_11_modules_attn_modules_c_v_parameters_weight_ = l_self_modules_transformer_modules_h_modules_11_modules_attn_modules_rotary_cos_cached = l_self_modules_transformer_modules_h_modules_11_modules_attn_modules_rotary_sin_cached = l_self_modules_transformer_modules_h_modules_11_modules_attn_modules_c_proj_parameters_weight_ = getitem_24 = l_self_modules_transformer_modules_h_modules_11_modules_mlp_modules_c_fc_parameters_weight_ = l_self_modules_transformer_modules_h_modules_11_modules_mlp_modules_c_proj_parameters_weight_ = None
[rank0]:         submod_13 = self.submod_13(submod_12, l_self_modules_transformer_modules_wte_parameters_weight_, l_targets_, s20);  submod_12 = l_self_modules_transformer_modules_wte_parameters_weight_ = l_targets_ = s20 = None
[rank0]:         return (submod_13,)
[rank0]:         
[rank0]:     class submod_0(torch.nn.Module):
[rank0]:         def forward(self, l_idx_: "i64[16, 256][s20, 1]", s20: "Sym(s20)", l_self_modules_transformer_modules_wte_parameters_weight_: "f32[50304, 768][768, 1]"):
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:221 in forward, code: x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)
[rank0]:             embedding: "f32[16, 256, 768][196608, 768, 1]" = torch.nn.functional.embedding(l_idx_, l_self_modules_transformer_modules_wte_parameters_weight_, None, None, 2.0, False, False);  l_idx_ = l_self_modules_transformer_modules_wte_parameters_weight_ = None
[rank0]:             
[rank0]:              # File: /usr/local/lib/python3.12/dist-packages/torch/nn/functional.py:2924 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
[rank0]:             rms_norm: "f32[16, 256, 768][196608, 768, 1]" = torch.rms_norm(embedding, (768,), None, None)
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:157 in forward, code: B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)
[rank0]:             size = rms_norm.size()
[rank0]:             getitem: "Sym(256)" = size[1];  size = None
[rank0]:             return (rms_norm, getitem, embedding)
[rank0]:             
[rank0]:     class submod_1(torch.nn.Module):
[rank0]:         def forward(self, rms_norm: "f32[16, 256, 768][196608, 768, 1]", l_self_modules_transformer_modules_h_modules_0_modules_attn_modules_c_q_parameters_weight_: "f32[768, 768][768, 1]", getitem_1: "Sym(256)", l_self_modules_transformer_modules_h_modules_0_modules_attn_modules_c_k_parameters_weight_: "f32[768, 768][768, 1]", l_self_modules_transformer_modules_h_modules_0_modules_attn_modules_c_v_parameters_weight_: "f32[768, 768][768, 1]", l_self_modules_transformer_modules_h_modules_0_modules_attn_modules_rotary_cos_cached: "bf16[256, 64][64, 1]", l_self_modules_transformer_modules_h_modules_0_modules_attn_modules_rotary_sin_cached: "bf16[256, 64][64, 1]", l_self_modules_transformer_modules_h_modules_0_modules_attn_modules_c_proj_parameters_weight_: "f32[768, 768][768, 1]", x: "f32[16, 256, 768][196608, 768, 1]", l_self_modules_transformer_modules_h_modules_0_modules_mlp_modules_c_fc_parameters_weight_: "f32[3072, 768][768, 1]", l_self_modules_transformer_modules_h_modules_0_modules_mlp_modules_c_proj_parameters_weight_: "f32[768, 3072][3072, 1]"):
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:158 in forward, code: q = self.c_q(x).view(B, T, self.n_head, self.head_dim)
[rank0]:             linear: "bf16[16, 256, 768][196608, 768, 1]" = torch._C._nn.linear(rms_norm, l_self_modules_transformer_modules_h_modules_0_modules_attn_modules_c_q_parameters_weight_, None);  l_self_modules_transformer_modules_h_modules_0_modules_attn_modules_c_q_parameters_weight_ = None
[rank0]:             view: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = linear.view(16, getitem_1, 6, 128);  linear = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:159 in forward, code: k = self.c_k(x).view(B, T, self.n_head, self.head_dim)
[rank0]:             linear_1: "bf16[16, 256, 768][196608, 768, 1]" = torch._C._nn.linear(rms_norm, l_self_modules_transformer_modules_h_modules_0_modules_attn_modules_c_k_parameters_weight_, None);  l_self_modules_transformer_modules_h_modules_0_modules_attn_modules_c_k_parameters_weight_ = None
[rank0]:             view_1: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = linear_1.view(16, getitem_1, 6, 128);  linear_1 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:160 in forward, code: v = self.c_v(x).view(B, T, self.n_head, self.head_dim)
[rank0]:             linear_2: "bf16[16, 256, 768][196608, 768, 1]" = torch._C._nn.linear(rms_norm, l_self_modules_transformer_modules_h_modules_0_modules_attn_modules_c_v_parameters_weight_, None);  l_self_modules_transformer_modules_h_modules_0_modules_attn_modules_c_v_parameters_weight_ = None
[rank0]:             view_2: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = linear_2.view(16, getitem_1, 6, 128);  linear_2 = getitem_1 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:129 in forward, code: return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]
[rank0]:             getitem: "bf16[1, 256, 1, 64][16384, 64, 64, 1]" = l_self_modules_transformer_modules_h_modules_0_modules_attn_modules_rotary_cos_cached[(None, slice(None, None, None), None, slice(None, None, None))];  l_self_modules_transformer_modules_h_modules_0_modules_attn_modules_rotary_cos_cached = None
[rank0]:             getitem_2: "bf16[1, 256, 1, 64][16384, 64, 64, 1]" = l_self_modules_transformer_modules_h_modules_0_modules_attn_modules_rotary_sin_cached[(None, slice(None, None, None), None, slice(None, None, None))];  l_self_modules_transformer_modules_h_modules_0_modules_attn_modules_rotary_sin_cached = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:134 in apply_rotary_emb, code: x1 = x[..., :d]
[rank0]:             getitem_3: "bf16[16, 256, 6, 64][196608, 768, 128, 1]" = view[(Ellipsis, slice(None, 64, None))]
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:135 in apply_rotary_emb, code: x2 = x[..., d:]
[rank0]:             getitem_4: "bf16[16, 256, 6, 64][196608, 768, 128, 1]" = view[(Ellipsis, slice(64, None, None))]
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:136 in apply_rotary_emb, code: y1 = x1 * cos + x2 * sin
[rank0]:             mul: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = getitem_3 * getitem
[rank0]:             mul_1: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = getitem_4 * getitem_2
[rank0]:             add: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = mul + mul_1;  mul = mul_1 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:137 in apply_rotary_emb, code: y2 = x1 * (-sin) + x2 * cos
[rank0]:             neg: "bf16[1, 256, 1, 64][16384, 64, 64, 1]" = -getitem_2
[rank0]:             mul_2: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = getitem_3 * neg;  getitem_3 = neg = None
[rank0]:             mul_3: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = getitem_4 * getitem;  getitem_4 = None
[rank0]:             add_1: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = mul_2 + mul_3;  mul_2 = mul_3 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:138 in apply_rotary_emb, code: return torch.cat([y1, y2], 3).type_as(x)
[rank0]:             cat: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = torch.cat([add, add_1], 3);  add = add_1 = None
[rank0]:             type_as: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = cat.type_as(view);  cat = view = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:134 in apply_rotary_emb, code: x1 = x[..., :d]
[rank0]:             getitem_5: "bf16[16, 256, 6, 64][196608, 768, 128, 1]" = view_1[(Ellipsis, slice(None, 64, None))]
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:135 in apply_rotary_emb, code: x2 = x[..., d:]
[rank0]:             getitem_6: "bf16[16, 256, 6, 64][196608, 768, 128, 1]" = view_1[(Ellipsis, slice(64, None, None))]
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:136 in apply_rotary_emb, code: y1 = x1 * cos + x2 * sin
[rank0]:             mul_4: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = getitem_5 * getitem
[rank0]:             mul_5: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = getitem_6 * getitem_2
[rank0]:             add_2: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = mul_4 + mul_5;  mul_4 = mul_5 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:137 in apply_rotary_emb, code: y2 = x1 * (-sin) + x2 * cos
[rank0]:             neg_1: "bf16[1, 256, 1, 64][16384, 64, 64, 1]" = -getitem_2;  getitem_2 = None
[rank0]:             mul_6: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = getitem_5 * neg_1;  getitem_5 = neg_1 = None
[rank0]:             mul_7: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = getitem_6 * getitem;  getitem_6 = getitem = None
[rank0]:             add_3: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = mul_6 + mul_7;  mul_6 = mul_7 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:138 in apply_rotary_emb, code: return torch.cat([y1, y2], 3).type_as(x)
[rank0]:             cat_1: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = torch.cat([add_2, add_3], 3);  add_2 = add_3 = None
[rank0]:             type_as_1: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = cat_1.type_as(view_1);  cat_1 = view_1 = None
[rank0]:             
[rank0]:              # File: /usr/local/lib/python3.12/dist-packages/torch/nn/functional.py:2924 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
[rank0]:             rms_norm_1: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = torch.rms_norm(type_as, (128,), None, None);  type_as = None
[rank0]:             rms_norm_2: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = torch.rms_norm(type_as_1, (128,), None, None);  type_as_1 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:164 in forward, code: y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)
[rank0]:             transpose: "bf16[16, 6, 256, 128][196608, 128, 768, 1]" = rms_norm_1.transpose(1, 2);  rms_norm_1 = None
[rank0]:             transpose_1: "bf16[16, 6, 256, 128][196608, 128, 768, 1]" = rms_norm_2.transpose(1, 2);  rms_norm_2 = None
[rank0]:             transpose_2: "bf16[16, 6, 256, 128][196608, 128, 768, 1]" = view_2.transpose(1, 2);  view_2 = None
[rank0]:             scaled_dot_product_attention: "bf16[16, 6, 256, 128][196608, 128, 768, 1]" = torch._C._nn.scaled_dot_product_attention(transpose, transpose_1, transpose_2, is_causal = True);  transpose = transpose_1 = transpose_2 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:165 in forward, code: y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
[rank0]:             transpose_3: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = scaled_dot_product_attention.transpose(1, 2);  scaled_dot_product_attention = None
[rank0]:             contiguous: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = transpose_3.contiguous();  transpose_3 = None
[rank0]:             view_as: "bf16[16, 256, 768][196608, 768, 1]" = contiguous.view_as(rms_norm);  contiguous = rms_norm = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:166 in forward, code: y = self.c_proj(y)
[rank0]:             linear_3: "bf16[16, 256, 768][196608, 768, 1]" = torch._C._nn.linear(view_as, l_self_modules_transformer_modules_h_modules_0_modules_attn_modules_c_proj_parameters_weight_, None);  view_as = l_self_modules_transformer_modules_h_modules_0_modules_attn_modules_c_proj_parameters_weight_ = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:191 in forward, code: x = x + self.attn(F.rms_norm(x, (x.size(-1),)))
[rank0]:             add_4: "f32[16, 256, 768][196608, 768, 1]" = x + linear_3;  x = linear_3 = None
[rank0]:             
[rank0]:              # File: /usr/local/lib/python3.12/dist-packages/torch/nn/functional.py:2924 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
[rank0]:             rms_norm_3: "f32[16, 256, 768][196608, 768, 1]" = torch.rms_norm(add_4, (768,), None, None)
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:178 in forward, code: x = self.c_fc(x)
[rank0]:             linear_4: "bf16[16, 256, 3072][786432, 3072, 1]" = torch._C._nn.linear(rms_norm_3, l_self_modules_transformer_modules_h_modules_0_modules_mlp_modules_c_fc_parameters_weight_, None);  rms_norm_3 = l_self_modules_transformer_modules_h_modules_0_modules_mlp_modules_c_fc_parameters_weight_ = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:179 in forward, code: x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
[rank0]:             relu: "bf16[16, 256, 3072][786432, 3072, 1]" = torch.nn.functional.relu(linear_4);  linear_4 = None
[rank0]:             square: "f32[16, 256, 3072][786432, 3072, 1]" = relu.square();  relu = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:180 in forward, code: x = self.c_proj(x)
[rank0]:             linear_5: "bf16[16, 256, 768][196608, 768, 1]" = torch._C._nn.linear(square, l_self_modules_transformer_modules_h_modules_0_modules_mlp_modules_c_proj_parameters_weight_, None);  square = l_self_modules_transformer_modules_h_modules_0_modules_mlp_modules_c_proj_parameters_weight_ = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:192 in forward, code: x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))
[rank0]:             add_5: "f32[16, 256, 768][196608, 768, 1]" = add_4 + linear_5;  add_4 = linear_5 = None
[rank0]:             
[rank0]:              # File: /usr/local/lib/python3.12/dist-packages/torch/nn/functional.py:2924 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
[rank0]:             rms_norm_4: "f32[16, 256, 768][196608, 768, 1]" = torch.rms_norm(add_5, (768,), None, None)
[rank0]:             return (rms_norm_4, add_5)
[rank0]:             
[rank0]:     class submod_2(torch.nn.Module):
[rank0]:         def forward(self, rms_norm_4: "f32[16, 256, 768][196608, 768, 1]", l_self_modules_transformer_modules_h_modules_1_modules_attn_modules_c_q_parameters_weight_: "f32[768, 768][768, 1]", l_self_modules_transformer_modules_h_modules_1_modules_attn_modules_c_k_parameters_weight_: "f32[768, 768][768, 1]", l_self_modules_transformer_modules_h_modules_1_modules_attn_modules_c_v_parameters_weight_: "f32[768, 768][768, 1]", l_self_modules_transformer_modules_h_modules_1_modules_attn_modules_rotary_cos_cached: "bf16[256, 64][64, 1]", l_self_modules_transformer_modules_h_modules_1_modules_attn_modules_rotary_sin_cached: "bf16[256, 64][64, 1]", l_self_modules_transformer_modules_h_modules_1_modules_attn_modules_c_proj_parameters_weight_: "f32[768, 768][768, 1]", x_5: "f32[16, 256, 768][196608, 768, 1]", l_self_modules_transformer_modules_h_modules_1_modules_mlp_modules_c_fc_parameters_weight_: "f32[3072, 768][768, 1]", l_self_modules_transformer_modules_h_modules_1_modules_mlp_modules_c_proj_parameters_weight_: "f32[768, 3072][3072, 1]"):
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:158 in forward, code: q = self.c_q(x).view(B, T, self.n_head, self.head_dim)
[rank0]:             linear: "bf16[16, 256, 768][196608, 768, 1]" = torch._C._nn.linear(rms_norm_4, l_self_modules_transformer_modules_h_modules_1_modules_attn_modules_c_q_parameters_weight_, None);  l_self_modules_transformer_modules_h_modules_1_modules_attn_modules_c_q_parameters_weight_ = None
[rank0]:             view: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = linear.view(16, 256, 6, 128);  linear = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:159 in forward, code: k = self.c_k(x).view(B, T, self.n_head, self.head_dim)
[rank0]:             linear_1: "bf16[16, 256, 768][196608, 768, 1]" = torch._C._nn.linear(rms_norm_4, l_self_modules_transformer_modules_h_modules_1_modules_attn_modules_c_k_parameters_weight_, None);  l_self_modules_transformer_modules_h_modules_1_modules_attn_modules_c_k_parameters_weight_ = None
[rank0]:             view_1: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = linear_1.view(16, 256, 6, 128);  linear_1 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:160 in forward, code: v = self.c_v(x).view(B, T, self.n_head, self.head_dim)
[rank0]:             linear_2: "bf16[16, 256, 768][196608, 768, 1]" = torch._C._nn.linear(rms_norm_4, l_self_modules_transformer_modules_h_modules_1_modules_attn_modules_c_v_parameters_weight_, None);  l_self_modules_transformer_modules_h_modules_1_modules_attn_modules_c_v_parameters_weight_ = None
[rank0]:             view_2: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = linear_2.view(16, 256, 6, 128);  linear_2 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:129 in forward, code: return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]
[rank0]:             getitem: "bf16[1, 256, 1, 64][16384, 64, 64, 1]" = l_self_modules_transformer_modules_h_modules_1_modules_attn_modules_rotary_cos_cached[(None, slice(None, None, None), None, slice(None, None, None))];  l_self_modules_transformer_modules_h_modules_1_modules_attn_modules_rotary_cos_cached = None
[rank0]:             getitem_1: "bf16[1, 256, 1, 64][16384, 64, 64, 1]" = l_self_modules_transformer_modules_h_modules_1_modules_attn_modules_rotary_sin_cached[(None, slice(None, None, None), None, slice(None, None, None))];  l_self_modules_transformer_modules_h_modules_1_modules_attn_modules_rotary_sin_cached = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:134 in apply_rotary_emb, code: x1 = x[..., :d]
[rank0]:             getitem_2: "bf16[16, 256, 6, 64][196608, 768, 128, 1]" = view[(Ellipsis, slice(None, 64, None))]
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:135 in apply_rotary_emb, code: x2 = x[..., d:]
[rank0]:             getitem_3: "bf16[16, 256, 6, 64][196608, 768, 128, 1]" = view[(Ellipsis, slice(64, None, None))]
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:136 in apply_rotary_emb, code: y1 = x1 * cos + x2 * sin
[rank0]:             mul: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = getitem_2 * getitem
[rank0]:             mul_1: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = getitem_3 * getitem_1
[rank0]:             add: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = mul + mul_1;  mul = mul_1 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:137 in apply_rotary_emb, code: y2 = x1 * (-sin) + x2 * cos
[rank0]:             neg: "bf16[1, 256, 1, 64][16384, 64, 64, 1]" = -getitem_1
[rank0]:             mul_2: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = getitem_2 * neg;  getitem_2 = neg = None
[rank0]:             mul_3: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = getitem_3 * getitem;  getitem_3 = None
[rank0]:             add_1: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = mul_2 + mul_3;  mul_2 = mul_3 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:138 in apply_rotary_emb, code: return torch.cat([y1, y2], 3).type_as(x)
[rank0]:             cat: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = torch.cat([add, add_1], 3);  add = add_1 = None
[rank0]:             type_as: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = cat.type_as(view);  cat = view = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:134 in apply_rotary_emb, code: x1 = x[..., :d]
[rank0]:             getitem_4: "bf16[16, 256, 6, 64][196608, 768, 128, 1]" = view_1[(Ellipsis, slice(None, 64, None))]
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:135 in apply_rotary_emb, code: x2 = x[..., d:]
[rank0]:             getitem_5: "bf16[16, 256, 6, 64][196608, 768, 128, 1]" = view_1[(Ellipsis, slice(64, None, None))]
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:136 in apply_rotary_emb, code: y1 = x1 * cos + x2 * sin
[rank0]:             mul_4: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = getitem_4 * getitem
[rank0]:             mul_5: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = getitem_5 * getitem_1
[rank0]:             add_2: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = mul_4 + mul_5;  mul_4 = mul_5 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:137 in apply_rotary_emb, code: y2 = x1 * (-sin) + x2 * cos
[rank0]:             neg_1: "bf16[1, 256, 1, 64][16384, 64, 64, 1]" = -getitem_1;  getitem_1 = None
[rank0]:             mul_6: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = getitem_4 * neg_1;  getitem_4 = neg_1 = None
[rank0]:             mul_7: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = getitem_5 * getitem;  getitem_5 = getitem = None
[rank0]:             add_3: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = mul_6 + mul_7;  mul_6 = mul_7 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:138 in apply_rotary_emb, code: return torch.cat([y1, y2], 3).type_as(x)
[rank0]:             cat_1: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = torch.cat([add_2, add_3], 3);  add_2 = add_3 = None
[rank0]:             type_as_1: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = cat_1.type_as(view_1);  cat_1 = view_1 = None
[rank0]:             
[rank0]:              # File: /usr/local/lib/python3.12/dist-packages/torch/nn/functional.py:2924 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
[rank0]:             rms_norm: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = torch.rms_norm(type_as, (128,), None, None);  type_as = None
[rank0]:             rms_norm_5: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = torch.rms_norm(type_as_1, (128,), None, None);  type_as_1 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:164 in forward, code: y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)
[rank0]:             transpose: "bf16[16, 6, 256, 128][196608, 128, 768, 1]" = rms_norm.transpose(1, 2);  rms_norm = None
[rank0]:             transpose_1: "bf16[16, 6, 256, 128][196608, 128, 768, 1]" = rms_norm_5.transpose(1, 2);  rms_norm_5 = None
[rank0]:             transpose_2: "bf16[16, 6, 256, 128][196608, 128, 768, 1]" = view_2.transpose(1, 2);  view_2 = None
[rank0]:             scaled_dot_product_attention: "bf16[16, 6, 256, 128][196608, 128, 768, 1]" = torch._C._nn.scaled_dot_product_attention(transpose, transpose_1, transpose_2, is_causal = True);  transpose = transpose_1 = transpose_2 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:165 in forward, code: y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
[rank0]:             transpose_3: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = scaled_dot_product_attention.transpose(1, 2);  scaled_dot_product_attention = None
[rank0]:             contiguous: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = transpose_3.contiguous();  transpose_3 = None
[rank0]:             view_as: "bf16[16, 256, 768][196608, 768, 1]" = contiguous.view_as(rms_norm_4);  contiguous = rms_norm_4 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:166 in forward, code: y = self.c_proj(y)
[rank0]:             linear_3: "bf16[16, 256, 768][196608, 768, 1]" = torch._C._nn.linear(view_as, l_self_modules_transformer_modules_h_modules_1_modules_attn_modules_c_proj_parameters_weight_, None);  view_as = l_self_modules_transformer_modules_h_modules_1_modules_attn_modules_c_proj_parameters_weight_ = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:191 in forward, code: x = x + self.attn(F.rms_norm(x, (x.size(-1),)))
[rank0]:             add_4: "f32[16, 256, 768][196608, 768, 1]" = x_5 + linear_3;  x_5 = linear_3 = None
[rank0]:             
[rank0]:              # File: /usr/local/lib/python3.12/dist-packages/torch/nn/functional.py:2924 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
[rank0]:             rms_norm_6: "f32[16, 256, 768][196608, 768, 1]" = torch.rms_norm(add_4, (768,), None, None)
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:178 in forward, code: x = self.c_fc(x)
[rank0]:             linear_4: "bf16[16, 256, 3072][786432, 3072, 1]" = torch._C._nn.linear(rms_norm_6, l_self_modules_transformer_modules_h_modules_1_modules_mlp_modules_c_fc_parameters_weight_, None);  rms_norm_6 = l_self_modules_transformer_modules_h_modules_1_modules_mlp_modules_c_fc_parameters_weight_ = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:179 in forward, code: x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
[rank0]:             relu: "bf16[16, 256, 3072][786432, 3072, 1]" = torch.nn.functional.relu(linear_4);  linear_4 = None
[rank0]:             square: "f32[16, 256, 3072][786432, 3072, 1]" = relu.square();  relu = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:180 in forward, code: x = self.c_proj(x)
[rank0]:             linear_5: "bf16[16, 256, 768][196608, 768, 1]" = torch._C._nn.linear(square, l_self_modules_transformer_modules_h_modules_1_modules_mlp_modules_c_proj_parameters_weight_, None);  square = l_self_modules_transformer_modules_h_modules_1_modules_mlp_modules_c_proj_parameters_weight_ = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:192 in forward, code: x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))
[rank0]:             add_5: "f32[16, 256, 768][196608, 768, 1]" = add_4 + linear_5;  add_4 = linear_5 = None
[rank0]:             
[rank0]:              # File: /usr/local/lib/python3.12/dist-packages/torch/nn/functional.py:2924 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
[rank0]:             rms_norm_7: "f32[16, 256, 768][196608, 768, 1]" = torch.rms_norm(add_5, (768,), None, None)
[rank0]:             return (rms_norm_7, add_5)
[rank0]:             
[rank0]:     class submod_3(torch.nn.Module):
[rank0]:         def forward(self, rms_norm_8: "f32[16, 256, 768][196608, 768, 1]", l_self_modules_transformer_modules_h_modules_2_modules_attn_modules_c_q_parameters_weight_: "f32[768, 768][768, 1]", l_self_modules_transformer_modules_h_modules_2_modules_attn_modules_c_k_parameters_weight_: "f32[768, 768][768, 1]", l_self_modules_transformer_modules_h_modules_2_modules_attn_modules_c_v_parameters_weight_: "f32[768, 768][768, 1]", l_self_modules_transformer_modules_h_modules_2_modules_attn_modules_rotary_cos_cached: "bf16[256, 64][64, 1]", l_self_modules_transformer_modules_h_modules_2_modules_attn_modules_rotary_sin_cached: "bf16[256, 64][64, 1]", l_self_modules_transformer_modules_h_modules_2_modules_attn_modules_c_proj_parameters_weight_: "f32[768, 768][768, 1]", x_10: "f32[16, 256, 768][196608, 768, 1]", l_self_modules_transformer_modules_h_modules_2_modules_mlp_modules_c_fc_parameters_weight_: "f32[3072, 768][768, 1]", l_self_modules_transformer_modules_h_modules_2_modules_mlp_modules_c_proj_parameters_weight_: "f32[768, 3072][3072, 1]"):
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:158 in forward, code: q = self.c_q(x).view(B, T, self.n_head, self.head_dim)
[rank0]:             linear: "bf16[16, 256, 768][196608, 768, 1]" = torch._C._nn.linear(rms_norm_8, l_self_modules_transformer_modules_h_modules_2_modules_attn_modules_c_q_parameters_weight_, None);  l_self_modules_transformer_modules_h_modules_2_modules_attn_modules_c_q_parameters_weight_ = None
[rank0]:             view: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = linear.view(16, 256, 6, 128);  linear = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:159 in forward, code: k = self.c_k(x).view(B, T, self.n_head, self.head_dim)
[rank0]:             linear_1: "bf16[16, 256, 768][196608, 768, 1]" = torch._C._nn.linear(rms_norm_8, l_self_modules_transformer_modules_h_modules_2_modules_attn_modules_c_k_parameters_weight_, None);  l_self_modules_transformer_modules_h_modules_2_modules_attn_modules_c_k_parameters_weight_ = None
[rank0]:             view_1: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = linear_1.view(16, 256, 6, 128);  linear_1 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:160 in forward, code: v = self.c_v(x).view(B, T, self.n_head, self.head_dim)
[rank0]:             linear_2: "bf16[16, 256, 768][196608, 768, 1]" = torch._C._nn.linear(rms_norm_8, l_self_modules_transformer_modules_h_modules_2_modules_attn_modules_c_v_parameters_weight_, None);  l_self_modules_transformer_modules_h_modules_2_modules_attn_modules_c_v_parameters_weight_ = None
[rank0]:             view_2: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = linear_2.view(16, 256, 6, 128);  linear_2 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:129 in forward, code: return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]
[rank0]:             getitem: "bf16[1, 256, 1, 64][16384, 64, 64, 1]" = l_self_modules_transformer_modules_h_modules_2_modules_attn_modules_rotary_cos_cached[(None, slice(None, None, None), None, slice(None, None, None))];  l_self_modules_transformer_modules_h_modules_2_modules_attn_modules_rotary_cos_cached = None
[rank0]:             getitem_1: "bf16[1, 256, 1, 64][16384, 64, 64, 1]" = l_self_modules_transformer_modules_h_modules_2_modules_attn_modules_rotary_sin_cached[(None, slice(None, None, None), None, slice(None, None, None))];  l_self_modules_transformer_modules_h_modules_2_modules_attn_modules_rotary_sin_cached = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:134 in apply_rotary_emb, code: x1 = x[..., :d]
[rank0]:             getitem_2: "bf16[16, 256, 6, 64][196608, 768, 128, 1]" = view[(Ellipsis, slice(None, 64, None))]
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:135 in apply_rotary_emb, code: x2 = x[..., d:]
[rank0]:             getitem_3: "bf16[16, 256, 6, 64][196608, 768, 128, 1]" = view[(Ellipsis, slice(64, None, None))]
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:136 in apply_rotary_emb, code: y1 = x1 * cos + x2 * sin
[rank0]:             mul: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = getitem_2 * getitem
[rank0]:             mul_1: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = getitem_3 * getitem_1
[rank0]:             add: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = mul + mul_1;  mul = mul_1 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:137 in apply_rotary_emb, code: y2 = x1 * (-sin) + x2 * cos
[rank0]:             neg: "bf16[1, 256, 1, 64][16384, 64, 64, 1]" = -getitem_1
[rank0]:             mul_2: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = getitem_2 * neg;  getitem_2 = neg = None
[rank0]:             mul_3: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = getitem_3 * getitem;  getitem_3 = None
[rank0]:             add_1: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = mul_2 + mul_3;  mul_2 = mul_3 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:138 in apply_rotary_emb, code: return torch.cat([y1, y2], 3).type_as(x)
[rank0]:             cat: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = torch.cat([add, add_1], 3);  add = add_1 = None
[rank0]:             type_as: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = cat.type_as(view);  cat = view = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:134 in apply_rotary_emb, code: x1 = x[..., :d]
[rank0]:             getitem_4: "bf16[16, 256, 6, 64][196608, 768, 128, 1]" = view_1[(Ellipsis, slice(None, 64, None))]
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:135 in apply_rotary_emb, code: x2 = x[..., d:]
[rank0]:             getitem_5: "bf16[16, 256, 6, 64][196608, 768, 128, 1]" = view_1[(Ellipsis, slice(64, None, None))]
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:136 in apply_rotary_emb, code: y1 = x1 * cos + x2 * sin
[rank0]:             mul_4: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = getitem_4 * getitem
[rank0]:             mul_5: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = getitem_5 * getitem_1
[rank0]:             add_2: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = mul_4 + mul_5;  mul_4 = mul_5 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:137 in apply_rotary_emb, code: y2 = x1 * (-sin) + x2 * cos
[rank0]:             neg_1: "bf16[1, 256, 1, 64][16384, 64, 64, 1]" = -getitem_1;  getitem_1 = None
[rank0]:             mul_6: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = getitem_4 * neg_1;  getitem_4 = neg_1 = None
[rank0]:             mul_7: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = getitem_5 * getitem;  getitem_5 = getitem = None
[rank0]:             add_3: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = mul_6 + mul_7;  mul_6 = mul_7 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:138 in apply_rotary_emb, code: return torch.cat([y1, y2], 3).type_as(x)
[rank0]:             cat_1: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = torch.cat([add_2, add_3], 3);  add_2 = add_3 = None
[rank0]:             type_as_1: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = cat_1.type_as(view_1);  cat_1 = view_1 = None
[rank0]:             
[rank0]:              # File: /usr/local/lib/python3.12/dist-packages/torch/nn/functional.py:2924 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
[rank0]:             rms_norm: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = torch.rms_norm(type_as, (128,), None, None);  type_as = None
[rank0]:             rms_norm_9: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = torch.rms_norm(type_as_1, (128,), None, None);  type_as_1 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:164 in forward, code: y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)
[rank0]:             transpose: "bf16[16, 6, 256, 128][196608, 128, 768, 1]" = rms_norm.transpose(1, 2);  rms_norm = None
[rank0]:             transpose_1: "bf16[16, 6, 256, 128][196608, 128, 768, 1]" = rms_norm_9.transpose(1, 2);  rms_norm_9 = None
[rank0]:             transpose_2: "bf16[16, 6, 256, 128][196608, 128, 768, 1]" = view_2.transpose(1, 2);  view_2 = None
[rank0]:             scaled_dot_product_attention: "bf16[16, 6, 256, 128][196608, 128, 768, 1]" = torch._C._nn.scaled_dot_product_attention(transpose, transpose_1, transpose_2, is_causal = True);  transpose = transpose_1 = transpose_2 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:165 in forward, code: y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
[rank0]:             transpose_3: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = scaled_dot_product_attention.transpose(1, 2);  scaled_dot_product_attention = None
[rank0]:             contiguous: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = transpose_3.contiguous();  transpose_3 = None
[rank0]:             view_as: "bf16[16, 256, 768][196608, 768, 1]" = contiguous.view_as(rms_norm_8);  contiguous = rms_norm_8 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:166 in forward, code: y = self.c_proj(y)
[rank0]:             linear_3: "bf16[16, 256, 768][196608, 768, 1]" = torch._C._nn.linear(view_as, l_self_modules_transformer_modules_h_modules_2_modules_attn_modules_c_proj_parameters_weight_, None);  view_as = l_self_modules_transformer_modules_h_modules_2_modules_attn_modules_c_proj_parameters_weight_ = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:191 in forward, code: x = x + self.attn(F.rms_norm(x, (x.size(-1),)))
[rank0]:             add_4: "f32[16, 256, 768][196608, 768, 1]" = x_10 + linear_3;  x_10 = linear_3 = None
[rank0]:             
[rank0]:              # File: /usr/local/lib/python3.12/dist-packages/torch/nn/functional.py:2924 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
[rank0]:             rms_norm_10: "f32[16, 256, 768][196608, 768, 1]" = torch.rms_norm(add_4, (768,), None, None)
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:178 in forward, code: x = self.c_fc(x)
[rank0]:             linear_4: "bf16[16, 256, 3072][786432, 3072, 1]" = torch._C._nn.linear(rms_norm_10, l_self_modules_transformer_modules_h_modules_2_modules_mlp_modules_c_fc_parameters_weight_, None);  rms_norm_10 = l_self_modules_transformer_modules_h_modules_2_modules_mlp_modules_c_fc_parameters_weight_ = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:179 in forward, code: x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
[rank0]:             relu: "bf16[16, 256, 3072][786432, 3072, 1]" = torch.nn.functional.relu(linear_4);  linear_4 = None
[rank0]:             square: "f32[16, 256, 3072][786432, 3072, 1]" = relu.square();  relu = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:180 in forward, code: x = self.c_proj(x)
[rank0]:             linear_5: "bf16[16, 256, 768][196608, 768, 1]" = torch._C._nn.linear(square, l_self_modules_transformer_modules_h_modules_2_modules_mlp_modules_c_proj_parameters_weight_, None);  square = l_self_modules_transformer_modules_h_modules_2_modules_mlp_modules_c_proj_parameters_weight_ = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:192 in forward, code: x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))
[rank0]:             add_5: "f32[16, 256, 768][196608, 768, 1]" = add_4 + linear_5;  add_4 = linear_5 = None
[rank0]:             
[rank0]:              # File: /usr/local/lib/python3.12/dist-packages/torch/nn/functional.py:2924 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
[rank0]:             rms_norm_11: "f32[16, 256, 768][196608, 768, 1]" = torch.rms_norm(add_5, (768,), None, None)
[rank0]:             return (rms_norm_11, add_5)
[rank0]:             
[rank0]:     class submod_4(torch.nn.Module):
[rank0]:         def forward(self, rms_norm_12: "f32[16, 256, 768][196608, 768, 1]", l_self_modules_transformer_modules_h_modules_3_modules_attn_modules_c_q_parameters_weight_: "f32[768, 768][768, 1]", l_self_modules_transformer_modules_h_modules_3_modules_attn_modules_c_k_parameters_weight_: "f32[768, 768][768, 1]", l_self_modules_transformer_modules_h_modules_3_modules_attn_modules_c_v_parameters_weight_: "f32[768, 768][768, 1]", l_self_modules_transformer_modules_h_modules_3_modules_attn_modules_rotary_cos_cached: "bf16[256, 64][64, 1]", l_self_modules_transformer_modules_h_modules_3_modules_attn_modules_rotary_sin_cached: "bf16[256, 64][64, 1]", l_self_modules_transformer_modules_h_modules_3_modules_attn_modules_c_proj_parameters_weight_: "f32[768, 768][768, 1]", x_15: "f32[16, 256, 768][196608, 768, 1]", l_self_modules_transformer_modules_h_modules_3_modules_mlp_modules_c_fc_parameters_weight_: "f32[3072, 768][768, 1]", l_self_modules_transformer_modules_h_modules_3_modules_mlp_modules_c_proj_parameters_weight_: "f32[768, 3072][3072, 1]"):
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:158 in forward, code: q = self.c_q(x).view(B, T, self.n_head, self.head_dim)
[rank0]:             linear: "bf16[16, 256, 768][196608, 768, 1]" = torch._C._nn.linear(rms_norm_12, l_self_modules_transformer_modules_h_modules_3_modules_attn_modules_c_q_parameters_weight_, None);  l_self_modules_transformer_modules_h_modules_3_modules_attn_modules_c_q_parameters_weight_ = None
[rank0]:             view: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = linear.view(16, 256, 6, 128);  linear = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:159 in forward, code: k = self.c_k(x).view(B, T, self.n_head, self.head_dim)
[rank0]:             linear_1: "bf16[16, 256, 768][196608, 768, 1]" = torch._C._nn.linear(rms_norm_12, l_self_modules_transformer_modules_h_modules_3_modules_attn_modules_c_k_parameters_weight_, None);  l_self_modules_transformer_modules_h_modules_3_modules_attn_modules_c_k_parameters_weight_ = None
[rank0]:             view_1: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = linear_1.view(16, 256, 6, 128);  linear_1 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:160 in forward, code: v = self.c_v(x).view(B, T, self.n_head, self.head_dim)
[rank0]:             linear_2: "bf16[16, 256, 768][196608, 768, 1]" = torch._C._nn.linear(rms_norm_12, l_self_modules_transformer_modules_h_modules_3_modules_attn_modules_c_v_parameters_weight_, None);  l_self_modules_transformer_modules_h_modules_3_modules_attn_modules_c_v_parameters_weight_ = None
[rank0]:             view_2: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = linear_2.view(16, 256, 6, 128);  linear_2 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:129 in forward, code: return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]
[rank0]:             getitem: "bf16[1, 256, 1, 64][16384, 64, 64, 1]" = l_self_modules_transformer_modules_h_modules_3_modules_attn_modules_rotary_cos_cached[(None, slice(None, None, None), None, slice(None, None, None))];  l_self_modules_transformer_modules_h_modules_3_modules_attn_modules_rotary_cos_cached = None
[rank0]:             getitem_1: "bf16[1, 256, 1, 64][16384, 64, 64, 1]" = l_self_modules_transformer_modules_h_modules_3_modules_attn_modules_rotary_sin_cached[(None, slice(None, None, None), None, slice(None, None, None))];  l_self_modules_transformer_modules_h_modules_3_modules_attn_modules_rotary_sin_cached = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:134 in apply_rotary_emb, code: x1 = x[..., :d]
[rank0]:             getitem_2: "bf16[16, 256, 6, 64][196608, 768, 128, 1]" = view[(Ellipsis, slice(None, 64, None))]
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:135 in apply_rotary_emb, code: x2 = x[..., d:]
[rank0]:             getitem_3: "bf16[16, 256, 6, 64][196608, 768, 128, 1]" = view[(Ellipsis, slice(64, None, None))]
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:136 in apply_rotary_emb, code: y1 = x1 * cos + x2 * sin
[rank0]:             mul: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = getitem_2 * getitem
[rank0]:             mul_1: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = getitem_3 * getitem_1
[rank0]:             add: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = mul + mul_1;  mul = mul_1 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:137 in apply_rotary_emb, code: y2 = x1 * (-sin) + x2 * cos
[rank0]:             neg: "bf16[1, 256, 1, 64][16384, 64, 64, 1]" = -getitem_1
[rank0]:             mul_2: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = getitem_2 * neg;  getitem_2 = neg = None
[rank0]:             mul_3: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = getitem_3 * getitem;  getitem_3 = None
[rank0]:             add_1: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = mul_2 + mul_3;  mul_2 = mul_3 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:138 in apply_rotary_emb, code: return torch.cat([y1, y2], 3).type_as(x)
[rank0]:             cat: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = torch.cat([add, add_1], 3);  add = add_1 = None
[rank0]:             type_as: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = cat.type_as(view);  cat = view = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:134 in apply_rotary_emb, code: x1 = x[..., :d]
[rank0]:             getitem_4: "bf16[16, 256, 6, 64][196608, 768, 128, 1]" = view_1[(Ellipsis, slice(None, 64, None))]
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:135 in apply_rotary_emb, code: x2 = x[..., d:]
[rank0]:             getitem_5: "bf16[16, 256, 6, 64][196608, 768, 128, 1]" = view_1[(Ellipsis, slice(64, None, None))]
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:136 in apply_rotary_emb, code: y1 = x1 * cos + x2 * sin
[rank0]:             mul_4: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = getitem_4 * getitem
[rank0]:             mul_5: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = getitem_5 * getitem_1
[rank0]:             add_2: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = mul_4 + mul_5;  mul_4 = mul_5 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:137 in apply_rotary_emb, code: y2 = x1 * (-sin) + x2 * cos
[rank0]:             neg_1: "bf16[1, 256, 1, 64][16384, 64, 64, 1]" = -getitem_1;  getitem_1 = None
[rank0]:             mul_6: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = getitem_4 * neg_1;  getitem_4 = neg_1 = None
[rank0]:             mul_7: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = getitem_5 * getitem;  getitem_5 = getitem = None
[rank0]:             add_3: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = mul_6 + mul_7;  mul_6 = mul_7 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:138 in apply_rotary_emb, code: return torch.cat([y1, y2], 3).type_as(x)
[rank0]:             cat_1: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = torch.cat([add_2, add_3], 3);  add_2 = add_3 = None
[rank0]:             type_as_1: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = cat_1.type_as(view_1);  cat_1 = view_1 = None
[rank0]:             
[rank0]:              # File: /usr/local/lib/python3.12/dist-packages/torch/nn/functional.py:2924 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
[rank0]:             rms_norm: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = torch.rms_norm(type_as, (128,), None, None);  type_as = None
[rank0]:             rms_norm_13: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = torch.rms_norm(type_as_1, (128,), None, None);  type_as_1 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:164 in forward, code: y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)
[rank0]:             transpose: "bf16[16, 6, 256, 128][196608, 128, 768, 1]" = rms_norm.transpose(1, 2);  rms_norm = None
[rank0]:             transpose_1: "bf16[16, 6, 256, 128][196608, 128, 768, 1]" = rms_norm_13.transpose(1, 2);  rms_norm_13 = None
[rank0]:             transpose_2: "bf16[16, 6, 256, 128][196608, 128, 768, 1]" = view_2.transpose(1, 2);  view_2 = None
[rank0]:             scaled_dot_product_attention: "bf16[16, 6, 256, 128][196608, 128, 768, 1]" = torch._C._nn.scaled_dot_product_attention(transpose, transpose_1, transpose_2, is_causal = True);  transpose = transpose_1 = transpose_2 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:165 in forward, code: y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
[rank0]:             transpose_3: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = scaled_dot_product_attention.transpose(1, 2);  scaled_dot_product_attention = None
[rank0]:             contiguous: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = transpose_3.contiguous();  transpose_3 = None
[rank0]:             view_as: "bf16[16, 256, 768][196608, 768, 1]" = contiguous.view_as(rms_norm_12);  contiguous = rms_norm_12 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:166 in forward, code: y = self.c_proj(y)
[rank0]:             linear_3: "bf16[16, 256, 768][196608, 768, 1]" = torch._C._nn.linear(view_as, l_self_modules_transformer_modules_h_modules_3_modules_attn_modules_c_proj_parameters_weight_, None);  view_as = l_self_modules_transformer_modules_h_modules_3_modules_attn_modules_c_proj_parameters_weight_ = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:191 in forward, code: x = x + self.attn(F.rms_norm(x, (x.size(-1),)))
[rank0]:             add_4: "f32[16, 256, 768][196608, 768, 1]" = x_15 + linear_3;  x_15 = linear_3 = None
[rank0]:             
[rank0]:              # File: /usr/local/lib/python3.12/dist-packages/torch/nn/functional.py:2924 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
[rank0]:             rms_norm_14: "f32[16, 256, 768][196608, 768, 1]" = torch.rms_norm(add_4, (768,), None, None)
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:178 in forward, code: x = self.c_fc(x)
[rank0]:             linear_4: "bf16[16, 256, 3072][786432, 3072, 1]" = torch._C._nn.linear(rms_norm_14, l_self_modules_transformer_modules_h_modules_3_modules_mlp_modules_c_fc_parameters_weight_, None);  rms_norm_14 = l_self_modules_transformer_modules_h_modules_3_modules_mlp_modules_c_fc_parameters_weight_ = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:179 in forward, code: x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
[rank0]:             relu: "bf16[16, 256, 3072][786432, 3072, 1]" = torch.nn.functional.relu(linear_4);  linear_4 = None
[rank0]:             square: "f32[16, 256, 3072][786432, 3072, 1]" = relu.square();  relu = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:180 in forward, code: x = self.c_proj(x)
[rank0]:             linear_5: "bf16[16, 256, 768][196608, 768, 1]" = torch._C._nn.linear(square, l_self_modules_transformer_modules_h_modules_3_modules_mlp_modules_c_proj_parameters_weight_, None);  square = l_self_modules_transformer_modules_h_modules_3_modules_mlp_modules_c_proj_parameters_weight_ = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:192 in forward, code: x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))
[rank0]:             add_5: "f32[16, 256, 768][196608, 768, 1]" = add_4 + linear_5;  add_4 = linear_5 = None
[rank0]:             
[rank0]:              # File: /usr/local/lib/python3.12/dist-packages/torch/nn/functional.py:2924 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
[rank0]:             rms_norm_15: "f32[16, 256, 768][196608, 768, 1]" = torch.rms_norm(add_5, (768,), None, None)
[rank0]:             return (rms_norm_15, add_5)
[rank0]:             
[rank0]:     class submod_5(torch.nn.Module):
[rank0]:         def forward(self, rms_norm_16: "f32[16, 256, 768][196608, 768, 1]", l_self_modules_transformer_modules_h_modules_4_modules_attn_modules_c_q_parameters_weight_: "f32[768, 768][768, 1]", l_self_modules_transformer_modules_h_modules_4_modules_attn_modules_c_k_parameters_weight_: "f32[768, 768][768, 1]", l_self_modules_transformer_modules_h_modules_4_modules_attn_modules_c_v_parameters_weight_: "f32[768, 768][768, 1]", l_self_modules_transformer_modules_h_modules_4_modules_attn_modules_rotary_cos_cached: "bf16[256, 64][64, 1]", l_self_modules_transformer_modules_h_modules_4_modules_attn_modules_rotary_sin_cached: "bf16[256, 64][64, 1]", l_self_modules_transformer_modules_h_modules_4_modules_attn_modules_c_proj_parameters_weight_: "f32[768, 768][768, 1]", x_20: "f32[16, 256, 768][196608, 768, 1]", l_self_modules_transformer_modules_h_modules_4_modules_mlp_modules_c_fc_parameters_weight_: "f32[3072, 768][768, 1]", l_self_modules_transformer_modules_h_modules_4_modules_mlp_modules_c_proj_parameters_weight_: "f32[768, 3072][3072, 1]"):
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:158 in forward, code: q = self.c_q(x).view(B, T, self.n_head, self.head_dim)
[rank0]:             linear: "bf16[16, 256, 768][196608, 768, 1]" = torch._C._nn.linear(rms_norm_16, l_self_modules_transformer_modules_h_modules_4_modules_attn_modules_c_q_parameters_weight_, None);  l_self_modules_transformer_modules_h_modules_4_modules_attn_modules_c_q_parameters_weight_ = None
[rank0]:             view: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = linear.view(16, 256, 6, 128);  linear = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:159 in forward, code: k = self.c_k(x).view(B, T, self.n_head, self.head_dim)
[rank0]:             linear_1: "bf16[16, 256, 768][196608, 768, 1]" = torch._C._nn.linear(rms_norm_16, l_self_modules_transformer_modules_h_modules_4_modules_attn_modules_c_k_parameters_weight_, None);  l_self_modules_transformer_modules_h_modules_4_modules_attn_modules_c_k_parameters_weight_ = None
[rank0]:             view_1: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = linear_1.view(16, 256, 6, 128);  linear_1 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:160 in forward, code: v = self.c_v(x).view(B, T, self.n_head, self.head_dim)
[rank0]:             linear_2: "bf16[16, 256, 768][196608, 768, 1]" = torch._C._nn.linear(rms_norm_16, l_self_modules_transformer_modules_h_modules_4_modules_attn_modules_c_v_parameters_weight_, None);  l_self_modules_transformer_modules_h_modules_4_modules_attn_modules_c_v_parameters_weight_ = None
[rank0]:             view_2: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = linear_2.view(16, 256, 6, 128);  linear_2 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:129 in forward, code: return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]
[rank0]:             getitem: "bf16[1, 256, 1, 64][16384, 64, 64, 1]" = l_self_modules_transformer_modules_h_modules_4_modules_attn_modules_rotary_cos_cached[(None, slice(None, None, None), None, slice(None, None, None))];  l_self_modules_transformer_modules_h_modules_4_modules_attn_modules_rotary_cos_cached = None
[rank0]:             getitem_1: "bf16[1, 256, 1, 64][16384, 64, 64, 1]" = l_self_modules_transformer_modules_h_modules_4_modules_attn_modules_rotary_sin_cached[(None, slice(None, None, None), None, slice(None, None, None))];  l_self_modules_transformer_modules_h_modules_4_modules_attn_modules_rotary_sin_cached = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:134 in apply_rotary_emb, code: x1 = x[..., :d]
[rank0]:             getitem_2: "bf16[16, 256, 6, 64][196608, 768, 128, 1]" = view[(Ellipsis, slice(None, 64, None))]
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:135 in apply_rotary_emb, code: x2 = x[..., d:]
[rank0]:             getitem_3: "bf16[16, 256, 6, 64][196608, 768, 128, 1]" = view[(Ellipsis, slice(64, None, None))]
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:136 in apply_rotary_emb, code: y1 = x1 * cos + x2 * sin
[rank0]:             mul: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = getitem_2 * getitem
[rank0]:             mul_1: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = getitem_3 * getitem_1
[rank0]:             add: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = mul + mul_1;  mul = mul_1 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:137 in apply_rotary_emb, code: y2 = x1 * (-sin) + x2 * cos
[rank0]:             neg: "bf16[1, 256, 1, 64][16384, 64, 64, 1]" = -getitem_1
[rank0]:             mul_2: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = getitem_2 * neg;  getitem_2 = neg = None
[rank0]:             mul_3: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = getitem_3 * getitem;  getitem_3 = None
[rank0]:             add_1: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = mul_2 + mul_3;  mul_2 = mul_3 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:138 in apply_rotary_emb, code: return torch.cat([y1, y2], 3).type_as(x)
[rank0]:             cat: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = torch.cat([add, add_1], 3);  add = add_1 = None
[rank0]:             type_as: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = cat.type_as(view);  cat = view = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:134 in apply_rotary_emb, code: x1 = x[..., :d]
[rank0]:             getitem_4: "bf16[16, 256, 6, 64][196608, 768, 128, 1]" = view_1[(Ellipsis, slice(None, 64, None))]
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:135 in apply_rotary_emb, code: x2 = x[..., d:]
[rank0]:             getitem_5: "bf16[16, 256, 6, 64][196608, 768, 128, 1]" = view_1[(Ellipsis, slice(64, None, None))]
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:136 in apply_rotary_emb, code: y1 = x1 * cos + x2 * sin
[rank0]:             mul_4: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = getitem_4 * getitem
[rank0]:             mul_5: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = getitem_5 * getitem_1
[rank0]:             add_2: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = mul_4 + mul_5;  mul_4 = mul_5 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:137 in apply_rotary_emb, code: y2 = x1 * (-sin) + x2 * cos
[rank0]:             neg_1: "bf16[1, 256, 1, 64][16384, 64, 64, 1]" = -getitem_1;  getitem_1 = None
[rank0]:             mul_6: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = getitem_4 * neg_1;  getitem_4 = neg_1 = None
[rank0]:             mul_7: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = getitem_5 * getitem;  getitem_5 = getitem = None
[rank0]:             add_3: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = mul_6 + mul_7;  mul_6 = mul_7 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:138 in apply_rotary_emb, code: return torch.cat([y1, y2], 3).type_as(x)
[rank0]:             cat_1: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = torch.cat([add_2, add_3], 3);  add_2 = add_3 = None
[rank0]:             type_as_1: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = cat_1.type_as(view_1);  cat_1 = view_1 = None
[rank0]:             
[rank0]:              # File: /usr/local/lib/python3.12/dist-packages/torch/nn/functional.py:2924 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
[rank0]:             rms_norm: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = torch.rms_norm(type_as, (128,), None, None);  type_as = None
[rank0]:             rms_norm_17: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = torch.rms_norm(type_as_1, (128,), None, None);  type_as_1 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:164 in forward, code: y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)
[rank0]:             transpose: "bf16[16, 6, 256, 128][196608, 128, 768, 1]" = rms_norm.transpose(1, 2);  rms_norm = None
[rank0]:             transpose_1: "bf16[16, 6, 256, 128][196608, 128, 768, 1]" = rms_norm_17.transpose(1, 2);  rms_norm_17 = None
[rank0]:             transpose_2: "bf16[16, 6, 256, 128][196608, 128, 768, 1]" = view_2.transpose(1, 2);  view_2 = None
[rank0]:             scaled_dot_product_attention: "bf16[16, 6, 256, 128][196608, 128, 768, 1]" = torch._C._nn.scaled_dot_product_attention(transpose, transpose_1, transpose_2, is_causal = True);  transpose = transpose_1 = transpose_2 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:165 in forward, code: y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
[rank0]:             transpose_3: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = scaled_dot_product_attention.transpose(1, 2);  scaled_dot_product_attention = None
[rank0]:             contiguous: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = transpose_3.contiguous();  transpose_3 = None
[rank0]:             view_as: "bf16[16, 256, 768][196608, 768, 1]" = contiguous.view_as(rms_norm_16);  contiguous = rms_norm_16 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:166 in forward, code: y = self.c_proj(y)
[rank0]:             linear_3: "bf16[16, 256, 768][196608, 768, 1]" = torch._C._nn.linear(view_as, l_self_modules_transformer_modules_h_modules_4_modules_attn_modules_c_proj_parameters_weight_, None);  view_as = l_self_modules_transformer_modules_h_modules_4_modules_attn_modules_c_proj_parameters_weight_ = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:191 in forward, code: x = x + self.attn(F.rms_norm(x, (x.size(-1),)))
[rank0]:             add_4: "f32[16, 256, 768][196608, 768, 1]" = x_20 + linear_3;  x_20 = linear_3 = None
[rank0]:             
[rank0]:              # File: /usr/local/lib/python3.12/dist-packages/torch/nn/functional.py:2924 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
[rank0]:             rms_norm_18: "f32[16, 256, 768][196608, 768, 1]" = torch.rms_norm(add_4, (768,), None, None)
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:178 in forward, code: x = self.c_fc(x)
[rank0]:             linear_4: "bf16[16, 256, 3072][786432, 3072, 1]" = torch._C._nn.linear(rms_norm_18, l_self_modules_transformer_modules_h_modules_4_modules_mlp_modules_c_fc_parameters_weight_, None);  rms_norm_18 = l_self_modules_transformer_modules_h_modules_4_modules_mlp_modules_c_fc_parameters_weight_ = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:179 in forward, code: x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
[rank0]:             relu: "bf16[16, 256, 3072][786432, 3072, 1]" = torch.nn.functional.relu(linear_4);  linear_4 = None
[rank0]:             square: "f32[16, 256, 3072][786432, 3072, 1]" = relu.square();  relu = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:180 in forward, code: x = self.c_proj(x)
[rank0]:             linear_5: "bf16[16, 256, 768][196608, 768, 1]" = torch._C._nn.linear(square, l_self_modules_transformer_modules_h_modules_4_modules_mlp_modules_c_proj_parameters_weight_, None);  square = l_self_modules_transformer_modules_h_modules_4_modules_mlp_modules_c_proj_parameters_weight_ = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:192 in forward, code: x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))
[rank0]:             add_5: "f32[16, 256, 768][196608, 768, 1]" = add_4 + linear_5;  add_4 = linear_5 = None
[rank0]:             
[rank0]:              # File: /usr/local/lib/python3.12/dist-packages/torch/nn/functional.py:2924 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
[rank0]:             rms_norm_19: "f32[16, 256, 768][196608, 768, 1]" = torch.rms_norm(add_5, (768,), None, None)
[rank0]:             return (rms_norm_19, add_5)
[rank0]:             
[rank0]:     class submod_6(torch.nn.Module):
[rank0]:         def forward(self, rms_norm_20: "f32[16, 256, 768][196608, 768, 1]", l_self_modules_transformer_modules_h_modules_5_modules_attn_modules_c_q_parameters_weight_: "f32[768, 768][768, 1]", l_self_modules_transformer_modules_h_modules_5_modules_attn_modules_c_k_parameters_weight_: "f32[768, 768][768, 1]", l_self_modules_transformer_modules_h_modules_5_modules_attn_modules_c_v_parameters_weight_: "f32[768, 768][768, 1]", l_self_modules_transformer_modules_h_modules_5_modules_attn_modules_rotary_cos_cached: "bf16[256, 64][64, 1]", l_self_modules_transformer_modules_h_modules_5_modules_attn_modules_rotary_sin_cached: "bf16[256, 64][64, 1]", l_self_modules_transformer_modules_h_modules_5_modules_attn_modules_c_proj_parameters_weight_: "f32[768, 768][768, 1]", x_25: "f32[16, 256, 768][196608, 768, 1]", l_self_modules_transformer_modules_h_modules_5_modules_mlp_modules_c_fc_parameters_weight_: "f32[3072, 768][768, 1]", l_self_modules_transformer_modules_h_modules_5_modules_mlp_modules_c_proj_parameters_weight_: "f32[768, 3072][3072, 1]"):
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:158 in forward, code: q = self.c_q(x).view(B, T, self.n_head, self.head_dim)
[rank0]:             linear: "bf16[16, 256, 768][196608, 768, 1]" = torch._C._nn.linear(rms_norm_20, l_self_modules_transformer_modules_h_modules_5_modules_attn_modules_c_q_parameters_weight_, None);  l_self_modules_transformer_modules_h_modules_5_modules_attn_modules_c_q_parameters_weight_ = None
[rank0]:             view: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = linear.view(16, 256, 6, 128);  linear = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:159 in forward, code: k = self.c_k(x).view(B, T, self.n_head, self.head_dim)
[rank0]:             linear_1: "bf16[16, 256, 768][196608, 768, 1]" = torch._C._nn.linear(rms_norm_20, l_self_modules_transformer_modules_h_modules_5_modules_attn_modules_c_k_parameters_weight_, None);  l_self_modules_transformer_modules_h_modules_5_modules_attn_modules_c_k_parameters_weight_ = None
[rank0]:             view_1: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = linear_1.view(16, 256, 6, 128);  linear_1 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:160 in forward, code: v = self.c_v(x).view(B, T, self.n_head, self.head_dim)
[rank0]:             linear_2: "bf16[16, 256, 768][196608, 768, 1]" = torch._C._nn.linear(rms_norm_20, l_self_modules_transformer_modules_h_modules_5_modules_attn_modules_c_v_parameters_weight_, None);  l_self_modules_transformer_modules_h_modules_5_modules_attn_modules_c_v_parameters_weight_ = None
[rank0]:             view_2: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = linear_2.view(16, 256, 6, 128);  linear_2 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:129 in forward, code: return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]
[rank0]:             getitem: "bf16[1, 256, 1, 64][16384, 64, 64, 1]" = l_self_modules_transformer_modules_h_modules_5_modules_attn_modules_rotary_cos_cached[(None, slice(None, None, None), None, slice(None, None, None))];  l_self_modules_transformer_modules_h_modules_5_modules_attn_modules_rotary_cos_cached = None
[rank0]:             getitem_1: "bf16[1, 256, 1, 64][16384, 64, 64, 1]" = l_self_modules_transformer_modules_h_modules_5_modules_attn_modules_rotary_sin_cached[(None, slice(None, None, None), None, slice(None, None, None))];  l_self_modules_transformer_modules_h_modules_5_modules_attn_modules_rotary_sin_cached = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:134 in apply_rotary_emb, code: x1 = x[..., :d]
[rank0]:             getitem_2: "bf16[16, 256, 6, 64][196608, 768, 128, 1]" = view[(Ellipsis, slice(None, 64, None))]
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:135 in apply_rotary_emb, code: x2 = x[..., d:]
[rank0]:             getitem_3: "bf16[16, 256, 6, 64][196608, 768, 128, 1]" = view[(Ellipsis, slice(64, None, None))]
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:136 in apply_rotary_emb, code: y1 = x1 * cos + x2 * sin
[rank0]:             mul: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = getitem_2 * getitem
[rank0]:             mul_1: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = getitem_3 * getitem_1
[rank0]:             add: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = mul + mul_1;  mul = mul_1 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:137 in apply_rotary_emb, code: y2 = x1 * (-sin) + x2 * cos
[rank0]:             neg: "bf16[1, 256, 1, 64][16384, 64, 64, 1]" = -getitem_1
[rank0]:             mul_2: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = getitem_2 * neg;  getitem_2 = neg = None
[rank0]:             mul_3: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = getitem_3 * getitem;  getitem_3 = None
[rank0]:             add_1: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = mul_2 + mul_3;  mul_2 = mul_3 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:138 in apply_rotary_emb, code: return torch.cat([y1, y2], 3).type_as(x)
[rank0]:             cat: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = torch.cat([add, add_1], 3);  add = add_1 = None
[rank0]:             type_as: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = cat.type_as(view);  cat = view = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:134 in apply_rotary_emb, code: x1 = x[..., :d]
[rank0]:             getitem_4: "bf16[16, 256, 6, 64][196608, 768, 128, 1]" = view_1[(Ellipsis, slice(None, 64, None))]
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:135 in apply_rotary_emb, code: x2 = x[..., d:]
[rank0]:             getitem_5: "bf16[16, 256, 6, 64][196608, 768, 128, 1]" = view_1[(Ellipsis, slice(64, None, None))]
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:136 in apply_rotary_emb, code: y1 = x1 * cos + x2 * sin
[rank0]:             mul_4: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = getitem_4 * getitem
[rank0]:             mul_5: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = getitem_5 * getitem_1
[rank0]:             add_2: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = mul_4 + mul_5;  mul_4 = mul_5 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:137 in apply_rotary_emb, code: y2 = x1 * (-sin) + x2 * cos
[rank0]:             neg_1: "bf16[1, 256, 1, 64][16384, 64, 64, 1]" = -getitem_1;  getitem_1 = None
[rank0]:             mul_6: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = getitem_4 * neg_1;  getitem_4 = neg_1 = None
[rank0]:             mul_7: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = getitem_5 * getitem;  getitem_5 = getitem = None
[rank0]:             add_3: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = mul_6 + mul_7;  mul_6 = mul_7 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:138 in apply_rotary_emb, code: return torch.cat([y1, y2], 3).type_as(x)
[rank0]:             cat_1: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = torch.cat([add_2, add_3], 3);  add_2 = add_3 = None
[rank0]:             type_as_1: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = cat_1.type_as(view_1);  cat_1 = view_1 = None
[rank0]:             
[rank0]:              # File: /usr/local/lib/python3.12/dist-packages/torch/nn/functional.py:2924 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
[rank0]:             rms_norm: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = torch.rms_norm(type_as, (128,), None, None);  type_as = None
[rank0]:             rms_norm_21: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = torch.rms_norm(type_as_1, (128,), None, None);  type_as_1 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:164 in forward, code: y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)
[rank0]:             transpose: "bf16[16, 6, 256, 128][196608, 128, 768, 1]" = rms_norm.transpose(1, 2);  rms_norm = None
[rank0]:             transpose_1: "bf16[16, 6, 256, 128][196608, 128, 768, 1]" = rms_norm_21.transpose(1, 2);  rms_norm_21 = None
[rank0]:             transpose_2: "bf16[16, 6, 256, 128][196608, 128, 768, 1]" = view_2.transpose(1, 2);  view_2 = None
[rank0]:             scaled_dot_product_attention: "bf16[16, 6, 256, 128][196608, 128, 768, 1]" = torch._C._nn.scaled_dot_product_attention(transpose, transpose_1, transpose_2, is_causal = True);  transpose = transpose_1 = transpose_2 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:165 in forward, code: y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
[rank0]:             transpose_3: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = scaled_dot_product_attention.transpose(1, 2);  scaled_dot_product_attention = None
[rank0]:             contiguous: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = transpose_3.contiguous();  transpose_3 = None
[rank0]:             view_as: "bf16[16, 256, 768][196608, 768, 1]" = contiguous.view_as(rms_norm_20);  contiguous = rms_norm_20 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:166 in forward, code: y = self.c_proj(y)
[rank0]:             linear_3: "bf16[16, 256, 768][196608, 768, 1]" = torch._C._nn.linear(view_as, l_self_modules_transformer_modules_h_modules_5_modules_attn_modules_c_proj_parameters_weight_, None);  view_as = l_self_modules_transformer_modules_h_modules_5_modules_attn_modules_c_proj_parameters_weight_ = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:191 in forward, code: x = x + self.attn(F.rms_norm(x, (x.size(-1),)))
[rank0]:             add_4: "f32[16, 256, 768][196608, 768, 1]" = x_25 + linear_3;  x_25 = linear_3 = None
[rank0]:             
[rank0]:              # File: /usr/local/lib/python3.12/dist-packages/torch/nn/functional.py:2924 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
[rank0]:             rms_norm_22: "f32[16, 256, 768][196608, 768, 1]" = torch.rms_norm(add_4, (768,), None, None)
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:178 in forward, code: x = self.c_fc(x)
[rank0]:             linear_4: "bf16[16, 256, 3072][786432, 3072, 1]" = torch._C._nn.linear(rms_norm_22, l_self_modules_transformer_modules_h_modules_5_modules_mlp_modules_c_fc_parameters_weight_, None);  rms_norm_22 = l_self_modules_transformer_modules_h_modules_5_modules_mlp_modules_c_fc_parameters_weight_ = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:179 in forward, code: x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
[rank0]:             relu: "bf16[16, 256, 3072][786432, 3072, 1]" = torch.nn.functional.relu(linear_4);  linear_4 = None
[rank0]:             square: "f32[16, 256, 3072][786432, 3072, 1]" = relu.square();  relu = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:180 in forward, code: x = self.c_proj(x)
[rank0]:             linear_5: "bf16[16, 256, 768][196608, 768, 1]" = torch._C._nn.linear(square, l_self_modules_transformer_modules_h_modules_5_modules_mlp_modules_c_proj_parameters_weight_, None);  square = l_self_modules_transformer_modules_h_modules_5_modules_mlp_modules_c_proj_parameters_weight_ = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:192 in forward, code: x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))
[rank0]:             add_5: "f32[16, 256, 768][196608, 768, 1]" = add_4 + linear_5;  add_4 = linear_5 = None
[rank0]:             
[rank0]:              # File: /usr/local/lib/python3.12/dist-packages/torch/nn/functional.py:2924 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
[rank0]:             rms_norm_23: "f32[16, 256, 768][196608, 768, 1]" = torch.rms_norm(add_5, (768,), None, None)
[rank0]:             return (rms_norm_23, add_5)
[rank0]:             
[rank0]:     class submod_7(torch.nn.Module):
[rank0]:         def forward(self, rms_norm_24: "f32[16, 256, 768][196608, 768, 1]", l_self_modules_transformer_modules_h_modules_6_modules_attn_modules_c_q_parameters_weight_: "f32[768, 768][768, 1]", l_self_modules_transformer_modules_h_modules_6_modules_attn_modules_c_k_parameters_weight_: "f32[768, 768][768, 1]", l_self_modules_transformer_modules_h_modules_6_modules_attn_modules_c_v_parameters_weight_: "f32[768, 768][768, 1]", l_self_modules_transformer_modules_h_modules_6_modules_attn_modules_rotary_cos_cached: "bf16[256, 64][64, 1]", l_self_modules_transformer_modules_h_modules_6_modules_attn_modules_rotary_sin_cached: "bf16[256, 64][64, 1]", l_self_modules_transformer_modules_h_modules_6_modules_attn_modules_c_proj_parameters_weight_: "f32[768, 768][768, 1]", x_30: "f32[16, 256, 768][196608, 768, 1]", l_self_modules_transformer_modules_h_modules_6_modules_mlp_modules_c_fc_parameters_weight_: "f32[3072, 768][768, 1]", l_self_modules_transformer_modules_h_modules_6_modules_mlp_modules_c_proj_parameters_weight_: "f32[768, 3072][3072, 1]"):
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:158 in forward, code: q = self.c_q(x).view(B, T, self.n_head, self.head_dim)
[rank0]:             linear: "bf16[16, 256, 768][196608, 768, 1]" = torch._C._nn.linear(rms_norm_24, l_self_modules_transformer_modules_h_modules_6_modules_attn_modules_c_q_parameters_weight_, None);  l_self_modules_transformer_modules_h_modules_6_modules_attn_modules_c_q_parameters_weight_ = None
[rank0]:             view: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = linear.view(16, 256, 6, 128);  linear = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:159 in forward, code: k = self.c_k(x).view(B, T, self.n_head, self.head_dim)
[rank0]:             linear_1: "bf16[16, 256, 768][196608, 768, 1]" = torch._C._nn.linear(rms_norm_24, l_self_modules_transformer_modules_h_modules_6_modules_attn_modules_c_k_parameters_weight_, None);  l_self_modules_transformer_modules_h_modules_6_modules_attn_modules_c_k_parameters_weight_ = None
[rank0]:             view_1: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = linear_1.view(16, 256, 6, 128);  linear_1 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:160 in forward, code: v = self.c_v(x).view(B, T, self.n_head, self.head_dim)
[rank0]:             linear_2: "bf16[16, 256, 768][196608, 768, 1]" = torch._C._nn.linear(rms_norm_24, l_self_modules_transformer_modules_h_modules_6_modules_attn_modules_c_v_parameters_weight_, None);  l_self_modules_transformer_modules_h_modules_6_modules_attn_modules_c_v_parameters_weight_ = None
[rank0]:             view_2: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = linear_2.view(16, 256, 6, 128);  linear_2 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:129 in forward, code: return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]
[rank0]:             getitem: "bf16[1, 256, 1, 64][16384, 64, 64, 1]" = l_self_modules_transformer_modules_h_modules_6_modules_attn_modules_rotary_cos_cached[(None, slice(None, None, None), None, slice(None, None, None))];  l_self_modules_transformer_modules_h_modules_6_modules_attn_modules_rotary_cos_cached = None
[rank0]:             getitem_1: "bf16[1, 256, 1, 64][16384, 64, 64, 1]" = l_self_modules_transformer_modules_h_modules_6_modules_attn_modules_rotary_sin_cached[(None, slice(None, None, None), None, slice(None, None, None))];  l_self_modules_transformer_modules_h_modules_6_modules_attn_modules_rotary_sin_cached = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:134 in apply_rotary_emb, code: x1 = x[..., :d]
[rank0]:             getitem_2: "bf16[16, 256, 6, 64][196608, 768, 128, 1]" = view[(Ellipsis, slice(None, 64, None))]
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:135 in apply_rotary_emb, code: x2 = x[..., d:]
[rank0]:             getitem_3: "bf16[16, 256, 6, 64][196608, 768, 128, 1]" = view[(Ellipsis, slice(64, None, None))]
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:136 in apply_rotary_emb, code: y1 = x1 * cos + x2 * sin
[rank0]:             mul: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = getitem_2 * getitem
[rank0]:             mul_1: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = getitem_3 * getitem_1
[rank0]:             add: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = mul + mul_1;  mul = mul_1 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:137 in apply_rotary_emb, code: y2 = x1 * (-sin) + x2 * cos
[rank0]:             neg: "bf16[1, 256, 1, 64][16384, 64, 64, 1]" = -getitem_1
[rank0]:             mul_2: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = getitem_2 * neg;  getitem_2 = neg = None
[rank0]:             mul_3: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = getitem_3 * getitem;  getitem_3 = None
[rank0]:             add_1: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = mul_2 + mul_3;  mul_2 = mul_3 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:138 in apply_rotary_emb, code: return torch.cat([y1, y2], 3).type_as(x)
[rank0]:             cat: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = torch.cat([add, add_1], 3);  add = add_1 = None
[rank0]:             type_as: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = cat.type_as(view);  cat = view = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:134 in apply_rotary_emb, code: x1 = x[..., :d]
[rank0]:             getitem_4: "bf16[16, 256, 6, 64][196608, 768, 128, 1]" = view_1[(Ellipsis, slice(None, 64, None))]
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:135 in apply_rotary_emb, code: x2 = x[..., d:]
[rank0]:             getitem_5: "bf16[16, 256, 6, 64][196608, 768, 128, 1]" = view_1[(Ellipsis, slice(64, None, None))]
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:136 in apply_rotary_emb, code: y1 = x1 * cos + x2 * sin
[rank0]:             mul_4: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = getitem_4 * getitem
[rank0]:             mul_5: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = getitem_5 * getitem_1
[rank0]:             add_2: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = mul_4 + mul_5;  mul_4 = mul_5 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:137 in apply_rotary_emb, code: y2 = x1 * (-sin) + x2 * cos
[rank0]:             neg_1: "bf16[1, 256, 1, 64][16384, 64, 64, 1]" = -getitem_1;  getitem_1 = None
[rank0]:             mul_6: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = getitem_4 * neg_1;  getitem_4 = neg_1 = None
[rank0]:             mul_7: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = getitem_5 * getitem;  getitem_5 = getitem = None
[rank0]:             add_3: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = mul_6 + mul_7;  mul_6 = mul_7 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:138 in apply_rotary_emb, code: return torch.cat([y1, y2], 3).type_as(x)
[rank0]:             cat_1: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = torch.cat([add_2, add_3], 3);  add_2 = add_3 = None
[rank0]:             type_as_1: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = cat_1.type_as(view_1);  cat_1 = view_1 = None
[rank0]:             
[rank0]:              # File: /usr/local/lib/python3.12/dist-packages/torch/nn/functional.py:2924 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
[rank0]:             rms_norm: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = torch.rms_norm(type_as, (128,), None, None);  type_as = None
[rank0]:             rms_norm_25: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = torch.rms_norm(type_as_1, (128,), None, None);  type_as_1 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:164 in forward, code: y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)
[rank0]:             transpose: "bf16[16, 6, 256, 128][196608, 128, 768, 1]" = rms_norm.transpose(1, 2);  rms_norm = None
[rank0]:             transpose_1: "bf16[16, 6, 256, 128][196608, 128, 768, 1]" = rms_norm_25.transpose(1, 2);  rms_norm_25 = None
[rank0]:             transpose_2: "bf16[16, 6, 256, 128][196608, 128, 768, 1]" = view_2.transpose(1, 2);  view_2 = None
[rank0]:             scaled_dot_product_attention: "bf16[16, 6, 256, 128][196608, 128, 768, 1]" = torch._C._nn.scaled_dot_product_attention(transpose, transpose_1, transpose_2, is_causal = True);  transpose = transpose_1 = transpose_2 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:165 in forward, code: y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
[rank0]:             transpose_3: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = scaled_dot_product_attention.transpose(1, 2);  scaled_dot_product_attention = None
[rank0]:             contiguous: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = transpose_3.contiguous();  transpose_3 = None
[rank0]:             view_as: "bf16[16, 256, 768][196608, 768, 1]" = contiguous.view_as(rms_norm_24);  contiguous = rms_norm_24 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:166 in forward, code: y = self.c_proj(y)
[rank0]:             linear_3: "bf16[16, 256, 768][196608, 768, 1]" = torch._C._nn.linear(view_as, l_self_modules_transformer_modules_h_modules_6_modules_attn_modules_c_proj_parameters_weight_, None);  view_as = l_self_modules_transformer_modules_h_modules_6_modules_attn_modules_c_proj_parameters_weight_ = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:191 in forward, code: x = x + self.attn(F.rms_norm(x, (x.size(-1),)))
[rank0]:             add_4: "f32[16, 256, 768][196608, 768, 1]" = x_30 + linear_3;  x_30 = linear_3 = None
[rank0]:             
[rank0]:              # File: /usr/local/lib/python3.12/dist-packages/torch/nn/functional.py:2924 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
[rank0]:             rms_norm_26: "f32[16, 256, 768][196608, 768, 1]" = torch.rms_norm(add_4, (768,), None, None)
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:178 in forward, code: x = self.c_fc(x)
[rank0]:             linear_4: "bf16[16, 256, 3072][786432, 3072, 1]" = torch._C._nn.linear(rms_norm_26, l_self_modules_transformer_modules_h_modules_6_modules_mlp_modules_c_fc_parameters_weight_, None);  rms_norm_26 = l_self_modules_transformer_modules_h_modules_6_modules_mlp_modules_c_fc_parameters_weight_ = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:179 in forward, code: x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
[rank0]:             relu: "bf16[16, 256, 3072][786432, 3072, 1]" = torch.nn.functional.relu(linear_4);  linear_4 = None
[rank0]:             square: "f32[16, 256, 3072][786432, 3072, 1]" = relu.square();  relu = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:180 in forward, code: x = self.c_proj(x)
[rank0]:             linear_5: "bf16[16, 256, 768][196608, 768, 1]" = torch._C._nn.linear(square, l_self_modules_transformer_modules_h_modules_6_modules_mlp_modules_c_proj_parameters_weight_, None);  square = l_self_modules_transformer_modules_h_modules_6_modules_mlp_modules_c_proj_parameters_weight_ = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:192 in forward, code: x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))
[rank0]:             add_5: "f32[16, 256, 768][196608, 768, 1]" = add_4 + linear_5;  add_4 = linear_5 = None
[rank0]:             
[rank0]:              # File: /usr/local/lib/python3.12/dist-packages/torch/nn/functional.py:2924 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
[rank0]:             rms_norm_27: "f32[16, 256, 768][196608, 768, 1]" = torch.rms_norm(add_5, (768,), None, None)
[rank0]:             return (rms_norm_27, add_5)
[rank0]:             
[rank0]:     class submod_8(torch.nn.Module):
[rank0]:         def forward(self, rms_norm_28: "f32[16, 256, 768][196608, 768, 1]", l_self_modules_transformer_modules_h_modules_7_modules_attn_modules_c_q_parameters_weight_: "f32[768, 768][768, 1]", l_self_modules_transformer_modules_h_modules_7_modules_attn_modules_c_k_parameters_weight_: "f32[768, 768][768, 1]", l_self_modules_transformer_modules_h_modules_7_modules_attn_modules_c_v_parameters_weight_: "f32[768, 768][768, 1]", l_self_modules_transformer_modules_h_modules_7_modules_attn_modules_rotary_cos_cached: "bf16[256, 64][64, 1]", l_self_modules_transformer_modules_h_modules_7_modules_attn_modules_rotary_sin_cached: "bf16[256, 64][64, 1]", l_self_modules_transformer_modules_h_modules_7_modules_attn_modules_c_proj_parameters_weight_: "f32[768, 768][768, 1]", x_35: "f32[16, 256, 768][196608, 768, 1]", l_self_modules_transformer_modules_h_modules_7_modules_mlp_modules_c_fc_parameters_weight_: "f32[3072, 768][768, 1]", l_self_modules_transformer_modules_h_modules_7_modules_mlp_modules_c_proj_parameters_weight_: "f32[768, 3072][3072, 1]"):
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:158 in forward, code: q = self.c_q(x).view(B, T, self.n_head, self.head_dim)
[rank0]:             linear: "bf16[16, 256, 768][196608, 768, 1]" = torch._C._nn.linear(rms_norm_28, l_self_modules_transformer_modules_h_modules_7_modules_attn_modules_c_q_parameters_weight_, None);  l_self_modules_transformer_modules_h_modules_7_modules_attn_modules_c_q_parameters_weight_ = None
[rank0]:             view: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = linear.view(16, 256, 6, 128);  linear = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:159 in forward, code: k = self.c_k(x).view(B, T, self.n_head, self.head_dim)
[rank0]:             linear_1: "bf16[16, 256, 768][196608, 768, 1]" = torch._C._nn.linear(rms_norm_28, l_self_modules_transformer_modules_h_modules_7_modules_attn_modules_c_k_parameters_weight_, None);  l_self_modules_transformer_modules_h_modules_7_modules_attn_modules_c_k_parameters_weight_ = None
[rank0]:             view_1: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = linear_1.view(16, 256, 6, 128);  linear_1 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:160 in forward, code: v = self.c_v(x).view(B, T, self.n_head, self.head_dim)
[rank0]:             linear_2: "bf16[16, 256, 768][196608, 768, 1]" = torch._C._nn.linear(rms_norm_28, l_self_modules_transformer_modules_h_modules_7_modules_attn_modules_c_v_parameters_weight_, None);  l_self_modules_transformer_modules_h_modules_7_modules_attn_modules_c_v_parameters_weight_ = None
[rank0]:             view_2: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = linear_2.view(16, 256, 6, 128);  linear_2 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:129 in forward, code: return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]
[rank0]:             getitem: "bf16[1, 256, 1, 64][16384, 64, 64, 1]" = l_self_modules_transformer_modules_h_modules_7_modules_attn_modules_rotary_cos_cached[(None, slice(None, None, None), None, slice(None, None, None))];  l_self_modules_transformer_modules_h_modules_7_modules_attn_modules_rotary_cos_cached = None
[rank0]:             getitem_1: "bf16[1, 256, 1, 64][16384, 64, 64, 1]" = l_self_modules_transformer_modules_h_modules_7_modules_attn_modules_rotary_sin_cached[(None, slice(None, None, None), None, slice(None, None, None))];  l_self_modules_transformer_modules_h_modules_7_modules_attn_modules_rotary_sin_cached = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:134 in apply_rotary_emb, code: x1 = x[..., :d]
[rank0]:             getitem_2: "bf16[16, 256, 6, 64][196608, 768, 128, 1]" = view[(Ellipsis, slice(None, 64, None))]
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:135 in apply_rotary_emb, code: x2 = x[..., d:]
[rank0]:             getitem_3: "bf16[16, 256, 6, 64][196608, 768, 128, 1]" = view[(Ellipsis, slice(64, None, None))]
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:136 in apply_rotary_emb, code: y1 = x1 * cos + x2 * sin
[rank0]:             mul: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = getitem_2 * getitem
[rank0]:             mul_1: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = getitem_3 * getitem_1
[rank0]:             add: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = mul + mul_1;  mul = mul_1 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:137 in apply_rotary_emb, code: y2 = x1 * (-sin) + x2 * cos
[rank0]:             neg: "bf16[1, 256, 1, 64][16384, 64, 64, 1]" = -getitem_1
[rank0]:             mul_2: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = getitem_2 * neg;  getitem_2 = neg = None
[rank0]:             mul_3: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = getitem_3 * getitem;  getitem_3 = None
[rank0]:             add_1: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = mul_2 + mul_3;  mul_2 = mul_3 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:138 in apply_rotary_emb, code: return torch.cat([y1, y2], 3).type_as(x)
[rank0]:             cat: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = torch.cat([add, add_1], 3);  add = add_1 = None
[rank0]:             type_as: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = cat.type_as(view);  cat = view = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:134 in apply_rotary_emb, code: x1 = x[..., :d]
[rank0]:             getitem_4: "bf16[16, 256, 6, 64][196608, 768, 128, 1]" = view_1[(Ellipsis, slice(None, 64, None))]
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:135 in apply_rotary_emb, code: x2 = x[..., d:]
[rank0]:             getitem_5: "bf16[16, 256, 6, 64][196608, 768, 128, 1]" = view_1[(Ellipsis, slice(64, None, None))]
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:136 in apply_rotary_emb, code: y1 = x1 * cos + x2 * sin
[rank0]:             mul_4: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = getitem_4 * getitem
[rank0]:             mul_5: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = getitem_5 * getitem_1
[rank0]:             add_2: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = mul_4 + mul_5;  mul_4 = mul_5 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:137 in apply_rotary_emb, code: y2 = x1 * (-sin) + x2 * cos
[rank0]:             neg_1: "bf16[1, 256, 1, 64][16384, 64, 64, 1]" = -getitem_1;  getitem_1 = None
[rank0]:             mul_6: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = getitem_4 * neg_1;  getitem_4 = neg_1 = None
[rank0]:             mul_7: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = getitem_5 * getitem;  getitem_5 = getitem = None
[rank0]:             add_3: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = mul_6 + mul_7;  mul_6 = mul_7 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:138 in apply_rotary_emb, code: return torch.cat([y1, y2], 3).type_as(x)
[rank0]:             cat_1: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = torch.cat([add_2, add_3], 3);  add_2 = add_3 = None
[rank0]:             type_as_1: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = cat_1.type_as(view_1);  cat_1 = view_1 = None
[rank0]:             
[rank0]:              # File: /usr/local/lib/python3.12/dist-packages/torch/nn/functional.py:2924 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
[rank0]:             rms_norm: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = torch.rms_norm(type_as, (128,), None, None);  type_as = None
[rank0]:             rms_norm_29: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = torch.rms_norm(type_as_1, (128,), None, None);  type_as_1 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:164 in forward, code: y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)
[rank0]:             transpose: "bf16[16, 6, 256, 128][196608, 128, 768, 1]" = rms_norm.transpose(1, 2);  rms_norm = None
[rank0]:             transpose_1: "bf16[16, 6, 256, 128][196608, 128, 768, 1]" = rms_norm_29.transpose(1, 2);  rms_norm_29 = None
[rank0]:             transpose_2: "bf16[16, 6, 256, 128][196608, 128, 768, 1]" = view_2.transpose(1, 2);  view_2 = None
[rank0]:             scaled_dot_product_attention: "bf16[16, 6, 256, 128][196608, 128, 768, 1]" = torch._C._nn.scaled_dot_product_attention(transpose, transpose_1, transpose_2, is_causal = True);  transpose = transpose_1 = transpose_2 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:165 in forward, code: y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
[rank0]:             transpose_3: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = scaled_dot_product_attention.transpose(1, 2);  scaled_dot_product_attention = None
[rank0]:             contiguous: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = transpose_3.contiguous();  transpose_3 = None
[rank0]:             view_as: "bf16[16, 256, 768][196608, 768, 1]" = contiguous.view_as(rms_norm_28);  contiguous = rms_norm_28 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:166 in forward, code: y = self.c_proj(y)
[rank0]:             linear_3: "bf16[16, 256, 768][196608, 768, 1]" = torch._C._nn.linear(view_as, l_self_modules_transformer_modules_h_modules_7_modules_attn_modules_c_proj_parameters_weight_, None);  view_as = l_self_modules_transformer_modules_h_modules_7_modules_attn_modules_c_proj_parameters_weight_ = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:191 in forward, code: x = x + self.attn(F.rms_norm(x, (x.size(-1),)))
[rank0]:             add_4: "f32[16, 256, 768][196608, 768, 1]" = x_35 + linear_3;  x_35 = linear_3 = None
[rank0]:             
[rank0]:              # File: /usr/local/lib/python3.12/dist-packages/torch/nn/functional.py:2924 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
[rank0]:             rms_norm_30: "f32[16, 256, 768][196608, 768, 1]" = torch.rms_norm(add_4, (768,), None, None)
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:178 in forward, code: x = self.c_fc(x)
[rank0]:             linear_4: "bf16[16, 256, 3072][786432, 3072, 1]" = torch._C._nn.linear(rms_norm_30, l_self_modules_transformer_modules_h_modules_7_modules_mlp_modules_c_fc_parameters_weight_, None);  rms_norm_30 = l_self_modules_transformer_modules_h_modules_7_modules_mlp_modules_c_fc_parameters_weight_ = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:179 in forward, code: x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
[rank0]:             relu: "bf16[16, 256, 3072][786432, 3072, 1]" = torch.nn.functional.relu(linear_4);  linear_4 = None
[rank0]:             square: "f32[16, 256, 3072][786432, 3072, 1]" = relu.square();  relu = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:180 in forward, code: x = self.c_proj(x)
[rank0]:             linear_5: "bf16[16, 256, 768][196608, 768, 1]" = torch._C._nn.linear(square, l_self_modules_transformer_modules_h_modules_7_modules_mlp_modules_c_proj_parameters_weight_, None);  square = l_self_modules_transformer_modules_h_modules_7_modules_mlp_modules_c_proj_parameters_weight_ = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:192 in forward, code: x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))
[rank0]:             add_5: "f32[16, 256, 768][196608, 768, 1]" = add_4 + linear_5;  add_4 = linear_5 = None
[rank0]:             
[rank0]:              # File: /usr/local/lib/python3.12/dist-packages/torch/nn/functional.py:2924 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
[rank0]:             rms_norm_31: "f32[16, 256, 768][196608, 768, 1]" = torch.rms_norm(add_5, (768,), None, None)
[rank0]:             return (rms_norm_31, add_5)
[rank0]:             
[rank0]:     class submod_9(torch.nn.Module):
[rank0]:         def forward(self, rms_norm_32: "f32[16, 256, 768][196608, 768, 1]", l_self_modules_transformer_modules_h_modules_8_modules_attn_modules_c_q_parameters_weight_: "f32[768, 768][768, 1]", l_self_modules_transformer_modules_h_modules_8_modules_attn_modules_c_k_parameters_weight_: "f32[768, 768][768, 1]", l_self_modules_transformer_modules_h_modules_8_modules_attn_modules_c_v_parameters_weight_: "f32[768, 768][768, 1]", l_self_modules_transformer_modules_h_modules_8_modules_attn_modules_rotary_cos_cached: "bf16[256, 64][64, 1]", l_self_modules_transformer_modules_h_modules_8_modules_attn_modules_rotary_sin_cached: "bf16[256, 64][64, 1]", l_self_modules_transformer_modules_h_modules_8_modules_attn_modules_c_proj_parameters_weight_: "f32[768, 768][768, 1]", x_40: "f32[16, 256, 768][196608, 768, 1]", l_self_modules_transformer_modules_h_modules_8_modules_mlp_modules_c_fc_parameters_weight_: "f32[3072, 768][768, 1]", l_self_modules_transformer_modules_h_modules_8_modules_mlp_modules_c_proj_parameters_weight_: "f32[768, 3072][3072, 1]"):
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:158 in forward, code: q = self.c_q(x).view(B, T, self.n_head, self.head_dim)
[rank0]:             linear: "bf16[16, 256, 768][196608, 768, 1]" = torch._C._nn.linear(rms_norm_32, l_self_modules_transformer_modules_h_modules_8_modules_attn_modules_c_q_parameters_weight_, None);  l_self_modules_transformer_modules_h_modules_8_modules_attn_modules_c_q_parameters_weight_ = None
[rank0]:             view: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = linear.view(16, 256, 6, 128);  linear = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:159 in forward, code: k = self.c_k(x).view(B, T, self.n_head, self.head_dim)
[rank0]:             linear_1: "bf16[16, 256, 768][196608, 768, 1]" = torch._C._nn.linear(rms_norm_32, l_self_modules_transformer_modules_h_modules_8_modules_attn_modules_c_k_parameters_weight_, None);  l_self_modules_transformer_modules_h_modules_8_modules_attn_modules_c_k_parameters_weight_ = None
[rank0]:             view_1: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = linear_1.view(16, 256, 6, 128);  linear_1 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:160 in forward, code: v = self.c_v(x).view(B, T, self.n_head, self.head_dim)
[rank0]:             linear_2: "bf16[16, 256, 768][196608, 768, 1]" = torch._C._nn.linear(rms_norm_32, l_self_modules_transformer_modules_h_modules_8_modules_attn_modules_c_v_parameters_weight_, None);  l_self_modules_transformer_modules_h_modules_8_modules_attn_modules_c_v_parameters_weight_ = None
[rank0]:             view_2: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = linear_2.view(16, 256, 6, 128);  linear_2 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:129 in forward, code: return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]
[rank0]:             getitem: "bf16[1, 256, 1, 64][16384, 64, 64, 1]" = l_self_modules_transformer_modules_h_modules_8_modules_attn_modules_rotary_cos_cached[(None, slice(None, None, None), None, slice(None, None, None))];  l_self_modules_transformer_modules_h_modules_8_modules_attn_modules_rotary_cos_cached = None
[rank0]:             getitem_1: "bf16[1, 256, 1, 64][16384, 64, 64, 1]" = l_self_modules_transformer_modules_h_modules_8_modules_attn_modules_rotary_sin_cached[(None, slice(None, None, None), None, slice(None, None, None))];  l_self_modules_transformer_modules_h_modules_8_modules_attn_modules_rotary_sin_cached = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:134 in apply_rotary_emb, code: x1 = x[..., :d]
[rank0]:             getitem_2: "bf16[16, 256, 6, 64][196608, 768, 128, 1]" = view[(Ellipsis, slice(None, 64, None))]
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:135 in apply_rotary_emb, code: x2 = x[..., d:]
[rank0]:             getitem_3: "bf16[16, 256, 6, 64][196608, 768, 128, 1]" = view[(Ellipsis, slice(64, None, None))]
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:136 in apply_rotary_emb, code: y1 = x1 * cos + x2 * sin
[rank0]:             mul: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = getitem_2 * getitem
[rank0]:             mul_1: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = getitem_3 * getitem_1
[rank0]:             add: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = mul + mul_1;  mul = mul_1 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:137 in apply_rotary_emb, code: y2 = x1 * (-sin) + x2 * cos
[rank0]:             neg: "bf16[1, 256, 1, 64][16384, 64, 64, 1]" = -getitem_1
[rank0]:             mul_2: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = getitem_2 * neg;  getitem_2 = neg = None
[rank0]:             mul_3: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = getitem_3 * getitem;  getitem_3 = None
[rank0]:             add_1: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = mul_2 + mul_3;  mul_2 = mul_3 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:138 in apply_rotary_emb, code: return torch.cat([y1, y2], 3).type_as(x)
[rank0]:             cat: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = torch.cat([add, add_1], 3);  add = add_1 = None
[rank0]:             type_as: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = cat.type_as(view);  cat = view = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:134 in apply_rotary_emb, code: x1 = x[..., :d]
[rank0]:             getitem_4: "bf16[16, 256, 6, 64][196608, 768, 128, 1]" = view_1[(Ellipsis, slice(None, 64, None))]
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:135 in apply_rotary_emb, code: x2 = x[..., d:]
[rank0]:             getitem_5: "bf16[16, 256, 6, 64][196608, 768, 128, 1]" = view_1[(Ellipsis, slice(64, None, None))]
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:136 in apply_rotary_emb, code: y1 = x1 * cos + x2 * sin
[rank0]:             mul_4: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = getitem_4 * getitem
[rank0]:             mul_5: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = getitem_5 * getitem_1
[rank0]:             add_2: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = mul_4 + mul_5;  mul_4 = mul_5 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:137 in apply_rotary_emb, code: y2 = x1 * (-sin) + x2 * cos
[rank0]:             neg_1: "bf16[1, 256, 1, 64][16384, 64, 64, 1]" = -getitem_1;  getitem_1 = None
[rank0]:             mul_6: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = getitem_4 * neg_1;  getitem_4 = neg_1 = None
[rank0]:             mul_7: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = getitem_5 * getitem;  getitem_5 = getitem = None
[rank0]:             add_3: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = mul_6 + mul_7;  mul_6 = mul_7 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:138 in apply_rotary_emb, code: return torch.cat([y1, y2], 3).type_as(x)
[rank0]:             cat_1: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = torch.cat([add_2, add_3], 3);  add_2 = add_3 = None
[rank0]:             type_as_1: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = cat_1.type_as(view_1);  cat_1 = view_1 = None
[rank0]:             
[rank0]:              # File: /usr/local/lib/python3.12/dist-packages/torch/nn/functional.py:2924 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
[rank0]:             rms_norm: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = torch.rms_norm(type_as, (128,), None, None);  type_as = None
[rank0]:             rms_norm_33: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = torch.rms_norm(type_as_1, (128,), None, None);  type_as_1 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:164 in forward, code: y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)
[rank0]:             transpose: "bf16[16, 6, 256, 128][196608, 128, 768, 1]" = rms_norm.transpose(1, 2);  rms_norm = None
[rank0]:             transpose_1: "bf16[16, 6, 256, 128][196608, 128, 768, 1]" = rms_norm_33.transpose(1, 2);  rms_norm_33 = None
[rank0]:             transpose_2: "bf16[16, 6, 256, 128][196608, 128, 768, 1]" = view_2.transpose(1, 2);  view_2 = None
[rank0]:             scaled_dot_product_attention: "bf16[16, 6, 256, 128][196608, 128, 768, 1]" = torch._C._nn.scaled_dot_product_attention(transpose, transpose_1, transpose_2, is_causal = True);  transpose = transpose_1 = transpose_2 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:165 in forward, code: y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
[rank0]:             transpose_3: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = scaled_dot_product_attention.transpose(1, 2);  scaled_dot_product_attention = None
[rank0]:             contiguous: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = transpose_3.contiguous();  transpose_3 = None
[rank0]:             view_as: "bf16[16, 256, 768][196608, 768, 1]" = contiguous.view_as(rms_norm_32);  contiguous = rms_norm_32 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:166 in forward, code: y = self.c_proj(y)
[rank0]:             linear_3: "bf16[16, 256, 768][196608, 768, 1]" = torch._C._nn.linear(view_as, l_self_modules_transformer_modules_h_modules_8_modules_attn_modules_c_proj_parameters_weight_, None);  view_as = l_self_modules_transformer_modules_h_modules_8_modules_attn_modules_c_proj_parameters_weight_ = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:191 in forward, code: x = x + self.attn(F.rms_norm(x, (x.size(-1),)))
[rank0]:             add_4: "f32[16, 256, 768][196608, 768, 1]" = x_40 + linear_3;  x_40 = linear_3 = None
[rank0]:             
[rank0]:              # File: /usr/local/lib/python3.12/dist-packages/torch/nn/functional.py:2924 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
[rank0]:             rms_norm_34: "f32[16, 256, 768][196608, 768, 1]" = torch.rms_norm(add_4, (768,), None, None)
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:178 in forward, code: x = self.c_fc(x)
[rank0]:             linear_4: "bf16[16, 256, 3072][786432, 3072, 1]" = torch._C._nn.linear(rms_norm_34, l_self_modules_transformer_modules_h_modules_8_modules_mlp_modules_c_fc_parameters_weight_, None);  rms_norm_34 = l_self_modules_transformer_modules_h_modules_8_modules_mlp_modules_c_fc_parameters_weight_ = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:179 in forward, code: x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
[rank0]:             relu: "bf16[16, 256, 3072][786432, 3072, 1]" = torch.nn.functional.relu(linear_4);  linear_4 = None
[rank0]:             square: "f32[16, 256, 3072][786432, 3072, 1]" = relu.square();  relu = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:180 in forward, code: x = self.c_proj(x)
[rank0]:             linear_5: "bf16[16, 256, 768][196608, 768, 1]" = torch._C._nn.linear(square, l_self_modules_transformer_modules_h_modules_8_modules_mlp_modules_c_proj_parameters_weight_, None);  square = l_self_modules_transformer_modules_h_modules_8_modules_mlp_modules_c_proj_parameters_weight_ = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:192 in forward, code: x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))
[rank0]:             add_5: "f32[16, 256, 768][196608, 768, 1]" = add_4 + linear_5;  add_4 = linear_5 = None
[rank0]:             
[rank0]:              # File: /usr/local/lib/python3.12/dist-packages/torch/nn/functional.py:2924 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
[rank0]:             rms_norm_35: "f32[16, 256, 768][196608, 768, 1]" = torch.rms_norm(add_5, (768,), None, None)
[rank0]:             return (rms_norm_35, add_5)
[rank0]:             
[rank0]:     class submod_10(torch.nn.Module):
[rank0]:         def forward(self, rms_norm_36: "f32[16, 256, 768][196608, 768, 1]", l_self_modules_transformer_modules_h_modules_9_modules_attn_modules_c_q_parameters_weight_: "f32[768, 768][768, 1]", l_self_modules_transformer_modules_h_modules_9_modules_attn_modules_c_k_parameters_weight_: "f32[768, 768][768, 1]", l_self_modules_transformer_modules_h_modules_9_modules_attn_modules_c_v_parameters_weight_: "f32[768, 768][768, 1]", l_self_modules_transformer_modules_h_modules_9_modules_attn_modules_rotary_cos_cached: "bf16[256, 64][64, 1]", l_self_modules_transformer_modules_h_modules_9_modules_attn_modules_rotary_sin_cached: "bf16[256, 64][64, 1]", l_self_modules_transformer_modules_h_modules_9_modules_attn_modules_c_proj_parameters_weight_: "f32[768, 768][768, 1]", x_45: "f32[16, 256, 768][196608, 768, 1]", l_self_modules_transformer_modules_h_modules_9_modules_mlp_modules_c_fc_parameters_weight_: "f32[3072, 768][768, 1]", l_self_modules_transformer_modules_h_modules_9_modules_mlp_modules_c_proj_parameters_weight_: "f32[768, 3072][3072, 1]"):
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:158 in forward, code: q = self.c_q(x).view(B, T, self.n_head, self.head_dim)
[rank0]:             linear: "bf16[16, 256, 768][196608, 768, 1]" = torch._C._nn.linear(rms_norm_36, l_self_modules_transformer_modules_h_modules_9_modules_attn_modules_c_q_parameters_weight_, None);  l_self_modules_transformer_modules_h_modules_9_modules_attn_modules_c_q_parameters_weight_ = None
[rank0]:             view: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = linear.view(16, 256, 6, 128);  linear = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:159 in forward, code: k = self.c_k(x).view(B, T, self.n_head, self.head_dim)
[rank0]:             linear_1: "bf16[16, 256, 768][196608, 768, 1]" = torch._C._nn.linear(rms_norm_36, l_self_modules_transformer_modules_h_modules_9_modules_attn_modules_c_k_parameters_weight_, None);  l_self_modules_transformer_modules_h_modules_9_modules_attn_modules_c_k_parameters_weight_ = None
[rank0]:             view_1: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = linear_1.view(16, 256, 6, 128);  linear_1 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:160 in forward, code: v = self.c_v(x).view(B, T, self.n_head, self.head_dim)
[rank0]:             linear_2: "bf16[16, 256, 768][196608, 768, 1]" = torch._C._nn.linear(rms_norm_36, l_self_modules_transformer_modules_h_modules_9_modules_attn_modules_c_v_parameters_weight_, None);  l_self_modules_transformer_modules_h_modules_9_modules_attn_modules_c_v_parameters_weight_ = None
[rank0]:             view_2: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = linear_2.view(16, 256, 6, 128);  linear_2 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:129 in forward, code: return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]
[rank0]:             getitem: "bf16[1, 256, 1, 64][16384, 64, 64, 1]" = l_self_modules_transformer_modules_h_modules_9_modules_attn_modules_rotary_cos_cached[(None, slice(None, None, None), None, slice(None, None, None))];  l_self_modules_transformer_modules_h_modules_9_modules_attn_modules_rotary_cos_cached = None
[rank0]:             getitem_1: "bf16[1, 256, 1, 64][16384, 64, 64, 1]" = l_self_modules_transformer_modules_h_modules_9_modules_attn_modules_rotary_sin_cached[(None, slice(None, None, None), None, slice(None, None, None))];  l_self_modules_transformer_modules_h_modules_9_modules_attn_modules_rotary_sin_cached = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:134 in apply_rotary_emb, code: x1 = x[..., :d]
[rank0]:             getitem_2: "bf16[16, 256, 6, 64][196608, 768, 128, 1]" = view[(Ellipsis, slice(None, 64, None))]
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:135 in apply_rotary_emb, code: x2 = x[..., d:]
[rank0]:             getitem_3: "bf16[16, 256, 6, 64][196608, 768, 128, 1]" = view[(Ellipsis, slice(64, None, None))]
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:136 in apply_rotary_emb, code: y1 = x1 * cos + x2 * sin
[rank0]:             mul: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = getitem_2 * getitem
[rank0]:             mul_1: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = getitem_3 * getitem_1
[rank0]:             add: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = mul + mul_1;  mul = mul_1 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:137 in apply_rotary_emb, code: y2 = x1 * (-sin) + x2 * cos
[rank0]:             neg: "bf16[1, 256, 1, 64][16384, 64, 64, 1]" = -getitem_1
[rank0]:             mul_2: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = getitem_2 * neg;  getitem_2 = neg = None
[rank0]:             mul_3: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = getitem_3 * getitem;  getitem_3 = None
[rank0]:             add_1: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = mul_2 + mul_3;  mul_2 = mul_3 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:138 in apply_rotary_emb, code: return torch.cat([y1, y2], 3).type_as(x)
[rank0]:             cat: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = torch.cat([add, add_1], 3);  add = add_1 = None
[rank0]:             type_as: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = cat.type_as(view);  cat = view = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:134 in apply_rotary_emb, code: x1 = x[..., :d]
[rank0]:             getitem_4: "bf16[16, 256, 6, 64][196608, 768, 128, 1]" = view_1[(Ellipsis, slice(None, 64, None))]
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:135 in apply_rotary_emb, code: x2 = x[..., d:]
[rank0]:             getitem_5: "bf16[16, 256, 6, 64][196608, 768, 128, 1]" = view_1[(Ellipsis, slice(64, None, None))]
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:136 in apply_rotary_emb, code: y1 = x1 * cos + x2 * sin
[rank0]:             mul_4: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = getitem_4 * getitem
[rank0]:             mul_5: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = getitem_5 * getitem_1
[rank0]:             add_2: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = mul_4 + mul_5;  mul_4 = mul_5 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:137 in apply_rotary_emb, code: y2 = x1 * (-sin) + x2 * cos
[rank0]:             neg_1: "bf16[1, 256, 1, 64][16384, 64, 64, 1]" = -getitem_1;  getitem_1 = None
[rank0]:             mul_6: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = getitem_4 * neg_1;  getitem_4 = neg_1 = None
[rank0]:             mul_7: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = getitem_5 * getitem;  getitem_5 = getitem = None
[rank0]:             add_3: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = mul_6 + mul_7;  mul_6 = mul_7 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:138 in apply_rotary_emb, code: return torch.cat([y1, y2], 3).type_as(x)
[rank0]:             cat_1: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = torch.cat([add_2, add_3], 3);  add_2 = add_3 = None
[rank0]:             type_as_1: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = cat_1.type_as(view_1);  cat_1 = view_1 = None
[rank0]:             
[rank0]:              # File: /usr/local/lib/python3.12/dist-packages/torch/nn/functional.py:2924 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
[rank0]:             rms_norm: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = torch.rms_norm(type_as, (128,), None, None);  type_as = None
[rank0]:             rms_norm_37: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = torch.rms_norm(type_as_1, (128,), None, None);  type_as_1 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:164 in forward, code: y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)
[rank0]:             transpose: "bf16[16, 6, 256, 128][196608, 128, 768, 1]" = rms_norm.transpose(1, 2);  rms_norm = None
[rank0]:             transpose_1: "bf16[16, 6, 256, 128][196608, 128, 768, 1]" = rms_norm_37.transpose(1, 2);  rms_norm_37 = None
[rank0]:             transpose_2: "bf16[16, 6, 256, 128][196608, 128, 768, 1]" = view_2.transpose(1, 2);  view_2 = None
[rank0]:             scaled_dot_product_attention: "bf16[16, 6, 256, 128][196608, 128, 768, 1]" = torch._C._nn.scaled_dot_product_attention(transpose, transpose_1, transpose_2, is_causal = True);  transpose = transpose_1 = transpose_2 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:165 in forward, code: y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
[rank0]:             transpose_3: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = scaled_dot_product_attention.transpose(1, 2);  scaled_dot_product_attention = None
[rank0]:             contiguous: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = transpose_3.contiguous();  transpose_3 = None
[rank0]:             view_as: "bf16[16, 256, 768][196608, 768, 1]" = contiguous.view_as(rms_norm_36);  contiguous = rms_norm_36 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:166 in forward, code: y = self.c_proj(y)
[rank0]:             linear_3: "bf16[16, 256, 768][196608, 768, 1]" = torch._C._nn.linear(view_as, l_self_modules_transformer_modules_h_modules_9_modules_attn_modules_c_proj_parameters_weight_, None);  view_as = l_self_modules_transformer_modules_h_modules_9_modules_attn_modules_c_proj_parameters_weight_ = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:191 in forward, code: x = x + self.attn(F.rms_norm(x, (x.size(-1),)))
[rank0]:             add_4: "f32[16, 256, 768][196608, 768, 1]" = x_45 + linear_3;  x_45 = linear_3 = None
[rank0]:             
[rank0]:              # File: /usr/local/lib/python3.12/dist-packages/torch/nn/functional.py:2924 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
[rank0]:             rms_norm_38: "f32[16, 256, 768][196608, 768, 1]" = torch.rms_norm(add_4, (768,), None, None)
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:178 in forward, code: x = self.c_fc(x)
[rank0]:             linear_4: "bf16[16, 256, 3072][786432, 3072, 1]" = torch._C._nn.linear(rms_norm_38, l_self_modules_transformer_modules_h_modules_9_modules_mlp_modules_c_fc_parameters_weight_, None);  rms_norm_38 = l_self_modules_transformer_modules_h_modules_9_modules_mlp_modules_c_fc_parameters_weight_ = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:179 in forward, code: x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
[rank0]:             relu: "bf16[16, 256, 3072][786432, 3072, 1]" = torch.nn.functional.relu(linear_4);  linear_4 = None
[rank0]:             square: "f32[16, 256, 3072][786432, 3072, 1]" = relu.square();  relu = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:180 in forward, code: x = self.c_proj(x)
[rank0]:             linear_5: "bf16[16, 256, 768][196608, 768, 1]" = torch._C._nn.linear(square, l_self_modules_transformer_modules_h_modules_9_modules_mlp_modules_c_proj_parameters_weight_, None);  square = l_self_modules_transformer_modules_h_modules_9_modules_mlp_modules_c_proj_parameters_weight_ = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:192 in forward, code: x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))
[rank0]:             add_5: "f32[16, 256, 768][196608, 768, 1]" = add_4 + linear_5;  add_4 = linear_5 = None
[rank0]:             
[rank0]:              # File: /usr/local/lib/python3.12/dist-packages/torch/nn/functional.py:2924 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
[rank0]:             rms_norm_39: "f32[16, 256, 768][196608, 768, 1]" = torch.rms_norm(add_5, (768,), None, None)
[rank0]:             return (rms_norm_39, add_5)
[rank0]:             
[rank0]:     class submod_11(torch.nn.Module):
[rank0]:         def forward(self, rms_norm_40: "f32[16, 256, 768][196608, 768, 1]", l_self_modules_transformer_modules_h_modules_10_modules_attn_modules_c_q_parameters_weight_: "f32[768, 768][768, 1]", l_self_modules_transformer_modules_h_modules_10_modules_attn_modules_c_k_parameters_weight_: "f32[768, 768][768, 1]", l_self_modules_transformer_modules_h_modules_10_modules_attn_modules_c_v_parameters_weight_: "f32[768, 768][768, 1]", l_self_modules_transformer_modules_h_modules_10_modules_attn_modules_rotary_cos_cached: "bf16[256, 64][64, 1]", l_self_modules_transformer_modules_h_modules_10_modules_attn_modules_rotary_sin_cached: "bf16[256, 64][64, 1]", l_self_modules_transformer_modules_h_modules_10_modules_attn_modules_c_proj_parameters_weight_: "f32[768, 768][768, 1]", x_50: "f32[16, 256, 768][196608, 768, 1]", l_self_modules_transformer_modules_h_modules_10_modules_mlp_modules_c_fc_parameters_weight_: "f32[3072, 768][768, 1]", l_self_modules_transformer_modules_h_modules_10_modules_mlp_modules_c_proj_parameters_weight_: "f32[768, 3072][3072, 1]"):
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:158 in forward, code: q = self.c_q(x).view(B, T, self.n_head, self.head_dim)
[rank0]:             linear: "bf16[16, 256, 768][196608, 768, 1]" = torch._C._nn.linear(rms_norm_40, l_self_modules_transformer_modules_h_modules_10_modules_attn_modules_c_q_parameters_weight_, None);  l_self_modules_transformer_modules_h_modules_10_modules_attn_modules_c_q_parameters_weight_ = None
[rank0]:             view: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = linear.view(16, 256, 6, 128);  linear = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:159 in forward, code: k = self.c_k(x).view(B, T, self.n_head, self.head_dim)
[rank0]:             linear_1: "bf16[16, 256, 768][196608, 768, 1]" = torch._C._nn.linear(rms_norm_40, l_self_modules_transformer_modules_h_modules_10_modules_attn_modules_c_k_parameters_weight_, None);  l_self_modules_transformer_modules_h_modules_10_modules_attn_modules_c_k_parameters_weight_ = None
[rank0]:             view_1: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = linear_1.view(16, 256, 6, 128);  linear_1 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:160 in forward, code: v = self.c_v(x).view(B, T, self.n_head, self.head_dim)
[rank0]:             linear_2: "bf16[16, 256, 768][196608, 768, 1]" = torch._C._nn.linear(rms_norm_40, l_self_modules_transformer_modules_h_modules_10_modules_attn_modules_c_v_parameters_weight_, None);  l_self_modules_transformer_modules_h_modules_10_modules_attn_modules_c_v_parameters_weight_ = None
[rank0]:             view_2: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = linear_2.view(16, 256, 6, 128);  linear_2 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:129 in forward, code: return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]
[rank0]:             getitem: "bf16[1, 256, 1, 64][16384, 64, 64, 1]" = l_self_modules_transformer_modules_h_modules_10_modules_attn_modules_rotary_cos_cached[(None, slice(None, None, None), None, slice(None, None, None))];  l_self_modules_transformer_modules_h_modules_10_modules_attn_modules_rotary_cos_cached = None
[rank0]:             getitem_1: "bf16[1, 256, 1, 64][16384, 64, 64, 1]" = l_self_modules_transformer_modules_h_modules_10_modules_attn_modules_rotary_sin_cached[(None, slice(None, None, None), None, slice(None, None, None))];  l_self_modules_transformer_modules_h_modules_10_modules_attn_modules_rotary_sin_cached = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:134 in apply_rotary_emb, code: x1 = x[..., :d]
[rank0]:             getitem_2: "bf16[16, 256, 6, 64][196608, 768, 128, 1]" = view[(Ellipsis, slice(None, 64, None))]
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:135 in apply_rotary_emb, code: x2 = x[..., d:]
[rank0]:             getitem_3: "bf16[16, 256, 6, 64][196608, 768, 128, 1]" = view[(Ellipsis, slice(64, None, None))]
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:136 in apply_rotary_emb, code: y1 = x1 * cos + x2 * sin
[rank0]:             mul: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = getitem_2 * getitem
[rank0]:             mul_1: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = getitem_3 * getitem_1
[rank0]:             add: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = mul + mul_1;  mul = mul_1 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:137 in apply_rotary_emb, code: y2 = x1 * (-sin) + x2 * cos
[rank0]:             neg: "bf16[1, 256, 1, 64][16384, 64, 64, 1]" = -getitem_1
[rank0]:             mul_2: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = getitem_2 * neg;  getitem_2 = neg = None
[rank0]:             mul_3: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = getitem_3 * getitem;  getitem_3 = None
[rank0]:             add_1: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = mul_2 + mul_3;  mul_2 = mul_3 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:138 in apply_rotary_emb, code: return torch.cat([y1, y2], 3).type_as(x)
[rank0]:             cat: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = torch.cat([add, add_1], 3);  add = add_1 = None
[rank0]:             type_as: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = cat.type_as(view);  cat = view = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:134 in apply_rotary_emb, code: x1 = x[..., :d]
[rank0]:             getitem_4: "bf16[16, 256, 6, 64][196608, 768, 128, 1]" = view_1[(Ellipsis, slice(None, 64, None))]
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:135 in apply_rotary_emb, code: x2 = x[..., d:]
[rank0]:             getitem_5: "bf16[16, 256, 6, 64][196608, 768, 128, 1]" = view_1[(Ellipsis, slice(64, None, None))]
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:136 in apply_rotary_emb, code: y1 = x1 * cos + x2 * sin
[rank0]:             mul_4: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = getitem_4 * getitem
[rank0]:             mul_5: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = getitem_5 * getitem_1
[rank0]:             add_2: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = mul_4 + mul_5;  mul_4 = mul_5 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:137 in apply_rotary_emb, code: y2 = x1 * (-sin) + x2 * cos
[rank0]:             neg_1: "bf16[1, 256, 1, 64][16384, 64, 64, 1]" = -getitem_1;  getitem_1 = None
[rank0]:             mul_6: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = getitem_4 * neg_1;  getitem_4 = neg_1 = None
[rank0]:             mul_7: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = getitem_5 * getitem;  getitem_5 = getitem = None
[rank0]:             add_3: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = mul_6 + mul_7;  mul_6 = mul_7 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:138 in apply_rotary_emb, code: return torch.cat([y1, y2], 3).type_as(x)
[rank0]:             cat_1: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = torch.cat([add_2, add_3], 3);  add_2 = add_3 = None
[rank0]:             type_as_1: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = cat_1.type_as(view_1);  cat_1 = view_1 = None
[rank0]:             
[rank0]:              # File: /usr/local/lib/python3.12/dist-packages/torch/nn/functional.py:2924 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
[rank0]:             rms_norm: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = torch.rms_norm(type_as, (128,), None, None);  type_as = None
[rank0]:             rms_norm_41: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = torch.rms_norm(type_as_1, (128,), None, None);  type_as_1 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:164 in forward, code: y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)
[rank0]:             transpose: "bf16[16, 6, 256, 128][196608, 128, 768, 1]" = rms_norm.transpose(1, 2);  rms_norm = None
[rank0]:             transpose_1: "bf16[16, 6, 256, 128][196608, 128, 768, 1]" = rms_norm_41.transpose(1, 2);  rms_norm_41 = None
[rank0]:             transpose_2: "bf16[16, 6, 256, 128][196608, 128, 768, 1]" = view_2.transpose(1, 2);  view_2 = None
[rank0]:             scaled_dot_product_attention: "bf16[16, 6, 256, 128][196608, 128, 768, 1]" = torch._C._nn.scaled_dot_product_attention(transpose, transpose_1, transpose_2, is_causal = True);  transpose = transpose_1 = transpose_2 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:165 in forward, code: y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
[rank0]:             transpose_3: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = scaled_dot_product_attention.transpose(1, 2);  scaled_dot_product_attention = None
[rank0]:             contiguous: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = transpose_3.contiguous();  transpose_3 = None
[rank0]:             view_as: "bf16[16, 256, 768][196608, 768, 1]" = contiguous.view_as(rms_norm_40);  contiguous = rms_norm_40 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:166 in forward, code: y = self.c_proj(y)
[rank0]:             linear_3: "bf16[16, 256, 768][196608, 768, 1]" = torch._C._nn.linear(view_as, l_self_modules_transformer_modules_h_modules_10_modules_attn_modules_c_proj_parameters_weight_, None);  view_as = l_self_modules_transformer_modules_h_modules_10_modules_attn_modules_c_proj_parameters_weight_ = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:191 in forward, code: x = x + self.attn(F.rms_norm(x, (x.size(-1),)))
[rank0]:             add_4: "f32[16, 256, 768][196608, 768, 1]" = x_50 + linear_3;  x_50 = linear_3 = None
[rank0]:             
[rank0]:              # File: /usr/local/lib/python3.12/dist-packages/torch/nn/functional.py:2924 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
[rank0]:             rms_norm_42: "f32[16, 256, 768][196608, 768, 1]" = torch.rms_norm(add_4, (768,), None, None)
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:178 in forward, code: x = self.c_fc(x)
[rank0]:             linear_4: "bf16[16, 256, 3072][786432, 3072, 1]" = torch._C._nn.linear(rms_norm_42, l_self_modules_transformer_modules_h_modules_10_modules_mlp_modules_c_fc_parameters_weight_, None);  rms_norm_42 = l_self_modules_transformer_modules_h_modules_10_modules_mlp_modules_c_fc_parameters_weight_ = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:179 in forward, code: x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
[rank0]:             relu: "bf16[16, 256, 3072][786432, 3072, 1]" = torch.nn.functional.relu(linear_4);  linear_4 = None
[rank0]:             square: "f32[16, 256, 3072][786432, 3072, 1]" = relu.square();  relu = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:180 in forward, code: x = self.c_proj(x)
[rank0]:             linear_5: "bf16[16, 256, 768][196608, 768, 1]" = torch._C._nn.linear(square, l_self_modules_transformer_modules_h_modules_10_modules_mlp_modules_c_proj_parameters_weight_, None);  square = l_self_modules_transformer_modules_h_modules_10_modules_mlp_modules_c_proj_parameters_weight_ = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:192 in forward, code: x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))
[rank0]:             add_5: "f32[16, 256, 768][196608, 768, 1]" = add_4 + linear_5;  add_4 = linear_5 = None
[rank0]:             
[rank0]:              # File: /usr/local/lib/python3.12/dist-packages/torch/nn/functional.py:2924 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
[rank0]:             rms_norm_43: "f32[16, 256, 768][196608, 768, 1]" = torch.rms_norm(add_5, (768,), None, None)
[rank0]:             return (rms_norm_43, add_5)
[rank0]:             
[rank0]:     class submod_12(torch.nn.Module):
[rank0]:         def forward(self, rms_norm_44: "f32[16, 256, 768][196608, 768, 1]", l_self_modules_transformer_modules_h_modules_11_modules_attn_modules_c_q_parameters_weight_: "f32[768, 768][768, 1]", l_self_modules_transformer_modules_h_modules_11_modules_attn_modules_c_k_parameters_weight_: "f32[768, 768][768, 1]", l_self_modules_transformer_modules_h_modules_11_modules_attn_modules_c_v_parameters_weight_: "f32[768, 768][768, 1]", l_self_modules_transformer_modules_h_modules_11_modules_attn_modules_rotary_cos_cached: "bf16[256, 64][64, 1]", l_self_modules_transformer_modules_h_modules_11_modules_attn_modules_rotary_sin_cached: "bf16[256, 64][64, 1]", l_self_modules_transformer_modules_h_modules_11_modules_attn_modules_c_proj_parameters_weight_: "f32[768, 768][768, 1]", x_55: "f32[16, 256, 768][196608, 768, 1]", l_self_modules_transformer_modules_h_modules_11_modules_mlp_modules_c_fc_parameters_weight_: "f32[3072, 768][768, 1]", l_self_modules_transformer_modules_h_modules_11_modules_mlp_modules_c_proj_parameters_weight_: "f32[768, 3072][3072, 1]"):
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:158 in forward, code: q = self.c_q(x).view(B, T, self.n_head, self.head_dim)
[rank0]:             linear: "bf16[16, 256, 768][196608, 768, 1]" = torch._C._nn.linear(rms_norm_44, l_self_modules_transformer_modules_h_modules_11_modules_attn_modules_c_q_parameters_weight_, None);  l_self_modules_transformer_modules_h_modules_11_modules_attn_modules_c_q_parameters_weight_ = None
[rank0]:             view: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = linear.view(16, 256, 6, 128);  linear = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:159 in forward, code: k = self.c_k(x).view(B, T, self.n_head, self.head_dim)
[rank0]:             linear_1: "bf16[16, 256, 768][196608, 768, 1]" = torch._C._nn.linear(rms_norm_44, l_self_modules_transformer_modules_h_modules_11_modules_attn_modules_c_k_parameters_weight_, None);  l_self_modules_transformer_modules_h_modules_11_modules_attn_modules_c_k_parameters_weight_ = None
[rank0]:             view_1: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = linear_1.view(16, 256, 6, 128);  linear_1 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:160 in forward, code: v = self.c_v(x).view(B, T, self.n_head, self.head_dim)
[rank0]:             linear_2: "bf16[16, 256, 768][196608, 768, 1]" = torch._C._nn.linear(rms_norm_44, l_self_modules_transformer_modules_h_modules_11_modules_attn_modules_c_v_parameters_weight_, None);  l_self_modules_transformer_modules_h_modules_11_modules_attn_modules_c_v_parameters_weight_ = None
[rank0]:             view_2: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = linear_2.view(16, 256, 6, 128);  linear_2 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:129 in forward, code: return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]
[rank0]:             getitem: "bf16[1, 256, 1, 64][16384, 64, 64, 1]" = l_self_modules_transformer_modules_h_modules_11_modules_attn_modules_rotary_cos_cached[(None, slice(None, None, None), None, slice(None, None, None))];  l_self_modules_transformer_modules_h_modules_11_modules_attn_modules_rotary_cos_cached = None
[rank0]:             getitem_1: "bf16[1, 256, 1, 64][16384, 64, 64, 1]" = l_self_modules_transformer_modules_h_modules_11_modules_attn_modules_rotary_sin_cached[(None, slice(None, None, None), None, slice(None, None, None))];  l_self_modules_transformer_modules_h_modules_11_modules_attn_modules_rotary_sin_cached = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:134 in apply_rotary_emb, code: x1 = x[..., :d]
[rank0]:             getitem_2: "bf16[16, 256, 6, 64][196608, 768, 128, 1]" = view[(Ellipsis, slice(None, 64, None))]
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:135 in apply_rotary_emb, code: x2 = x[..., d:]
[rank0]:             getitem_3: "bf16[16, 256, 6, 64][196608, 768, 128, 1]" = view[(Ellipsis, slice(64, None, None))]
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:136 in apply_rotary_emb, code: y1 = x1 * cos + x2 * sin
[rank0]:             mul: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = getitem_2 * getitem
[rank0]:             mul_1: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = getitem_3 * getitem_1
[rank0]:             add: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = mul + mul_1;  mul = mul_1 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:137 in apply_rotary_emb, code: y2 = x1 * (-sin) + x2 * cos
[rank0]:             neg: "bf16[1, 256, 1, 64][16384, 64, 64, 1]" = -getitem_1
[rank0]:             mul_2: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = getitem_2 * neg;  getitem_2 = neg = None
[rank0]:             mul_3: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = getitem_3 * getitem;  getitem_3 = None
[rank0]:             add_1: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = mul_2 + mul_3;  mul_2 = mul_3 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:138 in apply_rotary_emb, code: return torch.cat([y1, y2], 3).type_as(x)
[rank0]:             cat: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = torch.cat([add, add_1], 3);  add = add_1 = None
[rank0]:             type_as: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = cat.type_as(view);  cat = view = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:134 in apply_rotary_emb, code: x1 = x[..., :d]
[rank0]:             getitem_4: "bf16[16, 256, 6, 64][196608, 768, 128, 1]" = view_1[(Ellipsis, slice(None, 64, None))]
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:135 in apply_rotary_emb, code: x2 = x[..., d:]
[rank0]:             getitem_5: "bf16[16, 256, 6, 64][196608, 768, 128, 1]" = view_1[(Ellipsis, slice(64, None, None))]
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:136 in apply_rotary_emb, code: y1 = x1 * cos + x2 * sin
[rank0]:             mul_4: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = getitem_4 * getitem
[rank0]:             mul_5: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = getitem_5 * getitem_1
[rank0]:             add_2: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = mul_4 + mul_5;  mul_4 = mul_5 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:137 in apply_rotary_emb, code: y2 = x1 * (-sin) + x2 * cos
[rank0]:             neg_1: "bf16[1, 256, 1, 64][16384, 64, 64, 1]" = -getitem_1;  getitem_1 = None
[rank0]:             mul_6: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = getitem_4 * neg_1;  getitem_4 = neg_1 = None
[rank0]:             mul_7: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = getitem_5 * getitem;  getitem_5 = getitem = None
[rank0]:             add_3: "bf16[16, 256, 6, 64][98304, 384, 64, 1]" = mul_6 + mul_7;  mul_6 = mul_7 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:138 in apply_rotary_emb, code: return torch.cat([y1, y2], 3).type_as(x)
[rank0]:             cat_1: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = torch.cat([add_2, add_3], 3);  add_2 = add_3 = None
[rank0]:             type_as_1: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = cat_1.type_as(view_1);  cat_1 = view_1 = None
[rank0]:             
[rank0]:              # File: /usr/local/lib/python3.12/dist-packages/torch/nn/functional.py:2924 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
[rank0]:             rms_norm: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = torch.rms_norm(type_as, (128,), None, None);  type_as = None
[rank0]:             rms_norm_45: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = torch.rms_norm(type_as_1, (128,), None, None);  type_as_1 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:164 in forward, code: y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)
[rank0]:             transpose: "bf16[16, 6, 256, 128][196608, 128, 768, 1]" = rms_norm.transpose(1, 2);  rms_norm = None
[rank0]:             transpose_1: "bf16[16, 6, 256, 128][196608, 128, 768, 1]" = rms_norm_45.transpose(1, 2);  rms_norm_45 = None
[rank0]:             transpose_2: "bf16[16, 6, 256, 128][196608, 128, 768, 1]" = view_2.transpose(1, 2);  view_2 = None
[rank0]:             scaled_dot_product_attention: "bf16[16, 6, 256, 128][196608, 128, 768, 1]" = torch._C._nn.scaled_dot_product_attention(transpose, transpose_1, transpose_2, is_causal = True);  transpose = transpose_1 = transpose_2 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:165 in forward, code: y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
[rank0]:             transpose_3: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = scaled_dot_product_attention.transpose(1, 2);  scaled_dot_product_attention = None
[rank0]:             contiguous: "bf16[16, 256, 6, 128][196608, 768, 128, 1]" = transpose_3.contiguous();  transpose_3 = None
[rank0]:             view_as: "bf16[16, 256, 768][196608, 768, 1]" = contiguous.view_as(rms_norm_44);  contiguous = rms_norm_44 = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:166 in forward, code: y = self.c_proj(y)
[rank0]:             linear_3: "bf16[16, 256, 768][196608, 768, 1]" = torch._C._nn.linear(view_as, l_self_modules_transformer_modules_h_modules_11_modules_attn_modules_c_proj_parameters_weight_, None);  view_as = l_self_modules_transformer_modules_h_modules_11_modules_attn_modules_c_proj_parameters_weight_ = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:191 in forward, code: x = x + self.attn(F.rms_norm(x, (x.size(-1),)))
[rank0]:             add_4: "f32[16, 256, 768][196608, 768, 1]" = x_55 + linear_3;  x_55 = linear_3 = None
[rank0]:             
[rank0]:              # File: /usr/local/lib/python3.12/dist-packages/torch/nn/functional.py:2924 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
[rank0]:             rms_norm_46: "f32[16, 256, 768][196608, 768, 1]" = torch.rms_norm(add_4, (768,), None, None)
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:178 in forward, code: x = self.c_fc(x)
[rank0]:             linear_4: "bf16[16, 256, 3072][786432, 3072, 1]" = torch._C._nn.linear(rms_norm_46, l_self_modules_transformer_modules_h_modules_11_modules_mlp_modules_c_fc_parameters_weight_, None);  rms_norm_46 = l_self_modules_transformer_modules_h_modules_11_modules_mlp_modules_c_fc_parameters_weight_ = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:179 in forward, code: x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
[rank0]:             relu: "bf16[16, 256, 3072][786432, 3072, 1]" = torch.nn.functional.relu(linear_4);  linear_4 = None
[rank0]:             square: "f32[16, 256, 3072][786432, 3072, 1]" = relu.square();  relu = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:180 in forward, code: x = self.c_proj(x)
[rank0]:             linear_5: "bf16[16, 256, 768][196608, 768, 1]" = torch._C._nn.linear(square, l_self_modules_transformer_modules_h_modules_11_modules_mlp_modules_c_proj_parameters_weight_, None);  square = l_self_modules_transformer_modules_h_modules_11_modules_mlp_modules_c_proj_parameters_weight_ = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:192 in forward, code: x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))
[rank0]:             add_5: "f32[16, 256, 768][196608, 768, 1]" = add_4 + linear_5;  add_4 = linear_5 = None
[rank0]:             
[rank0]:              # File: /usr/local/lib/python3.12/dist-packages/torch/nn/functional.py:2924 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
[rank0]:             rms_norm_47: "f32[16, 256, 768][196608, 768, 1]" = torch.rms_norm(add_5, (768,), None, None);  add_5 = None
[rank0]:             return rms_norm_47
[rank0]:             
[rank0]:     class submod_13(torch.nn.Module):
[rank0]:         def forward(self, x_61: "f32[16, 256, 768][196608, 768, 1]", l_self_modules_transformer_modules_wte_parameters_weight_: "f32[50304, 768][768, 1]", l_targets_: "i64[16, 256][s20, 1]", s20: "Sym(s20)"):
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:228 in forward, code: logits = self.lm_head(x)
[rank0]:             linear: "bf16[16, 256, 50304][12877824, 50304, 1]" = torch._C._nn.linear(x_61, l_self_modules_transformer_modules_wte_parameters_weight_, None);  x_61 = l_self_modules_transformer_modules_wte_parameters_weight_ = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:229 in forward, code: logits = logits.float() # use tf32/fp32 for logits
[rank0]:             float_1: "f32[16, 256, 50304][12877824, 50304, 1]" = linear.float();  linear = None
[rank0]:             
[rank0]:              # File: /deep-learning-speedrun-project/nanogpt/experiments/train_gpt_curriculum.py:230 in forward, code: loss = F.cross_entropy(logits.reshape(-1, logits.size(-1)), targets.reshape(-1), ignore_index=-1)
[rank0]:             reshape: "f32[4096, 50304][50304, 1]" = float_1.reshape(-1, 50304);  float_1 = None
[rank0]:             reshape_1: "i64[4096][1]" = l_targets_.reshape(-1);  l_targets_ = None
[rank0]:             cross_entropy: "f32[][]" = torch.nn.functional.cross_entropy(reshape, reshape_1, ignore_index = -1);  reshape = reshape_1 = None
[rank0]:             return cross_entropy
[rank0]:             

[rank0]: Original traceback:
[rank0]: None

[rank0]: Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"

E1208 21:01:38.282000 33059 torch/distributed/elastic/multiprocessing/api.py:874] failed (exitcode: 1) local_rank: 0 (pid: 33126) of binary: /usr/local/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 357, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/launcher/api.py", line 143, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/launcher/api.py", line 277, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
experiments/train_gpt_curriculum.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-12-08_21:01:38
  host      : fc99fc618882
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 33126)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
