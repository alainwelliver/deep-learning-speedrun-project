
================================================================================
NANOGPT EXPERIMENT LOG
================================================================================
Experiment Name: nanogpt_palm_parallel
Start Time: 2025-12-09T17:27:02.210144

GIT INFORMATION:
  Commit:  99352aac39f7798169e163db866987ab1956a981
  Branch:  main
  Remote:  https://github.com/alainwelliver/deep-learning-speedrun-project.git
  Dirty:   True

GPU INFORMATION:
  CUDA Available: True
  Device Count:   8
  CUDA Version:   12.8
  PyTorch:        2.8.0+cu128
  Device 0: NVIDIA H100 PCIe (85.0 GB)
  Device 1: NVIDIA H100 PCIe (85.0 GB)
  Device 2: NVIDIA H100 PCIe (85.0 GB)
  Device 3: NVIDIA H100 PCIe (85.0 GB)
  Device 4: NVIDIA H100 PCIe (85.0 GB)
  Device 5: NVIDIA H100 PCIe (85.0 GB)
  Device 6: NVIDIA H100 PCIe (85.0 GB)
  Device 7: NVIDIA H100 PCIe (85.0 GB)

================================================================================

[2025-12-09 17:27:02.210] [INFO] Starting experiment: nanogpt_palm_parallel
[2025-12-09 17:27:02.210] [INFO] Configuration file: configs/stage_c_full/palm_parallel.json
[2025-12-09 17:27:02.210] [INFO] Training script: experiments/train_gpt_palm.py
[2025-12-09 17:27:02.210] [INFO] Number of GPUs: 8
[2025-12-09 17:27:02.210] [WARNING] WARNING: GPU count overridden from 8 to 8
[2025-12-09 17:27:02.210] [INFO] Number of runs: 3
[2025-12-09 17:27:02.210] [INFO] Base seed: 42
[2025-12-09 17:27:02.210] [INFO] Description: Stage C: PaLM-style Parallel Attention and MLP - full run (5100 iters, 8 H100s)
[2025-12-09 17:27:02.210] [INFO] Target validation loss: 3.28
[2025-12-09 17:27:02.211] [INFO] Configuration saved to config.json
[2025-12-09 17:27:02.211] [INFO] Configuration: {
  "experiment_name": "nanogpt_palm_parallel",
  "description": "Stage C: PaLM-style Parallel Attention and MLP - full run (5100 iters, 8 H100s)",
  "script": "experiments/train_gpt_palm.py",
  "stage": "C_full",
  "n_gpus": 8,
  "base_seed": 42,
  "target_val_loss": 3.28,
  "modification_type": "architecture",
  "modification_details": "Parallel Attn+MLP block structure with 1/sqrt(2) scaling and shared RMSNorm",
  "stage_config": {
    "recommended_runs": 5,
    "notes": "Full training (5100 iters) - uses default hyperparameters from training script"
  },
  "hyperparameters": {
    "batch_size": 512,
    "device_batch_size": 64,
    "sequence_length": 1024,
    "num_iterations": 5100,
    "learning_rate": 0.0036,
    "warmup_iters": 0,
    "warmdown_iters": 1450,
    "weight_decay": 0,
    "val_loss_every": 125,
    "val_tokens": 10485760,
    "notes": "Using default hyperparameters, only architecture changed"
  }
}
[2025-12-09 17:27:02.211] [INFO] ================================================================================
[2025-12-09 17:27:02.211] [INFO] Starting 3 training runs...
[2025-12-09 17:27:02.211] [INFO] ================================================================================
[2025-12-09 17:27:02.211] [INFO] Starting run 0 with seed 42
[2025-12-09 17:27:02.211] [INFO] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
[2025-12-09 17:27:02.211] [INFO] Running: torchrun --standalone --nproc_per_node=8 experiments/train_gpt_palm.py
[2025-12-09 17:27:02.211] [INFO] Working directory: /deep-learning-speedrun-project/nanogpt
[2025-12-09 17:27:02.211] [INFO] Stdout log: experiment_logs/nanogpt_palm_parallel_20251209_172659/runs/run_0_seed_42/stdout.log
[2025-12-09 17:29:47.885] [INFO] Run 0 - Step 125: val_loss=4.9642
[2025-12-09 17:30:23.760] [INFO] Run 0 - Step 250: val_loss=4.2520
[2025-12-09 17:30:58.422] [INFO] Run 0 - Step 375: val_loss=4.0277
[2025-12-09 17:31:33.465] [INFO] Run 0 - Step 500: val_loss=3.9086
[2025-12-09 17:32:08.523] [INFO] Run 0 - Step 625: val_loss=3.8317
[2025-12-09 17:32:43.176] [INFO] Run 0 - Step 750: val_loss=3.7792
[2025-12-09 17:33:18.217] [INFO] Run 0 - Step 875: val_loss=3.7375
[2025-12-09 17:33:53.249] [INFO] Run 0 - Step 1000: val_loss=3.6994
[2025-12-09 17:34:27.888] [INFO] Run 0 - Step 1125: val_loss=3.6718
[2025-12-09 17:35:02.928] [INFO] Run 0 - Step 1250: val_loss=3.6445
[2025-12-09 17:35:37.951] [INFO] Run 0 - Step 1375: val_loss=3.6242
[2025-12-09 17:36:12.597] [INFO] Run 0 - Step 1500: val_loss=3.6017
[2025-12-09 17:36:47.649] [INFO] Run 0 - Step 1625: val_loss=3.5852
[2025-12-09 17:37:22.711] [INFO] Run 0 - Step 1750: val_loss=3.5682
[2025-12-09 17:37:57.370] [INFO] Run 0 - Step 1875: val_loss=3.5556
[2025-12-09 17:38:32.428] [INFO] Run 0 - Step 2000: val_loss=3.5440
[2025-12-09 17:39:07.456] [INFO] Run 0 - Step 2125: val_loss=3.5331
[2025-12-09 17:39:42.092] [INFO] Run 0 - Step 2250: val_loss=3.5254
[2025-12-09 17:40:17.114] [INFO] Run 0 - Step 2375: val_loss=3.5179
[2025-12-09 17:40:52.154] [INFO] Run 0 - Step 2500: val_loss=3.5097
[2025-12-09 17:41:26.785] [INFO] Run 0 - Step 2625: val_loss=3.4999
[2025-12-09 17:42:01.816] [INFO] Run 0 - Step 2750: val_loss=3.4935
[2025-12-09 17:42:36.832] [INFO] Run 0 - Step 2875: val_loss=3.4867
[2025-12-09 17:43:11.497] [INFO] Run 0 - Step 3000: val_loss=3.4786
[2025-12-09 17:43:46.519] [INFO] Run 0 - Step 3125: val_loss=3.4757
[2025-12-09 17:44:21.553] [INFO] Run 0 - Step 3250: val_loss=3.4666
[2025-12-09 17:44:56.200] [INFO] Run 0 - Step 3375: val_loss=3.4635
[2025-12-09 17:45:31.238] [INFO] Run 0 - Step 3500: val_loss=3.4585
[2025-12-09 17:46:06.360] [INFO] Run 0 - Step 3625: val_loss=3.4534
[2025-12-09 17:46:41.020] [INFO] Run 0 - Step 3750: val_loss=3.4465
[2025-12-09 17:47:16.061] [INFO] Run 0 - Step 3875: val_loss=3.4349
[2025-12-09 17:47:51.081] [INFO] Run 0 - Step 4000: val_loss=3.4227
[2025-12-09 17:48:25.718] [INFO] Run 0 - Step 4125: val_loss=3.4095
[2025-12-09 17:49:00.753] [INFO] Run 0 - Step 4250: val_loss=3.3981
[2025-12-09 17:49:35.807] [INFO] Run 0 - Step 4375: val_loss=3.3867
[2025-12-09 17:50:10.465] [INFO] Run 0 - Step 4500: val_loss=3.3746
[2025-12-09 17:50:45.522] [INFO] Run 0 - Step 4625: val_loss=3.3634
[2025-12-09 17:51:20.566] [INFO] Run 0 - Step 4750: val_loss=3.3535
[2025-12-09 17:51:55.226] [INFO] Run 0 - Step 4875: val_loss=3.3441
[2025-12-09 17:52:30.276] [INFO] Run 0 - Step 5000: val_loss=3.3364
[2025-12-09 17:52:58.349] [INFO] Run 0 - Step 5100: val_loss=3.3333
[2025-12-09 17:53:11.033] [INFO] Run 0 complete: val_loss=3.3333, train_loss=3.6687, time=1568.82s, seed=42
[2025-12-09 17:53:11.033] [INFO] Progress: 1/3 runs completed (1 successful)
[2025-12-09 17:53:11.034] [INFO] --------------------------------------------------------------------------------
[2025-12-09 17:53:11.034] [INFO] Starting run 1 with seed 43
[2025-12-09 17:53:11.034] [INFO] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
[2025-12-09 17:53:11.034] [INFO] Running: torchrun --standalone --nproc_per_node=8 experiments/train_gpt_palm.py
[2025-12-09 17:53:11.034] [INFO] Working directory: /deep-learning-speedrun-project/nanogpt
[2025-12-09 17:53:11.034] [INFO] Stdout log: experiment_logs/nanogpt_palm_parallel_20251209_172659/runs/run_1_seed_43/stdout.log
[2025-12-09 17:55:18.496] [INFO] Run 1 - Step 125: val_loss=4.9805
[2025-12-09 17:55:53.723] [INFO] Run 1 - Step 250: val_loss=4.2632
[2025-12-09 17:56:28.510] [INFO] Run 1 - Step 375: val_loss=4.0352
[2025-12-09 17:57:03.664] [INFO] Run 1 - Step 500: val_loss=3.9148
[2025-12-09 17:57:38.828] [INFO] Run 1 - Step 625: val_loss=3.8356
[2025-12-09 17:58:13.567] [INFO] Run 1 - Step 750: val_loss=3.7848
[2025-12-09 17:58:48.725] [INFO] Run 1 - Step 875: val_loss=3.7413
[2025-12-09 17:59:23.868] [INFO] Run 1 - Step 1000: val_loss=3.7030
[2025-12-09 17:59:58.621] [INFO] Run 1 - Step 1125: val_loss=3.6765
[2025-12-09 18:00:33.851] [INFO] Run 1 - Step 1250: val_loss=3.6494
[2025-12-09 18:01:09.012] [INFO] Run 1 - Step 1375: val_loss=3.6295
[2025-12-09 18:01:43.774] [INFO] Run 1 - Step 1500: val_loss=3.6058
[2025-12-09 18:02:19.015] [INFO] Run 1 - Step 1625: val_loss=3.5888
[2025-12-09 18:02:54.218] [INFO] Run 1 - Step 1750: val_loss=3.5714
[2025-12-09 18:03:28.994] [INFO] Run 1 - Step 1875: val_loss=3.5592
[2025-12-09 18:04:04.176] [INFO] Run 1 - Step 2000: val_loss=3.5479
[2025-12-09 18:04:39.336] [INFO] Run 1 - Step 2125: val_loss=3.5366
[2025-12-09 18:05:14.104] [INFO] Run 1 - Step 2250: val_loss=3.5288
[2025-12-09 18:05:49.260] [INFO] Run 1 - Step 2375: val_loss=3.5221
[2025-12-09 18:06:24.439] [INFO] Run 1 - Step 2500: val_loss=3.5134
[2025-12-09 18:06:59.201] [INFO] Run 1 - Step 2625: val_loss=3.5041
[2025-12-09 18:07:34.358] [INFO] Run 1 - Step 2750: val_loss=3.4977
[2025-12-09 18:08:09.505] [INFO] Run 1 - Step 2875: val_loss=3.4907
[2025-12-09 18:08:44.275] [INFO] Run 1 - Step 3000: val_loss=3.4833
[2025-12-09 18:09:19.442] [INFO] Run 1 - Step 3125: val_loss=3.4799
[2025-12-09 18:09:54.643] [INFO] Run 1 - Step 3250: val_loss=3.4706
[2025-12-09 18:10:29.416] [INFO] Run 1 - Step 3375: val_loss=3.4667
[2025-12-09 18:11:04.743] [INFO] Run 1 - Step 3500: val_loss=3.4617
[2025-12-09 18:11:40.122] [INFO] Run 1 - Step 3625: val_loss=3.4571
[2025-12-09 18:12:14.953] [INFO] Run 1 - Step 3750: val_loss=3.4500
[2025-12-09 18:12:50.255] [INFO] Run 1 - Step 3875: val_loss=3.4378
[2025-12-09 18:13:25.570] [INFO] Run 1 - Step 4000: val_loss=3.4254
[2025-12-09 18:14:00.402] [INFO] Run 1 - Step 4125: val_loss=3.4117
[2025-12-09 18:14:35.683] [INFO] Run 1 - Step 4250: val_loss=3.4007
[2025-12-09 18:15:10.983] [INFO] Run 1 - Step 4375: val_loss=3.3894
[2025-12-09 18:15:45.810] [INFO] Run 1 - Step 4500: val_loss=3.3773
[2025-12-09 18:16:21.222] [INFO] Run 1 - Step 4625: val_loss=3.3659
[2025-12-09 18:16:56.532] [INFO] Run 1 - Step 4750: val_loss=3.3561
[2025-12-09 18:17:31.373] [INFO] Run 1 - Step 4875: val_loss=3.3466
[2025-12-09 18:18:06.530] [INFO] Run 1 - Step 5000: val_loss=3.3392
[2025-12-09 18:18:34.682] [INFO] Run 1 - Step 5100: val_loss=3.3359
[2025-12-09 18:18:46.604] [INFO] Run 1 complete: val_loss=3.3359, train_loss=3.6720, time=1535.57s, seed=43
[2025-12-09 18:18:46.605] [INFO] Progress: 2/3 runs completed (2 successful)
[2025-12-09 18:18:46.605] [INFO] --------------------------------------------------------------------------------
[2025-12-09 18:18:46.605] [INFO] Starting run 2 with seed 44
[2025-12-09 18:18:46.605] [INFO] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
[2025-12-09 18:18:46.605] [INFO] Running: torchrun --standalone --nproc_per_node=8 experiments/train_gpt_palm.py
[2025-12-09 18:18:46.605] [INFO] Working directory: /deep-learning-speedrun-project/nanogpt
[2025-12-09 18:18:46.605] [INFO] Stdout log: experiment_logs/nanogpt_palm_parallel_20251209_172659/runs/run_2_seed_44/stdout.log
[2025-12-09 18:21:10.403] [INFO] Run 2 - Step 125: val_loss=4.9986
[2025-12-09 18:21:45.541] [INFO] Run 2 - Step 250: val_loss=4.2736
[2025-12-09 18:22:20.275] [INFO] Run 2 - Step 375: val_loss=4.0372
[2025-12-09 18:22:55.397] [INFO] Run 2 - Step 500: val_loss=3.9153
[2025-12-09 18:23:30.498] [INFO] Run 2 - Step 625: val_loss=3.8377
[2025-12-09 18:24:05.200] [INFO] Run 2 - Step 750: val_loss=3.7844
[2025-12-09 18:24:40.300] [INFO] Run 2 - Step 875: val_loss=3.7387
[2025-12-09 18:25:15.420] [INFO] Run 2 - Step 1000: val_loss=3.7009
[2025-12-09 18:25:50.122] [INFO] Run 2 - Step 1125: val_loss=3.6746
[2025-12-09 18:26:25.180] [INFO] Run 2 - Step 1250: val_loss=3.6484
[2025-12-09 18:27:00.253] [INFO] Run 2 - Step 1375: val_loss=3.6265
[2025-12-09 18:27:34.945] [INFO] Run 2 - Step 1500: val_loss=3.6067
[2025-12-09 18:28:10.020] [INFO] Run 2 - Step 1625: val_loss=3.5907
[2025-12-09 18:28:45.083] [INFO] Run 2 - Step 1750: val_loss=3.5717
[2025-12-09 18:29:19.762] [INFO] Run 2 - Step 1875: val_loss=3.5588
[2025-12-09 18:29:54.823] [INFO] Run 2 - Step 2000: val_loss=3.5467
[2025-12-09 18:30:29.862] [INFO] Run 2 - Step 2125: val_loss=3.5359
[2025-12-09 18:31:04.512] [INFO] Run 2 - Step 2250: val_loss=3.5274
[2025-12-09 18:31:39.547] [INFO] Run 2 - Step 2375: val_loss=3.5194
[2025-12-09 18:32:14.576] [INFO] Run 2 - Step 2500: val_loss=3.5131
[2025-12-09 18:32:49.211] [INFO] Run 2 - Step 2625: val_loss=3.5022
[2025-12-09 18:33:24.232] [INFO] Run 2 - Step 2750: val_loss=3.4957
[2025-12-09 18:33:59.254] [INFO] Run 2 - Step 2875: val_loss=3.4895
[2025-12-09 18:34:33.942] [INFO] Run 2 - Step 3000: val_loss=3.4810
[2025-12-09 18:35:08.971] [INFO] Run 2 - Step 3125: val_loss=3.4777
[2025-12-09 18:35:42.807] [WARNING] Run 2 interrupted by user
