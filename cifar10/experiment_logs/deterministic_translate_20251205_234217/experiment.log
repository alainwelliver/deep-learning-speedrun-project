
================================================================================
EXPERIMENT LOG
================================================================================
Experiment Name: deterministic_translate
Start Time: 2025-12-05T23:42:17.854328

GIT INFORMATION:
  Commit:  3cea4d3d5f0a29bdd78c133ddab13316376cb5e4
  Branch:  main
  Remote:  https://github.com/alainwelliver/deep-learning-speedrun-project.git
  Dirty:   True
  
GPU INFORMATION:
  CUDA Available: True
  Device Count:   1
  CUDA Version:   12.8
  PyTorch:        2.8.0+cu128
  Device 0: NVIDIA A100 80GB PCIe (85.1 GB)

================================================================================

[2025-12-05 23:42:17.854] [INFO] Starting experiment: deterministic_translate
[2025-12-05 23:42:17.854] [INFO] Configuration file: configs/deterministic_translate.json
[2025-12-05 23:42:17.854] [INFO] Module: airbench94_deterministic_translate
[2025-12-05 23:42:17.854] [INFO] Number of runs: 5
[2025-12-05 23:42:17.854] [INFO] Base seed: 42
[2025-12-05 23:42:17.854] [INFO] Hypothesis: Deterministic cycling through translation positions reduces redundancy (like alternating flip) and speeds up training convergence
[2025-12-05 23:42:17.854] [INFO] Modification: Replace random 2-pixel translation with deterministic position cycling based on hash(image_index) + epoch. Similar to how alternating flip removes randomness while preserving data diversity, cycling through translation positions should provide consistent augmentation without redundancy.
[2025-12-05 23:42:17.855] [INFO] Hyperparameters saved to hyperparameters.json
[2025-12-05 23:42:17.855] [INFO] Hyperparameters: {
  "experiment_name": "deterministic_translate",
  "hypothesis": "Deterministic cycling through translation positions reduces redundancy (like alternating flip) and speeds up training convergence",
  "modification": "Replace random 2-pixel translation with deterministic position cycling based on hash(image_index) + epoch. Similar to how alternating flip removes randomness while preserving data diversity, cycling through translation positions should provide consistent augmentation without redundancy.",
  "description": "The baseline uses random translation which can cause redundancy - the same image might get similar translations across epochs. By deterministically cycling through translation positions based on hash(image_index) + epoch, we ensure each image sees different translations across epochs while maintaining reproducibility. This is analogous to the alternating flip strategy that's already part of airbench94.",
  "base_seed": 42,
  "batch_size": 1024,
  "learning_rate": 11.5,
  "epochs": 9.9,
  "momentum": 0.85,
  "weight_decay": 0.0153,
  "bias_scaler": 64.0,
  "label_smoothing": 0.2,
  "whiten_bias_epochs": 3,
  "augmentation": {
    "flip": true,
    "translate": 2,
    "deterministic_translate": true
  },
  "target_accuracy": 0.94,
  "baseline_accuracy": 0.9401369762420654,
  "baseline_std": 0.001366604202900286,
  "baseline_n": 100,
  "implementation_notes": [
    "Modify the CifarLoader class in airbench94.py",
    "Replace random translation with deterministic cycling",
    "Use hash(image_index) + epoch to generate consistent translation offsets",
    "Cycle through the 25 possible translation positions (5x5 grid for 2-pixel translation)",
    "Maintain same translation magnitude (2 pixels) as baseline"
  ],
  "expected_impact": {
    "accuracy_change": "0.0% to +0.3%",
    "speed_improvement": "5-10%",
    "justification": "Removing randomness should slightly speed up training by reducing redundant similar augmentations. Similar to how alternating flip improved over random flip."
  },
  "ablations_planned": [
    "deterministic cycling (this experiment)",
    "random translation (baseline)",
    "no translation",
    "deterministic but fixed per image (no epoch variation)"
  ],
  "compute_requirements": {
    "gpu": "NVIDIA A100",
    "estimated_runs": 100,
    "estimated_total_time_minutes": 7
  }
}
[2025-12-05 23:42:17.855] [INFO] Loading and configuring airbench94_deterministic_translate...
[2025-12-05 23:42:18.587] [INFO] Effective augmentation config: {'flip': True, 'translate': 2, 'deterministic_translate': True}
[2025-12-05 23:42:18.587] [INFO] Running warmup...
